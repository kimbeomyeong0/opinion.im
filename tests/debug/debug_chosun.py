import asyncio
import aiohttp
from bs4 import BeautifulSoup
from rich.console import Console
from rich.panel import Panel
import json

console = Console()

async def debug_chosun_politics():
    """ì¡°ì„ ì¼ë³´ ì •ì¹˜ í˜ì´ì§€ HTML êµ¬ì¡° ë¶„ì„"""
    
    url = "https://www.chosun.com/politics/"
    
    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url, headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }) as response:
                
                if response.status != 200:
                    console.print(f"[red]ì ‘ê·¼ ì‹¤íŒ¨: {response.status}[/red]")
                    return
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                console.print(Panel("ğŸ” ì¡°ì„ ì¼ë³´ ì •ì¹˜ í˜ì´ì§€ HTML êµ¬ì¡° ë¶„ì„", border_style="blue"))
                
                # 1. ëª¨ë“  ë§í¬ ì°¾ê¸°
                all_links = soup.find_all('a', href=True)
                politics_links = [link for link in all_links if '/politics/' in link.get('href', '')]
                
                console.print(f"[green]ì´ ë§í¬ ìˆ˜: {len(all_links)}[/green]")
                console.print(f"[green]ì •ì¹˜ ê´€ë ¨ ë§í¬ ìˆ˜: {len(politics_links)}[/green]")
                
                # 2. ì •ì¹˜ ë§í¬ ìƒì„¸ ë¶„ì„
                console.print("\n[cyan]ğŸ“° ì •ì¹˜ ë§í¬ ìƒì„¸ ë¶„ì„:[/cyan]")
                for i, link in enumerate(politics_links[:10], 1):  # ìƒìœ„ 10ê°œë§Œ
                    href = link.get('href')
                    text = link.get_text(strip=True)
                    parent_tag = link.parent.name if link.parent else "None"
                    parent_class = link.parent.get('class', []) if link.parent else []
                    
                    console.print(f"{i}. {text[:50]}...")
                    console.print(f"   URL: {href}")
                    console.print(f"   ë¶€ëª¨: {parent_tag} {parent_class}")
                    console.print()
                
                # 3. ì œëª© ìš”ì†Œ ì°¾ê¸°
                console.print("\n[cyan]ğŸ“ ì œëª© ìš”ì†Œ ì°¾ê¸°:[/cyan]")
                title_selectors = ['h1', 'h2', 'h3', 'h4', '.title', '.headline', '.article-title']
                
                for selector in title_selectors:
                    elements = soup.select(selector)
                    if elements:
                        console.print(f"[green]âœ… {selector}: {len(elements)}ê°œ ë°œê²¬[/green]")
                        for elem in elements[:3]:  # ìƒìœ„ 3ê°œë§Œ
                            text = elem.get_text(strip=True)
                            if text:
                                console.print(f"   - {text[:50]}...")
                    else:
                        console.print(f"[red]âŒ {selector}: ì—†ìŒ[/red]")
                
                # 4. ë³¸ë¬¸ ìš”ì†Œ ì°¾ê¸°
                console.print("\n[cyan]ğŸ“„ ë³¸ë¬¸ ìš”ì†Œ ì°¾ê¸°:[/cyan]")
                content_selectors = ['.content', '.body', '.article-content', '.article-body', '.text', 'article']
                
                for selector in content_selectors:
                    elements = soup.select(selector)
                    if elements:
                        console.print(f"[green]âœ… {selector}: {len(elements)}ê°œ ë°œê²¬[/green]")
                        for elem in elements[:2]:  # ìƒìœ„ 2ê°œë§Œ
                            text = elem.get_text(strip=True)
                            if text:
                                console.print(f"   - {text[:100]}...")
                    else:
                        console.print(f"[red]âŒ {selector}: ì—†ìŒ[/red]")
                
                # 5. ì‹œê°„ ìš”ì†Œ ì°¾ê¸°
                console.print("\n[cyan]â° ì‹œê°„ ìš”ì†Œ ì°¾ê¸°:[/cyan]")
                time_selectors = ['.time', '.date', '.published-time', '.article-time', 'time']
                
                for selector in time_selectors:
                    elements = soup.select(selector)
                    if elements:
                        console.print(f"[green]âœ… {selector}: {len(elements)}ê°œ ë°œê²¬[/green]")
                        for elem in elements[:3]:  # ìƒìœ„ 3ê°œë§Œ
                            text = elem.get_text(strip=True)
                            if text:
                                console.print(f"   - {text}")
                    else:
                        console.print(f"[red]âŒ {selector}: ì—†ìŒ[/red]")
                
                # 6. ì‹¤ì œ ê¸°ì‚¬ URL íŒ¨í„´ ë¶„ì„
                console.print("\n[cyan]ğŸ”— ê¸°ì‚¬ URL íŒ¨í„´ ë¶„ì„:[/cyan]")
                article_urls = []
                for link in politics_links:
                    href = link.get('href')
                    if href and len(href) > 20:  # ì‹¤ì œ ê¸°ì‚¬ ë§í¬ëŠ” ê¸¸ì´ê°€ ê¹€
                        article_urls.append(href)
                
                # URL íŒ¨í„´ ê·¸ë£¹í™”
                url_patterns = {}
                for url in article_urls[:20]:  # ìƒìœ„ 20ê°œë§Œ
                    if url.startswith('/'):
                        url = url
                    
                    # íŒ¨í„´ ì¶”ì¶œ
                    if '/politics/' in url:
                        if '/article/' in url:
                            pattern = '/politics/article/'
                        elif '/north_korea/' in url:
                            pattern = '/politics/north_korea/'
                        elif '/politics_general/' in url:
                            pattern = '/politics/politics_general/'
                        else:
                            pattern = '/politics/'
                    else:
                        pattern = 'other'
                    
                    if pattern not in url_patterns:
                        url_patterns[pattern] = []
                    url_patterns[pattern].append(url)
                
                for pattern, urls in url_patterns.items():
                    console.print(f"[yellow]{pattern}: {len(urls)}ê°œ[/yellow]")
                    for url in urls[:3]:  # ìƒìœ„ 3ê°œë§Œ
                        console.print(f"   - {url}")
                
                # 7. HTML êµ¬ì¡° ì €ì¥ (ë””ë²„ê¹…ìš©)
                with open('chosun_politics_debug.html', 'w', encoding='utf-8') as f:
                    f.write(html)
                console.print(f"\n[green]ğŸ’¾ HTML êµ¬ì¡°ê°€ 'chosun_politics_debug.html'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.[/green]")
                
        except Exception as e:
            console.print(f"[red]ì—ëŸ¬ ë°œìƒ: {str(e)}[/red]")

if __name__ == "__main__":
    asyncio.run(debug_chosun_politics())
