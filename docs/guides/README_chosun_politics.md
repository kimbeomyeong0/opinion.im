# 조선일보 정치 크롤러

조선일보 정치 섹션의 기사를 자동으로 수집하는 크롤러입니다.

## 주요 기능

- **빠른 크롤링**: 20초 내 크롤링 완료 목표
- **병렬 처리**: ThreadPoolExecutor를 활용한 효율적인 데이터 수집
- **HTML 구조 활용**: 조선일보의 실제 HTML 구조에 최적화
- **시각적 진행률**: Rich 라이브러리를 활용한 진행률 표시
- **중복 제거**: 해시 기반 중복 체크로 중복 기사 방지

## 수집 정보

각 기사에서 다음 정보를 수집합니다:

- **제목**: 기사 제목
- **요약**: 기사 요약 (deck)
- **링크**: 기사 URL
- **시간**: 발행 시간
- **기자**: 기자명
- **이미지**: 썸네일 이미지 URL
- **본문**: 기사 전체 내용
- **카테고리**: 정치, 북한, 정치일반 등

## 설치 및 실행

### 1. 필요한 패키지 설치

```bash
pip install -r requirements.txt
```

### 2. 크롤러 실행

#### Python 직접 실행
```bash
python chosun_politics_crawler.py
```

#### 스크립트 실행
```bash
./run_chosun_politics.sh
```

## 설정 옵션

크롤러 클래스 초기화 시 다음 옵션을 설정할 수 있습니다:

```python
crawler = ChosunPoliticsCrawler(
    max_workers=10,  # 동시 작업자 수 (기본값: 10)
    timeout=10        # 요청 타임아웃 (기본값: 10초)
)
```

## 출력 파일

크롤링 결과는 JSON 파일로 저장됩니다:

- 파일명: `chosun_politics_YYYYMMDD_HHMMSS.json`
- 인코딩: UTF-8
- 형식: 구조화된 JSON 데이터

## 크롤링 과정

1. **기사 리스트 수집**: 정치 섹션 메인 페이지에서 기사 목록 수집
2. **상세 내용 수집**: 각 기사 링크를 방문하여 본문 내용 수집
3. **데이터 정리**: 중복 제거 및 데이터 정규화
4. **결과 저장**: JSON 파일로 저장 및 요약 정보 표시

## 주의사항

- **서버 부하 방지**: 요청 간 1초 간격을 두어 서버에 부하를 주지 않습니다
- **User-Agent 설정**: 실제 브라우저처럼 보이도록 User-Agent를 설정합니다
- **에러 처리**: 네트워크 오류나 파싱 오류에 대한 적절한 에러 처리를 포함합니다

## 성능 최적화

- **병렬 처리**: 여러 기사를 동시에 크롤링하여 속도 향상
- **세마포어**: 동시 요청 수를 제한하여 서버 부하 방지
- **타임아웃**: 각 요청에 타임아웃을 설정하여 무한 대기 방지

## 문제 해결

### 크롤링이 느린 경우
- `max_workers` 값을 늘려보세요
- `timeout` 값을 조정해보세요

### 기사를 찾을 수 없는 경우
- 조선일보 웹사이트 구조가 변경되었을 수 있습니다
- HTML 선택자를 확인하고 업데이트가 필요합니다

### 메모리 사용량이 높은 경우
- `max_workers` 값을 줄여보세요
- 한 번에 처리하는 기사 수를 줄여보세요

## 라이선스

이 프로젝트는 교육 및 연구 목적으로 제작되었습니다.
