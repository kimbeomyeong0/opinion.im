#!/usr/bin/env python3
"""
Leiden ê¸°ë°˜ ê·¸ë˜í”„ í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸

ëª©í‘œ:
1) (title + lead ìµœëŒ€ 500ì) ì„ë² ë”© ë²¡í„°ë¥¼ L2 ì •ê·œí™”
2) FAISS(ë˜ëŠ” sklearn NearestNeighbors, cosine)ë¡œ k=25 kNN ê·¸ë˜í”„ ìƒì„±
3) ì—£ì§€ ê°€ì¤‘ì¹˜=ì½”ì‚¬ì¸ìœ ì‚¬ë„, ë¬´ë°©í–¥ ê·¸ë˜í”„(igraph) êµ¬ì„±
4) Leidenìœ¼ë¡œ resolution âˆˆ {0.8, 1.0, 1.2, 1.3} ìŠ¤ìœ•
5) ê° ê²°ê³¼ì— ëŒ€í•´ (a) í´ëŸ¬ìŠ¤í„° ê°œìˆ˜, (b) ìµœëŒ€/ì¤‘ê°„/ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°, (c) ì½”ì‚¬ì¸ ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
   â†’ ê°€ì¥ â€œ10~20ê°œâ€ì— ê°€ê¹Œìš°ë©´ì„œ ì‹¤ë£¨ì—£ì´ ë†’ì€ ëª¨ë¸ ì„ íƒ
6) ì„ íƒ ê²°ê³¼ì˜ cluster_idë¥¼ Supabase articles.issue_id(ë˜ëŠ” ë³„ë„ mapping)ë¡œ ì €ì¥
7) ê° í´ëŸ¬ìŠ¤í„°ë³„ c-TF-IDF(ë˜ëŠ” ë‹¨ìˆœ TF-IDF)ë¡œ ìƒìœ„ í‚¤ì›Œë“œ/bi-gram 10ê°œ ì¶”ì¶œí•´ issue ë ˆì´ë¸” í›„ë³´ ìƒì„±
8) (ì˜µì…˜) 2D UMAPì€ ì‹œê°í™” ì „ìš©ìœ¼ë¡œë§Œ ì‚°ì¶œ, ì €ì¥

ì£¼ì˜:
- metricì€ cosine ê³ ì •, ì„ë² ë”©ì€ ë°˜ë“œì‹œ L2 normalize
- ë…¸ì´ì¦ˆ ì—†ì´ ëª¨ë“  ë…¸ë“œê°€ ì–´ë–¤ ì»¤ë®¤ë‹ˆí‹°ì—” ì†í•˜ê²Œ
- ê²°ê³¼ ìš”ì•½ í‘œ(í´ëŸ¬ìŠ¤í„° ê°œìˆ˜, í¬ê¸°ë¶„í¬, ì‹¤ë£¨ì—£) ì½˜ì†”ì— ì¶œë ¥
"""

import os
import sys
import re
import json
from datetime import datetime
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
from dotenv import load_dotenv

load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from utils.supabase_manager_unified import UnifiedSupabaseManager
except ImportError:
    from supabase_manager_unified import UnifiedSupabaseManager

# ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
try:
    import faiss  # type: ignore
    FAISS_AVAILABLE = True
except Exception:
    FAISS_AVAILABLE = False

from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize
from sklearn.feature_extraction.text import TfidfVectorizer

try:
    import igraph as ig
    import leidenalg as la
except Exception as e:
    print("âŒ igraph/leidenalg ë¯¸ì„¤ì¹˜: pip install python-igraph leidenalg")
    raise

try:
    import umap
    UMAP_AVAILABLE = True
except Exception:
    UMAP_AVAILABLE = False


def parse_embedding(value) -> np.ndarray:
    """Supabase USER-DEFINED embeddingì„ numpy ë°°ì—´ë¡œ íŒŒì‹±."""
    if isinstance(value, (list, np.ndarray)):
        return np.array(value, dtype=np.float32)
    if isinstance(value, str):
        nums = re.findall(r"-?\d+\.?\d*", value)
        arr = np.array([float(x) for x in nums], dtype=np.float32)
        return arr
    raise ValueError("Unknown embedding format")


def l2_normalize_rows(matrix: np.ndarray) -> np.ndarray:
    return normalize(matrix, norm='l2', axis=1)


def build_knn_cosine(embeddings: np.ndarray, k: int = 25) -> Tuple[np.ndarray, np.ndarray]:
    """ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜ kNN ì¸ë±ìŠ¤ êµ¬ì„±. ë°˜í™˜: (indices, similarities)"""
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = ë‚´ì  (L2 ì •ê·œí™” ì „ì œ)
    if FAISS_AVAILABLE:
        index = faiss.IndexFlatIP(embeddings.shape[1])
        index.add(embeddings.astype(np.float32))
        sims, idxs = index.search(embeddings.astype(np.float32), k + 1)  # self í¬í•¨
        # ì²« ì—´ì€ ìê¸° ìì‹ (ìœ ì‚¬ë„=1) ì œê±°
        return idxs[:, 1:], sims[:, 1:]
    # fallback: sklearn
    nn = NearestNeighbors(n_neighbors=k + 1, metric='cosine', algorithm='brute').fit(embeddings)
    distances, indices = nn.kneighbors(embeddings)
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ = 1 - distance
    sims = 1.0 - distances
    return indices[:, 1:], sims[:, 1:]


def knn_to_igraph(indices: np.ndarray, similarities: np.ndarray) -> ig.Graph:
    """kNN ê²°ê³¼ë¥¼ ë¬´ë°©í–¥ ê°€ì¤‘ ê·¸ë˜í”„ë¡œ ë³€í™˜."""
    n = indices.shape[0]
    edges = []
    weights = []
    for i in range(n):
        for j in range(indices.shape[1]):
            nbr = int(indices[i, j])
            w = float(similarities[i, j])
            if i == nbr:
                continue
            # ë¬´ë°©í–¥ ê·¸ë˜í”„: (min,max)ë¡œ ì¤‘ë³µ ë°©ì§€
            a, b = (i, nbr) if i < nbr else (nbr, i)
            edges.append((a, b))
            weights.append(w)
    g = ig.Graph()
    g.add_vertices(n)
    g.add_edges(edges)
    g.es['weight'] = weights
    return g.simplify(combine_edges={'weight': 'mean'})


def leiden_sweep(graph: ig.Graph, resolutions: List[float], embeddings: np.ndarray) -> Dict:
    """ì—¬ëŸ¬ resolutionì—ì„œ Leiden ì‹¤í–‰ í›„ í†µê³„/ì‹¤ë£¨ì—£ ìˆ˜ì§‘."""
    results = {}
    for res in resolutions:
        # CPM ëŒ€ì‹  Modularity ë˜ëŠ” RBConfiguration ì‚¬ìš©ìœ¼ë¡œ ê³¼ë¶„í•  ë°©ì§€
        try:
            # ModularityëŠ” ìŒìˆ˜ ê°€ì¤‘ì¹˜ë¥¼ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì ˆëŒ“ê°’ ì‚¬ìš©
            weights = [abs(w) for w in graph.es['weight']]
            part = la.find_partition(graph, la.ModularityVertexPartition, weights=weights)
        except Exception:
            # fallback: RBConfiguration
            part = la.find_partition(graph, la.RBConfigurationVertexPartition, weights='weight', resolution_parameter=res)
        labels = np.array(part.membership)
        # ì‹¤ë£¨ì—£ (ì½”ì‚¬ì¸ ê±°ë¦¬)
        try:
            sil = silhouette_score(embeddings, labels, metric='cosine') if len(np.unique(labels)) > 1 else -1.0
        except Exception:
            sil = -1.0
        sizes = np.bincount(labels)
        sizes_sorted = np.sort(sizes)
        results[res] = {
            'labels': labels,
            'n_clusters': int(len(sizes)),
            'size_min': int(sizes_sorted[0]) if len(sizes_sorted) else 0,
            'size_med': int(np.median(sizes_sorted)) if len(sizes_sorted) else 0,
            'size_max': int(sizes_sorted[-1]) if len(sizes_sorted) else 0,
            'silhouette': float(sil),
        }
    return results


def choose_best_result(results: Dict) -> Tuple[float, Dict]:
    """10~20ê°œì— ê°€ê¹ê³  ì‹¤ë£¨ì—£ì´ ë†’ì€ ê²°ê³¼ ì„ íƒ."""
    best_key = None
    best_score = -1e9
    best = None
    for res, info in results.items():
        c = info['n_clusters']
        sil = info['silhouette']
        # êµ°ì§‘ ìˆ˜ íŒ¨ë„í‹°: 10~20 ë²”ìœ„ ë°–ì´ë©´ ì„ í˜• í˜ë„í‹°
        if c < 10:
            penalty = (10 - c) * 0.1
        elif c > 20:
            penalty = (c - 20) * 0.1
        else:
            penalty = 0.0
        score = sil - penalty
        if score > best_score:
            best_score = score
            best_key = res
            best = info
    return best_key, best


def extract_keywords_by_cluster(titles: List[str], labels: np.ndarray, n_top: int = 10) -> Dict[int, List[Tuple[str, float]]]:
    """ë‹¨ìˆœ TF-IDFë¡œ êµ°ì§‘ë³„ ìƒìœ„ n-gram í‚¤ì›Œë“œ ì¶”ì¶œ."""
    df = pd.DataFrame({'title': titles, 'label': labels})
    keywords = {}
    for label in sorted(df['label'].unique()):
        texts = df[df['label'] == label]['title'].fillna("").astype(str).tolist()
        if not texts:
            keywords[label] = []
            continue
        # í´ëŸ¬ìŠ¤í„° í¬ê¸°ì— ë”°ë¼ ë™ì  min_df ì„¤ì •ìœ¼ë¡œ ì¶©ëŒ ë°©ì§€
        cluster_size = len(texts)
        if cluster_size < 3:
            # ì†Œí˜• í´ëŸ¬ìŠ¤í„°: uni-gramë§Œ, min_df=1
            ngram_range = (1, 1)
            min_df = 1
        else:
            # ëŒ€í˜• í´ëŸ¬ìŠ¤í„°: bi-gram í¬í•¨, min_df=2
            ngram_range = (1, 2)
            min_df = 2
        
        vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=5000, min_df=min_df)
        X = vectorizer.fit_transform(texts)
        vocab = np.array(vectorizer.get_feature_names_out())
        # í‰ê·  TF-IDFë¡œ ìƒìœ„ n ì¶”ì¶œ
        scores = np.asarray(X.mean(axis=0)).ravel()
        top_idx = np.argsort(scores)[::-1][:n_top]
        keywords[label] = [(vocab[i], float(scores[i])) for i in top_idx]
    return keywords


def load_embeddings_from_supabase() -> pd.DataFrame:
    sm = UnifiedSupabaseManager()
    emb = sm.client.table('embeddings').select('article_id, embedding').execute()
    if not emb.data:
        raise ValueError('embeddings í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤')
    # lead ì»¬ëŸ¼ ì´ìŠˆ(í•¨ìˆ˜ë¡œ ì¸ì‹)ë¡œ ì¸í•´ contentë¥¼ ì‚¬ìš©
    arts = sm.client.table('articles').select('id, title, content, issue_id').execute()
    if not arts.data:
        raise ValueError('articles í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤')
    df_emb = pd.DataFrame(emb.data)
    df_art = pd.DataFrame(arts.data)
    df = df_emb.merge(df_art, left_on='article_id', right_on='id', how='inner')
    # íŒŒì‹± ë° ì •ì œ
    df['embedding'] = df['embedding'].apply(parse_embedding)
    df = df.dropna(subset=['embedding'])
    # ì°¨ì› í†µì¼ (ìµœì†Œ ì°¨ì›)
    min_dim = min(len(x) for x in df['embedding'])
    df['embedding'] = df['embedding'].apply(lambda x: x[:min_dim].astype(np.float32))
    return df


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--k', type=int, default=50, help='kNNì—ì„œ k (25â†’50ìœ¼ë¡œ ìƒí–¥)')
    parser.add_argument('--update-articles', action='store_true', help='articles.issue_idì— ì €ì¥ ì‹œë„')
    parser.add_argument('--save-mapping', action='store_true', help='ë³„ë„ ë§¤í•‘ í…Œì´ë¸” ì €ì¥ ì‹œë„(article_cluster_mapping)')
    parser.add_argument('--save-umap', action='store_true', help='UMAP 2D ê³„ì‚° ë° CSV ì €ì¥')
    args = parser.parse_args()

    print('ğŸš€ Leiden í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘')
    df = load_embeddings_from_supabase()
    # title + content ìµœëŒ€ 500ì
    titles = (df['title'].fillna("").astype(str) + ' ' + df['content'].fillna("").astype(str)).str.slice(0, 500).tolist()
    article_ids = df['id'].tolist()

    embeddings = np.stack(df['embedding'].to_list()).astype(np.float32)
    embeddings = l2_normalize_rows(embeddings)

    print('ğŸ”— kNN ê·¸ë˜í”„ êµ¬ì„± ì¤‘...')
    idxs, sims = build_knn_cosine(embeddings, k=args.k)
    graph = knn_to_igraph(idxs, sims)
    print(f'âœ… ê·¸ë˜í”„: |V|={graph.vcount()}, |E|={graph.ecount()}')

    # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ resolution ê·¸ë¦¬ë“œ ì¬ì„¤ê³„
    resolutions = [0.1, 0.2, 0.3, 0.5, 0.8]
    print(f'ğŸ§ª Leiden ìŠ¤ìœ•: {resolutions}')
    sweep = leiden_sweep(graph, resolutions, embeddings)

    # ìš”ì•½ í‘œ ì¶œë ¥
    print('\nğŸ“Š ìŠ¤ìœ• ê²°ê³¼ ìš”ì•½:')
    print('resolution,n_clusters,size_min,size_med,size_max,silhouette')
    for r in resolutions:
        info = sweep[r]
        print(f"{r},{info['n_clusters']},{info['size_min']},{info['size_med']},{info['size_max']},{info['silhouette']:.4f}")

    best_res, best_info = choose_best_result(sweep)
    labels = best_info['labels']
    print(f"\nâœ… ì„ íƒëœ resolution={best_res} â†’ n_clusters={best_info['n_clusters']}, silhouette={best_info['silhouette']:.4f}")

    # í‚¤ì›Œë“œ ì¶”ì¶œ
    print('ğŸ”¤ í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ(TF-IDF, uni/bi-gram) ...')
    keywords = extract_keywords_by_cluster(titles, labels, n_top=10)
    for lab, items in keywords.items():
        top_terms = ', '.join([w for w, _ in items])
        print(f"  - Cluster {lab}: {top_terms}")

    # ì„ íƒ ê²°ê³¼ ì €ì¥
    sm = UnifiedSupabaseManager()
    
    print('ğŸ’¾ issues í…Œì´ë¸”ì— í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥ ì¤‘...')
    issue_ids = {}  # cluster_label -> issue_id ë§¤í•‘
    
    # ê° í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥
    for cluster_label in sorted(set(labels)):
        cluster_mask = labels == cluster_label
        cluster_titles = [titles[i] for i in range(len(titles)) if cluster_mask[i]]
        cluster_articles = [article_ids[i] for i in range(len(article_ids)) if cluster_mask[i]]
        
        # í´ëŸ¬ìŠ¤í„° ì œëª© ìƒì„± (ì²« ë²ˆì§¸ ê¸°ì‚¬ ì œëª© ì‚¬ìš©)
        if cluster_titles:
            cluster_title = cluster_titles[0][:100]  # ìµœëŒ€ 100ì
        else:
            cluster_title = f"í´ëŸ¬ìŠ¤í„° {cluster_label}"
        
        # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ìƒì„±
        cluster_summary = f"ì´ {len(cluster_articles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±ëœ í´ëŸ¬ìŠ¤í„°"
        
        # bias í†µê³„ ê³„ì‚° (ê°„ë‹¨í•œ ì˜ˆì‹œ)
        bias_left_pct = 0.0
        bias_center_pct = 0.0
        bias_right_pct = 0.0
        dominant_bias = "Center"  # ê¸°ë³¸ê°’
        
        try:
            # issues í…Œì´ë¸”ì— ì‚½ì…
            issue_data = {
                'title': cluster_title,
                'subtitle': f"í´ëŸ¬ìŠ¤í„° {cluster_label}",
                'summary': cluster_summary,
                'bias_left_pct': bias_left_pct,
                'bias_center_pct': bias_center_pct,
                'bias_right_pct': bias_right_pct,
                'dominant_bias': dominant_bias,
                'source_count': len(cluster_articles),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            result = sm.client.table('issues').insert(issue_data).execute()
            if result.data:
                issue_id = result.data[0]['id']
                issue_ids[cluster_label] = issue_id
                print(f"  âœ… í´ëŸ¬ìŠ¤í„° {cluster_label}: issue_id {issue_id} ìƒì„±")
            else:
                print(f"  âŒ í´ëŸ¬ìŠ¤í„° {cluster_label}: issues í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"  âŒ í´ëŸ¬ìŠ¤í„° {cluster_label}: {e}")
    
    print(f'âœ… ì´ {len(issue_ids)}ê°œ í´ëŸ¬ìŠ¤í„°ë¥¼ issues í…Œì´ë¸”ì— ì €ì¥')
    
    # articles.issue_id ì—…ë°ì´íŠ¸
    print('ğŸ’¾ articles.issue_id ì—…ë°ì´íŠ¸ ì¤‘...')
    updated_count = 0
    for i, (article_id, cluster_label) in enumerate(zip(article_ids, labels)):
        if cluster_label in issue_ids:
            try:
                result = sm.client.table('articles').update({
                    'issue_id': issue_ids[cluster_label]
                }).eq('id', article_id).execute()
                if result.data:
                    updated_count += 1
            except Exception as e:
                print(f"  âš ï¸ ê¸°ì‚¬ {article_id} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    print(f'âœ… articles.issue_id ì—…ë°ì´íŠ¸ ì™„ë£Œ: {updated_count}ê°œ ê¸°ì‚¬')

    # UMAP 2D ì €ì¥(ì˜µì…˜)
    if args.save_umap and UMAP_AVAILABLE:
        print('ğŸ—ºï¸ UMAP 2D ê³„ì‚° ì¤‘...')
        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric='cosine', random_state=42)
        emb2d = reducer.fit_transform(embeddings)
        out = pd.DataFrame({
            'article_id': article_ids,
            'x': emb2d[:, 0],
            'y': emb2d[:, 1],
            'label': labels,
        })
        os.makedirs('outputs', exist_ok=True)
        out_path = os.path.join('outputs', 'umap_2d.csv')
        out.to_csv(out_path, index=False)
        print(f'âœ… UMAP 2D ì €ì¥: {out_path}')

    # ë§¤í•‘ ë¡œì»¬ ì €ì¥ (fallback)
    if not (args.save_mapping or args.update_articles):
        os.makedirs('outputs', exist_ok=True)
        mapping_path = os.path.join('outputs', 'article_cluster_mapping.json')
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump({str(a): int(l) for a, l in zip(article_ids, labels.tolist())}, f, ensure_ascii=False)
        print(f'ğŸ’¾ ë¡œì»¬ ë§¤í•‘ ì €ì¥: {mapping_path}')

    print('\nğŸ‰ ì™„ë£Œ')


if __name__ == '__main__':
    main()


