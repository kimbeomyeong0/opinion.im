#!/usr/bin/env python3
"""
ê¸°ì‚¬ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ ë° ì´ìŠˆ ìƒì„± ìŠ¤í¬ë¦½íŠ¸
- Supabase embeddings í…Œì´ë¸”ì—ì„œ ì„ë² ë”© ë°ì´í„°ë¥¼ ì§ì ‘ ë¡œë“œ
- HDBSCANìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§
- í´ëŸ¬ìŠ¤í„°ë³„ë¡œ issues í…Œì´ë¸”ì— ìƒˆ row ìƒì„±
- í•´ë‹¹ í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ë“¤ì˜ issue_id ì—…ë°ì´íŠ¸
- GPT-4o-minië¡œ í´ëŸ¬ìŠ¤í„°ë³„ ì œëª©, ë¶€ì œëª©, ìš”ì•½ ìƒì„±
- bias_summaries, common_points, media_summaries í…Œì´ë¸”ì— GPT ìƒì„± ë°ì´í„° ì €ì¥
"""

import sys
import os
import json
import logging
import argparse
import random
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    import pandas as pd
    import numpy as np
    import umap
    import hdbscan
    from sklearn.preprocessing import StandardScaler
except ImportError as e:
    print(f"âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {e}")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
    print("pip install pandas umap-learn hdbscan scikit-learn")
    sys.exit(1)

try:
    from openai import OpenAI
except ImportError:
    print("OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install openai'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    sys.exit(1)

try:
    from utils.supabase_manager_unified import UnifiedSupabaseManager
except ImportError:
    from supabase_manager_unified import UnifiedSupabaseManager

class ArticleClusterer:
    """ê¸°ì‚¬ ì„ë² ë”© í´ëŸ¬ìŠ¤í„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.supabase = UnifiedSupabaseManager()
        self.logger = logging.getLogger(__name__)
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = self._init_openai_client()
        
        # í´ëŸ¬ìŠ¤í„°ë§ íŒŒë¼ë¯¸í„° (í’ˆì§ˆ ê°œì„ )
        self.umap_params = {
            'n_neighbors': 10,       # 20 â†’ 10ìœ¼ë¡œ ê°ì†Œ (ë” ì§€ì—­ì  êµ¬ì¡° í¬ì°©)
            'min_dist': 0.1,         # 0.05 â†’ 0.1ë¡œ ì¦ê°€ (í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ì¦ê°€)
            'n_components': 30,      # 100 â†’ 30ìœ¼ë¡œ ê°ì†Œ (ì°¨ì› ê³¼ë¶€í•˜ ë°©ì§€)
            'metric': 'cosine',
            'random_state': 42
        }
        
        self.hdbscan_params = {
            'min_cluster_size': 3,  # 5 â†’ 3ìœ¼ë¡œ ë” ì™„í™” (ë§¤ìš° ì‘ì€ í´ëŸ¬ìŠ¤í„° í—ˆìš©)
            'min_samples': 1,        # 2 â†’ 1ë¡œ ë” ì™„í™” (ë…¸ì´ì¦ˆ í•„í„°ë§ ìµœì†Œí™”)
            'metric': 'euclidean',
            'cluster_selection_method': 'eom'
        }
        
        # í†µê³„ ì •ë³´
        self.stats = {
            'total_articles': 0,
            'clusters_created': 0,
            'articles_updated': 0,
            'noise_articles': 0,
            'gpt_success': 0,
            'gpt_failed': 0,
            'bias_summaries_created': 0,
            'common_points_created': 0,
            'media_summaries_created': 0,
            'start_time': None,
            'end_time': None
        }
    
    def _init_openai_client(self) -> Optional[OpenAI]:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âŒ OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("í™˜ê²½ ë³€ìˆ˜ì— OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            return None
        
        try:
            client = OpenAI(api_key=api_key)
            print("âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            return client
        except Exception as e:
            print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            self.logger.error(f"OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return None
    
    def load_embeddings_from_supabase(self) -> pd.DataFrame:
        """Supabase embeddings í…Œì´ë¸”ì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ"""
        try:
            print("ğŸ“ Supabase embeddings í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘...")
            
            # embeddings í…Œì´ë¸” ì¡°íšŒ
            print("  ğŸ” embeddings í…Œì´ë¸” ì¡°íšŒ ì¤‘...")
            embeddings_result = self.supabase.client.table('embeddings').select('article_id, embedding').execute()
            if not embeddings_result.data:
                raise ValueError("embeddings í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"  âœ… embeddings í…Œì´ë¸”: {len(embeddings_result.data)}ê°œ í–‰ ì¡°íšŒ")
            
            # articles í…Œì´ë¸” ì¡°íšŒ
            print("  ğŸ” articles í…Œì´ë¸” ì¡°íšŒ ì¤‘...")
            articles_result = self.supabase.client.table('articles').select('id, title, bias, media_id').execute()
            if not articles_result.data:
                raise ValueError("articles í…Œì´ë¸”ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            print(f"  âœ… articles í…Œì´ë¸”: {len(articles_result.data)}ê°œ í–‰ ì¡°íšŒ")
            
            # ë°ì´í„° ë³‘í•©
            print("  ğŸ”„ ë°ì´í„° ë³‘í•© ì¤‘...")
            embeddings_df = pd.DataFrame(embeddings_result.data)
            articles_df = pd.DataFrame(articles_result.data)
            
            # ì¡°ì¸
            df = embeddings_df.merge(articles_df, left_on='article_id', right_on='id', how='inner')
            df = df.drop('article_id', axis=1)  # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
            
            print(f"  âœ… JOIN ì™„ë£Œ: {len(df)}ê°œ í–‰")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['id', 'embedding', 'title', 'bias', 'media_id']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_columns}")
            
            # embeddingì„ numpy ë°°ì—´ë¡œ ë³€í™˜ (ë¬¸ìì—´ íŒŒì‹±)
            def parse_embedding(embedding_str):
                try:
                    # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œí•˜ì—¬ numpy ë°°ì—´ë¡œ ë³€í™˜
                    if isinstance(embedding_str, str):
                        # ë¬¸ìì—´ì—ì„œ ìˆ«ì ë¶€ë¶„ë§Œ ì¶”ì¶œ
                        import re
                        numbers = re.findall(r'-?\d+\.?\d*', embedding_str)
                        # ì°¨ì›ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                        if len(numbers) < 1000:
                            print(f"âš ï¸ ì„ë² ë”© ì°¨ì›ì´ ë„ˆë¬´ ì‘ìŒ: {len(numbers)}")
                            return None
                        return np.array([float(num) for num in numbers])
                    else:
                        return np.array(embedding_str)
                except Exception as e:
                    print(f"âš ï¸ ì„ë² ë”© íŒŒì‹± ì‹¤íŒ¨: {e}")
                    return None
            
            df['embedding'] = df['embedding'].apply(parse_embedding)
            
            # íŒŒì‹± ì‹¤íŒ¨í•œ í–‰ ì œê±°
            df = df.dropna(subset=['embedding'])
            print(f"  âœ… íŒŒì‹± ì™„ë£Œ: {len(df)}ê°œ í–‰ (ì‹¤íŒ¨ ì œê±° í›„)")
            
            # ì„ë² ë”© ë°°ì—´ì„ 2D numpy ë°°ì—´ë¡œ ë³€í™˜ (ì°¨ì› í†µì¼)
            embeddings_list = df['embedding'].tolist()
            
            # ëª¨ë“  ì„ë² ë”©ì„ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ë§ì¶¤ (ê°€ì¥ ì‘ì€ ì°¨ì› ê¸°ì¤€)
            min_dim = min(len(emb) for emb in embeddings_list)
            print(f"  ğŸ“ ìµœì†Œ ì°¨ì›: {min_dim}")
            
            # ëª¨ë“  ì„ë² ë”©ì„ ë™ì¼í•œ ì°¨ì›ìœ¼ë¡œ ìë¥´ê¸°
            embeddings_unified = []
            for i, emb in enumerate(embeddings_list):
                if len(emb) >= min_dim:
                    embeddings_unified.append(emb[:min_dim])
                else:
                    print(f"âš ï¸ ì°¨ì›ì´ ë„ˆë¬´ ì‘ì€ ì„ë² ë”© ì œì™¸ (í–‰ {i}): {len(emb)}")
            
            print(f"  ğŸ”¢ í†µì¼ëœ ì„ë² ë”© ìˆ˜: {len(embeddings_unified)}")
            
            # numpy ë°°ì—´ë¡œ ë³€í™˜
            try:
                embeddings_array = np.array(embeddings_unified)
                print(f"  âœ… numpy ë°°ì—´ ë³€í™˜ ì™„ë£Œ: {embeddings_array.shape}")
            except Exception as e:
                print(f"âŒ numpy ë°°ì—´ ë³€í™˜ ì‹¤íŒ¨: {e}")
                # ì²« ë²ˆì§¸ ì„ë² ë”©ì˜ êµ¬ì¡° í™•ì¸
                if embeddings_unified:
                    print(f"  ğŸ” ì²« ë²ˆì§¸ ì„ë² ë”© íƒ€ì…: {type(embeddings_unified[0])}")
                    print(f"  ğŸ” ì²« ë²ˆì§¸ ì„ë² ë”© ê¸¸ì´: {len(embeddings_unified[0])}")
                    if len(embeddings_unified[0]) > 0:
                        print(f"  ğŸ” ì²« ë²ˆì§¸ ì„ë² ë”© ì²« ìš”ì†Œ íƒ€ì…: {type(embeddings_unified[0][0])}")
                raise
            
            # ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['id'])
            
            # embedding ì»¬ëŸ¼ì„ numpy ë°°ì—´ë¡œ ë³€í™˜
            df['embedding'] = embeddings_array
            
            self.stats['total_articles'] = len(df)
            print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ì„ë² ë”© ë¡œë“œ ì™„ë£Œ")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Supabaseì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def reduce_dimensions(self, embeddings: np.ndarray) -> np.ndarray:
        """UMAPìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ"""
        try:
            # embeddingsê°€ numpy ë°°ì—´ì¸ì§€ í™•ì¸
            if not isinstance(embeddings, np.ndarray):
                print(f"âš ï¸ embeddings íƒ€ì… ë³€í™˜: {type(embeddings)} â†’ numpy.ndarray")
                embeddings = np.array(embeddings)
            
            print(f"ğŸ” embeddings íƒ€ì…: {type(embeddings)}")
            print(f"ğŸ” embeddings shape: {embeddings.shape}")
            print(f"ğŸ” embeddings dtype: {embeddings.dtype}")
            
            if len(embeddings.shape) < 2:
                print(f"âš ï¸ embeddingsê°€ 1ì°¨ì›ì…ë‹ˆë‹¤. reshape í•„ìš”")
                embeddings = embeddings.reshape(-1, 1)
                print(f"ğŸ” reshape í›„ shape: {embeddings.shape}")
            
            print(f"ğŸ”„ UMAPìœ¼ë¡œ ì°¨ì› ì¶•ì†Œ ì¤‘... ({embeddings.shape[1]} â†’ {self.umap_params['n_components']})")
            
            # í‘œì¤€í™”
            scaler = StandardScaler()
            embeddings_scaled = scaler.fit_transform(embeddings)
            
            # UMAP ì ìš©
            reducer = umap.UMAP(**self.umap_params)
            embeddings_reduced = reducer.fit_transform(embeddings_scaled)
            
            print(f"âœ… ì°¨ì› ì¶•ì†Œ ì™„ë£Œ: {embeddings_reduced.shape}")
            return embeddings_reduced
            
        except Exception as e:
            self.logger.error(f"ì°¨ì› ì¶•ì†Œ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def cluster_articles(self, embeddings_reduced: np.ndarray) -> np.ndarray:
        """HDBSCANìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        try:
            print("ğŸ” HDBSCANìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
            
            # HDBSCAN ì ìš©
            clusterer = hdbscan.HDBSCAN(**self.hdbscan_params)
            cluster_labels = clusterer.fit_predict(embeddings_reduced)
            
            # í´ëŸ¬ìŠ¤í„° í†µê³„
            unique_labels = np.unique(cluster_labels)
            n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
            noise_count = np.sum(cluster_labels == -1)
            
            print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {n_clusters}ê°œ í´ëŸ¬ìŠ¤í„° ìƒì„±")
            print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸”: {unique_labels}")
            print(f"ğŸ“Š ë…¸ì´ì¦ˆ ê¸°ì‚¬ ìˆ˜: {noise_count}ê°œ")
            
            # í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ í‰ê°€
            self.evaluate_clustering_quality(embeddings_reduced, cluster_labels)
            
            return cluster_labels
            
        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def evaluate_clustering_quality(self, embeddings_reduced: np.ndarray, cluster_labels: np.ndarray):
        """í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€"""
        try:
            print("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì¤‘...")
            
            # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ í´ëŸ¬ìŠ¤í„°ë§Œ í•„í„°ë§
            valid_mask = cluster_labels != -1
            if np.sum(valid_mask) == 0:
                print("âš ï¸ ìœ íš¨í•œ í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            valid_embeddings = embeddings_reduced[valid_mask]
            valid_labels = cluster_labels[valid_mask]
            
            # í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸° ë¶„ì„
            unique_labels, counts = np.unique(valid_labels, return_counts=True)
            print(f"ğŸ“ í´ëŸ¬ìŠ¤í„°ë³„ í¬ê¸°:")
            for label, count in zip(unique_labels, counts):
                print(f"  í´ëŸ¬ìŠ¤í„° {label}: {count}ê°œ ê¸°ì‚¬")
            
            # í´ëŸ¬ìŠ¤í„° ê°„ ê±°ë¦¬ ë¶„ì„
            if len(unique_labels) > 1:
                from sklearn.metrics import silhouette_score
                try:
                    silhouette_avg = silhouette_score(valid_embeddings, valid_labels)
                    print(f"ğŸ¯ ì‹¤ë£¨ì—£ ì ìˆ˜: {silhouette_avg:.3f}")
                    if silhouette_avg > 0.5:
                        print("  âœ… ì¢‹ì€ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ")
                    elif silhouette_avg > 0.25:
                        print("  âš ï¸ ë³´í†µ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ")
                    else:
                        print("  âŒ ë‚®ì€ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ")
                except Exception as e:
                    print(f"âš ï¸ ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
            # í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê²½ê³ 
            if len(unique_labels) < 8:
                print("âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("  ğŸ’¡ ì œì•ˆ: min_cluster_sizeë¥¼ ë” ì¤„ì´ê±°ë‚˜, min_samplesë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
                print("  ğŸ’¡ ì œì•ˆ: UMAP n_neighborsë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, min_distë¥¼ ì¤„ì—¬ë³´ì„¸ìš”")
            elif len(unique_labels) > 50:
                print("âš ï¸ í´ëŸ¬ìŠ¤í„° ìˆ˜ê°€ ë„ˆë¬´ ë§ìŠµë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                print("  ğŸ’¡ ì œì•ˆ: min_cluster_sizeë¥¼ ëŠ˜ë¦¬ê±°ë‚˜, min_samplesë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”")
                print("  ğŸ’¡ ì œì•ˆ: UMAP n_neighborsë¥¼ ì¤„ì´ê±°ë‚˜, min_distë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”")
            else:
                print(f"âœ… ì ì ˆí•œ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(unique_labels)}ê°œ")
                
        except Exception as e:
            print(f"âš ï¸ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
    
    def analyze_clusters(self, df: pd.DataFrame, cluster_labels: np.ndarray) -> Dict:
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ë° ìš”ì•½ ìƒì„±"""
        try:
            print("ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")
            
            # í´ëŸ¬ìŠ¤í„° ë ˆì´ë¸” ì¶”ê°€
            df['cluster'] = cluster_labels
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ë¶„ì„
            clusters_summary = {}
            
            for cluster_id in sorted(df['cluster'].unique()):
                if cluster_id == -1:  # ë…¸ì´ì¦ˆ
                    self.stats['noise_articles'] = len(df[df['cluster'] == cluster_id])
                    continue
                
                cluster_articles = df[df['cluster'] == cluster_id]
                cluster_size = len(cluster_articles)
                
                # bias ë¶„í¬ ê³„ì‚°
                bias_counts = cluster_articles['bias'].value_counts()
                total_articles = len(cluster_articles)
                
                bias_left_pct = (bias_counts.get('left', 0) / total_articles) * 100
                bias_center_pct = (bias_counts.get('center', 0) / total_articles) * 100
                bias_right_pct = (bias_counts.get('right', 0) / total_articles) * 100
                
                # dominant bias ê²°ì •
                dominant_bias = bias_counts.idxmax() if len(bias_counts) > 0 else 'unknown'
                
                # ëŒ€í‘œ ì œëª© (ì²« ë²ˆì§¸ ê¸°ì‚¬)
                representative_title = cluster_articles.iloc[0]['title']
                
                # ê¸°ì‚¬ ì œëª© ìƒ˜í”Œë§ (ìµœëŒ€ 5ê°œ)
                sample_titles = cluster_articles['title'].sample(
                    min(5, len(cluster_articles)), 
                    random_state=42
                ).tolist()
                
                # biasë³„ ê¸°ì‚¬ ê·¸ë£¹í™”
                bias_groups = {}
                for bias in ['left', 'center', 'right']:
                    bias_articles = cluster_articles[cluster_articles['bias'] == bias]
                    if len(bias_articles) > 0:
                        bias_groups[bias] = bias_articles['title'].tolist()
                
                # ì–¸ë¡ ì‚¬ë³„ ê¸°ì‚¬ ê·¸ë£¹í™”
                media_groups = {}
                for media_id in cluster_articles['media_id'].unique():
                    media_articles = cluster_articles[cluster_articles['media_id'] == media_id]
                    if len(media_articles) > 0:
                        media_groups[media_id] = media_articles['title'].tolist()
                
                clusters_summary[cluster_id] = {
                    'size': cluster_size,
                    'title': representative_title,
                    'sample_titles': sample_titles,
                    'all_titles': cluster_articles['title'].tolist(),
                    'bias_groups': bias_groups,
                    'media_groups': media_groups,
                    'bias_left_pct': round(bias_left_pct, 1),
                    'bias_center_pct': round(bias_center_pct, 1),
                    'bias_right_pct': round(bias_right_pct, 1),
                    'dominant_bias': dominant_bias,
                    'articles': cluster_articles
                }
            
            self.stats['clusters_created'] = len(clusters_summary)
            print(f"âœ… {len(clusters_summary)}ê°œ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì™„ë£Œ")
            
            return clusters_summary
            
        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def generate_gpt_content(self, cluster_id: int, summary: Dict) -> Dict[str, str]:
        """GPT-4o-minië¡œ í´ëŸ¬ìŠ¤í„°ë³„ ì œëª©, ë¶€ì œëª©, ìš”ì•½ ìƒì„±"""
        if not self.openai_client:
            # OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ìœ¼ë©´ fallback
            return {
                'title': summary['title'],
                'subtitle': f"í´ëŸ¬ìŠ¤í„° {cluster_id}: {summary['size']}ê°œ ê¸°ì‚¬",
                'summary': f"ì´ í´ëŸ¬ìŠ¤í„°ëŠ” {summary['size']}ê°œì˜ ê¸°ì‚¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì£¼ìš” biasëŠ” {summary['dominant_bias']}ì…ë‹ˆë‹¤."
            }
        
        try:
            # ê¸°ì‚¬ ì œëª©ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
            titles_text = "\n".join([f"- {title}" for title in summary['sample_titles']])
            
            # bias ì •ë³´ ì¶”ê°€
            bias_info = f"\n\nBias ë¶„í¬: Left {summary['bias_left_pct']}%, Center {summary['bias_center_pct']}%, Right {summary['bias_right_pct']}%"
            dominant_bias_info = f"\nì£¼ìš” Bias: {summary['dominant_bias']}"
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            title_prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ ì œëª©ë“¤ì„ ì¢…í•©í•´ ê°„ê²°í•œ í•œ ì¤„ ëŒ€í‘œ ì œëª©ì„ ì‘ì„±í•´ì¤˜.

{titles_text}{bias_info}{dominant_bias_info}

ì œëª©ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            subtitle_prompt = f"""ì´ ì´ìŠˆë¥¼ ë³´ì¡° ì„¤ëª…í•˜ëŠ” ì§§ì€ ë¶€ì œëª©ì„ ì‘ì„±í•´ì¤˜.

{titles_text}{bias_info}{dominant_bias_info}

ë¶€ì œëª©ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            summary_prompt = f"""í•´ë‹¹ ê¸°ì‚¬ë“¤ì„ ì¢…í•©í•´ 3~4ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ì„ ì‘ì„±í•´ì¤˜.

{titles_text}{bias_info}{dominant_bias_info}

ìš”ì•½ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""

            # GPT í˜¸ì¶œ
            print(f"ğŸ¤– í´ëŸ¬ìŠ¤í„° {cluster_id}: GPTë¡œ ì œëª©/ë¶€ì œëª©/ìš”ì•½ ìƒì„± ì¤‘...")
            
            # ì œëª© ìƒì„±
            title_response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": title_prompt}],
                max_tokens=100,
                temperature=0.7
            )
            generated_title = title_response.choices[0].message.content.strip()
            
            # ë¶€ì œëª© ìƒì„±
            subtitle_response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": subtitle_prompt}],
                max_tokens=150,
                temperature=0.7
            )
            generated_subtitle = subtitle_response.choices[0].message.content.strip()
            
            # ìš”ì•½ ìƒì„±
            summary_response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": summary_prompt}],
                max_tokens=300,
                temperature=0.7
            )
            generated_summary = summary_response.choices[0].message.content.strip()
            
            self.stats['gpt_success'] += 1
            
            return {
                'title': generated_title,
                'subtitle': generated_subtitle,
                'summary': generated_summary
            }
            
        except Exception as e:
            self.logger.error(f"GPT ìƒì„± ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° {cluster_id}): {str(e)}")
            self.stats['gpt_failed'] += 1
            
            # fallback ë°˜í™˜
            return {
                'title': summary['title'],
                'subtitle': f"í´ëŸ¬ìŠ¤í„° {cluster_id}: {summary['size']}ê°œ ê¸°ì‚¬",
                'summary': f"ì´ í´ëŸ¬ìŠ¤í„°ëŠ” {summary['size']}ê°œì˜ ê¸°ì‚¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì£¼ìš” biasëŠ” {summary['dominant_bias']}ì…ë‹ˆë‹¤."
            }
    
    def generate_bias_summaries(self, cluster_id: int, issue_id: str, summary: Dict) -> List[Dict]:
        """ê° bias ê·¸ë£¹ë³„ë¡œ GPTë¡œ ìš”ì•½ ìƒì„±"""
        bias_summaries = []
        
        for bias in ['left', 'center', 'right']:
            if bias not in summary['bias_groups'] or len(summary['bias_groups'][bias]) == 0:
                continue
            
            bias_titles = summary['bias_groups'][bias]
            titles_text = "\n".join([f"- {title}" for title in bias_titles])
            
            if not self.openai_client:
                # fallback
                bias_summary = f"{bias} ì„±í–¥ ì–¸ë¡ ì˜ ì‹œê°: {len(bias_titles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±"
            else:
                try:
                    prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ ì œëª©ë“¤ì„ ìš”ì•½í•´ì„œ {bias} ì„±í–¥ ì–¸ë¡ ì´ ë°”ë¼ë³¸ ì‹œê°ì„ 3~4ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜.

{titles_text}

{bias} ì„±í–¥ ì–¸ë¡ ì˜ ì‹œê°ë§Œ ìš”ì•½í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
                    
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=200,
                        temperature=0.7
                    )
                    bias_summary = response.choices[0].message.content.strip()
                    
                except Exception as e:
                    self.logger.error(f"Bias ìš”ì•½ GPT ìƒì„± ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° {cluster_id}, {bias}): {str(e)}")
                    bias_summary = f"{bias} ì„±í–¥ ì–¸ë¡ ì˜ ì‹œê°: {len(bias_titles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±"
            
            bias_summaries.append({
                'issue_id': issue_id,
                'bias': bias,
                'summary': bias_summary
            })
        
        return bias_summaries
    
    def generate_common_points(self, cluster_id: int, issue_id: str, summary: Dict) -> List[Dict]:
        """í´ëŸ¬ìŠ¤í„° ë‚´ ëª¨ë“  ê¸°ì‚¬ ì œëª©ì„ GPTì— ë„£ê³  ê³µí†µ í¬ì¸íŠ¸ 3ê°œ ìƒì„±"""
        all_titles = summary['all_titles']
        titles_text = "\n".join([f"- {title}" for title in all_titles])
        
        if not self.openai_client:
            # fallback
            common_points = [
                f"ê³µí†µ í¬ì¸íŠ¸ 1: {all_titles[0][:50]}...",
                f"ê³µí†µ í¬ì¸íŠ¸ 2: {all_titles[1][:50]}..." if len(all_titles) > 1 else "ê³µí†µ í¬ì¸íŠ¸ 2: ê¸°ì‚¬ ë‚´ìš© ë¶„ì„",
                f"ê³µí†µ í¬ì¸íŠ¸ 3: {all_titles[2][:50]}..." if len(all_titles) > 2 else "ê³µí†µ í¬ì¸íŠ¸ 3: ì´ìŠˆ ìš”ì•½"
            ]
        else:
            try:
                prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ë“¤ì´ ê³µí†µì ìœ¼ë¡œ ê°•ì¡°í•˜ëŠ” í•µì‹¬ í¬ì¸íŠ¸ë¥¼ 3ê°œ bullet pointë¡œ ë½‘ì•„ì¤˜.

{titles_text}

3ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì‘ì„±í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”. ê° í¬ì¸íŠ¸ëŠ” í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                
                response = self.openai_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=300,
                    temperature=0.7
                )
                
                content = response.choices[0].message.content.strip()
                # bullet pointë“¤ì„ ë¶„ë¦¬
                lines = [line.strip() for line in content.split('\n') if line.strip()]
                common_points = []
                
                for line in lines:
                    # bullet point ê¸°í˜¸ ì œê±°
                    point = line.lstrip('â€¢-*').strip()
                    if point:
                        common_points.append(point)
                
                # 3ê°œë¡œ ë§ì¶”ê¸°
                while len(common_points) < 3:
                    common_points.append(f"ê³µí†µ í¬ì¸íŠ¸ {len(common_points) + 1}: ê¸°ì‚¬ ë‚´ìš© ë¶„ì„")
                common_points = common_points[:3]
                
            except Exception as e:
                self.logger.error(f"ê³µí†µ í¬ì¸íŠ¸ GPT ìƒì„± ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° {cluster_id}): {str(e)}")
                common_points = [
                    f"ê³µí†µ í¬ì¸íŠ¸ 1: {all_titles[0][:50]}...",
                    f"ê³µí†µ í¬ì¸íŠ¸ 2: {all_titles[1][:50]}..." if len(all_titles) > 1 else "ê³µí†µ í¬ì¸íŠ¸ 2: ê¸°ì‚¬ ë‚´ìš© ë¶„ì„",
                    f"ê³µí†µ í¬ì¸íŠ¸ 3: {all_titles[2][:50]}..." if len(all_titles) > 2 else "ê³µí†µ í¬ì¸íŠ¸ 3: ì´ìŠˆ ìš”ì•½"
                ]
        
        return [{'issue_id': issue_id, 'point': point} for point in common_points]
    
    def generate_media_summaries(self, cluster_id: int, issue_id: str, summary: Dict) -> List[Dict]:
        """ê° ì–¸ë¡ ì‚¬ë³„ë¡œ GPTë¡œ ìš”ì•½ ìƒì„±"""
        media_summaries = []
        
        for media_id, titles in summary['media_groups'].items():
            if len(titles) == 0:
                continue
            
            titles_text = "\n".join([f"- {title}" for title in titles])
            
            if not self.openai_client:
                # fallback
                media_summary = f"ì–¸ë¡ ì‚¬ {media_id}ì˜ ì‹œê°: {len(titles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±"
            else:
                try:
                    prompt = f"""ë‹¤ìŒì€ ì–¸ë¡ ì‚¬ {media_id}ê°€ ë³´ë„í•œ ê¸°ì‚¬ë“¤ì´ë‹¤. ì´ ì–¸ë¡ ì‚¬ì˜ ì‹œê°ì„ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜.

{titles_text}

ì–¸ë¡ ì‚¬ {media_id}ì˜ ì‹œê°ë§Œ ìš”ì•½í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”."""
                    
                    response = self.openai_client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=200,
                        temperature=0.7
                    )
                    media_summary = response.choices[0].message.content.strip()
                    
                except Exception as e:
                    self.logger.error(f"ì–¸ë¡ ì‚¬ ìš”ì•½ GPT ìƒì„± ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° {cluster_id}, ì–¸ë¡ ì‚¬ {media_id}): {str(e)}")
                    media_summary = f"ì–¸ë¡ ì‚¬ {media_id}ì˜ ì‹œê°: {len(titles)}ê°œ ê¸°ì‚¬ë¡œ êµ¬ì„±"
            
            media_summaries.append({
                'issue_id': issue_id,
                'media_id': media_id,
                'summary': media_summary
            })
        
        return media_summaries
    
    def create_issues(self, clusters_summary: Dict) -> Dict[int, str]:
        """í´ëŸ¬ìŠ¤í„°ë³„ë¡œ issues í…Œì´ë¸”ì— ìƒˆ row ìƒì„±"""
        try:
            print("ğŸ’¾ issues í…Œì´ë¸”ì— í´ëŸ¬ìŠ¤í„° ì €ì¥ ì¤‘...")
            
            cluster_issue_ids = {}
            
            for cluster_id, summary in clusters_summary.items():
                # GPTë¡œ ì œëª©, ë¶€ì œëª©, ìš”ì•½ ìƒì„±
                gpt_content = self.generate_gpt_content(cluster_id, summary)
                
                # ì½˜ì†”ì— GPT ìƒì„± ê²°ê³¼ ì¶œë ¥
                print(f"\nğŸ“‹ í´ëŸ¬ìŠ¤í„° {cluster_id} GPT ìƒì„± ê²°ê³¼:")
                print(f"  ğŸ·ï¸ ì œëª©: {gpt_content['title']}")
                print(f"  ğŸ“ ë¶€ì œëª©: {gpt_content['subtitle']}")
                print(f"  ğŸ“„ ìš”ì•½: {gpt_content['summary']}")
                print("-" * 50)
                
                # issues í…Œì´ë¸”ì— insert
                issue_data = {
                    'title': gpt_content['title'],
                    'subtitle': gpt_content['subtitle'],
                    'summary': gpt_content['summary'],
                    'bias_left_pct': summary['bias_left_pct'],
                    'bias_center_pct': summary['bias_center_pct'],
                    'bias_right_pct': summary['bias_right_pct'],
                    'dominant_bias': summary['dominant_bias'],
                    'source_count': summary['size']
                }
                
                result = self.supabase.client.table('issues').insert(issue_data).execute()
                
                if result.data and len(result.data) > 0:
                    issue_id = result.data[0]['id']
                    cluster_issue_ids[cluster_id] = issue_id
                    print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_id}: issue ID {issue_id} ìƒì„±")
                    
                    # ì¶”ê°€ í…Œì´ë¸” ë°ì´í„° ìƒì„±
                    self._create_additional_tables(cluster_id, issue_id, summary)
                else:
                    print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id}: issue ìƒì„± ì‹¤íŒ¨")
            
            print(f"âœ… {len(cluster_issue_ids)}ê°œ issue ìƒì„± ì™„ë£Œ")
            return cluster_issue_ids
            
        except Exception as e:
            self.logger.error(f"issues ìƒì„± ì‹¤íŒ¨: {str(e)}")
            raise
    
    def _create_additional_tables(self, cluster_id: int, issue_id: str, summary: Dict):
        """ì¶”ê°€ í…Œì´ë¸”ë“¤(bias_summaries, common_points, media_summaries)ì— ë°ì´í„° ìƒì„±"""
        try:
            print(f"ğŸ”„ í´ëŸ¬ìŠ¤í„° {cluster_id}: ì¶”ê°€ í…Œì´ë¸” ë°ì´í„° ìƒì„± ì¤‘...")
            
            # 1. bias_summaries ìƒì„±
            bias_summaries = self.generate_bias_summaries(cluster_id, issue_id, summary)
            if bias_summaries:
                result = self.supabase.client.table('bias_summaries').insert(bias_summaries).execute()
                if result.data:
                    self.stats['bias_summaries_created'] += len(result.data)
                    print(f"  âœ… bias_summaries: {len(result.data)}ê°œ ìƒì„±")
                else:
                    print(f"  âŒ bias_summaries ìƒì„± ì‹¤íŒ¨")
            
            # 2. common_points ìƒì„±
            common_points = self.generate_common_points(cluster_id, issue_id, summary)
            if common_points:
                result = self.supabase.client.table('common_points').insert(common_points).execute()
                if result.data:
                    self.stats['common_points_created'] += len(result.data)
                    print(f"  âœ… common_points: {len(result.data)}ê°œ ìƒì„±")
                else:
                    print(f"  âŒ common_points ìƒì„± ì‹¤íŒ¨")
            
            # 3. media_summaries ìƒì„±
            media_summaries = self.generate_media_summaries(cluster_id, issue_id, summary)
            if media_summaries:
                result = self.supabase.client.table('media_summaries').insert(media_summaries).execute()
                if result.data:
                    self.stats['media_summaries_created'] += len(result.data)
                    print(f"  âœ… media_summaries: {len(result.data)}ê°œ ìƒì„±")
                else:
                    print(f"  âŒ media_summaries ìƒì„± ì‹¤íŒ¨")
            
            print(f"  âœ… í´ëŸ¬ìŠ¤í„° {cluster_id} ì¶”ê°€ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ì¶”ê°€ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨ (í´ëŸ¬ìŠ¤í„° {cluster_id}): {str(e)}")
            print(f"  âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ì¶”ê°€ í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def update_articles_issue_id(self, clusters_summary: Dict, cluster_issue_ids: Dict[int, str]) -> None:
        """í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê¸°ì‚¬ë“¤ì˜ issue_id ì—…ë°ì´íŠ¸"""
        try:
            print("ğŸ”„ ê¸°ì‚¬ë“¤ì˜ issue_id ì—…ë°ì´íŠ¸ ì¤‘...")
            
            total_updated = 0
            
            for cluster_id, summary in clusters_summary.items():
                if cluster_id not in cluster_issue_ids:
                    continue
                
                issue_id = cluster_issue_ids[cluster_id]
                article_ids = summary['articles']['id'].tolist()
                
                # articles í…Œì´ë¸” ì—…ë°ì´íŠ¸
                for article_id in article_ids:
                    result = self.supabase.client.table('articles').update({
                        'issue_id': issue_id
                    }).eq('id', article_id).execute()
                    
                    if result.data:
                        total_updated += 1
                    else:
                        print(f"âš ï¸ ê¸°ì‚¬ ID {article_id} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            
            self.stats['articles_updated'] = total_updated
            print(f"âœ… {total_updated}ê°œ ê¸°ì‚¬ issue_id ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ issue_id ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            raise
    
    def display_results(self, clusters_summary: Dict):
        """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¶œë ¥"""
        print("\nğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
        print("=" * 60)
        
        # í´ëŸ¬ìŠ¤í„°ë³„ ìš”ì•½
        print(f"ğŸ“Š ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clusters_summary)}ê°œ")
        print(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {self.stats['total_articles']}ê°œ")
        print(f"ğŸ“Š ë…¸ì´ì¦ˆ ê¸°ì‚¬ ìˆ˜: {self.stats['noise_articles']}ê°œ")
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ìƒì„±: {self.stats['clusters_created']}ê°œ")
        print(f"ğŸ“Š ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {self.stats['articles_updated']}ê°œ")
        print(f"ğŸ“Š GPT ì„±ê³µ: {self.stats['gpt_success']}ê°œ")
        print(f"ğŸ“Š GPT ì‹¤íŒ¨: {self.stats['gpt_failed']}ê°œ")
        print(f"ğŸ“Š Bias ìš”ì•½ ìƒì„±: {self.stats['bias_summaries_created']}ê°œ")
        print(f"ğŸ“Š ê³µí†µ í¬ì¸íŠ¸ ìƒì„±: {self.stats['common_points_created']}ê°œ")
        print(f"ğŸ“Š ì–¸ë¡ ì‚¬ ìš”ì•½ ìƒì„±: {self.stats['media_summaries_created']}ê°œ")
        
        # ì†Œìš” ì‹œê°„
        if self.stats['start_time'] and self.stats['end_time']:
            duration = self.stats['end_time'] - self.stats['start_time']
            print(f"â±ï¸ ì†Œìš” ì‹œê°„: {str(duration).split('.')[0]}")
        
        print("\nğŸ“‹ í´ëŸ¬ìŠ¤í„°ë³„ ìƒì„¸ ì •ë³´:")
        print("-" * 60)
        
        for cluster_id, summary in sorted(clusters_summary.items()):
            print(f"í´ëŸ¬ìŠ¤í„° {cluster_id}:")
            print(f"  ğŸ“° ê¸°ì‚¬ ìˆ˜: {summary['size']}ê°œ")
            print(f"  ğŸ·ï¸ ëŒ€í‘œ ì œëª©: {summary['title'][:50]}...")
            print(f"  âš–ï¸ Bias ë¶„í¬: L({summary['bias_left_pct']}%) C({summary['bias_center_pct']}%) R({summary['bias_right_pct']}%)")
            print(f"  ğŸ¯ ì£¼ìš” Bias: {summary['dominant_bias']}")
            print()
        
        print("=" * 60)
    
    def run_clustering(self) -> bool:
        """ì „ì²´ í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            # ì‹œì‘ ì‹œê°„ ê¸°ë¡
            self.stats['start_time'] = datetime.now()
            
            print("ğŸš€ OPINION.IM ê¸°ì‚¬ í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
            print("=" * 60)
            
            # 1. Supabaseì—ì„œ ì„ë² ë”© ë°ì´í„° ë¡œë“œ
            df = self.load_embeddings_from_supabase()
            
            # 2. ì°¨ì› ì¶•ì†Œ
            embeddings_array = df['embedding'].values  # pandas Seriesì—ì„œ numpy ë°°ì—´ ì¶”ì¶œ
            embeddings_reduced = self.reduce_dimensions(embeddings_array)
            
            # 3. í´ëŸ¬ìŠ¤í„°ë§
            cluster_labels = self.cluster_articles(embeddings_reduced)
            
            # 4. í´ëŸ¬ìŠ¤í„° ë¶„ì„
            clusters_summary = self.analyze_clusters(df, cluster_labels)
            
            # 5. issues í…Œì´ë¸”ì— ì €ì¥ (GPT ìƒì„± í¬í•¨)
            cluster_issue_ids = self.create_issues(clusters_summary)
            
            # 6. ê¸°ì‚¬ë“¤ì˜ issue_id ì—…ë°ì´íŠ¸
            self.update_articles_issue_id(clusters_summary, cluster_issue_ids)
            
            # 7. ê²°ê³¼ ì¶œë ¥
            self.display_results(clusters_summary)
            
            # ì¢…ë£Œ ì‹œê°„ ê¸°ë¡
            self.stats['end_time'] = datetime.now()
            
            return True
            
        except Exception as e:
            self.logger.error(f"í´ëŸ¬ìŠ¤í„°ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {str(e)}")
            print(f"ğŸ’¥ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {str(e)}")
            return False


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        clusterer = ArticleClusterer()
        success = clusterer.run_clustering()
        
        if success:
            print("\nâœ… í´ëŸ¬ìŠ¤í„°ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            print("\nâŒ í´ëŸ¬ìŠ¤í„°ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
