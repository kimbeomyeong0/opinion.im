#!/usr/bin/env python3
"""
Issues í…Œì´ë¸” ë°ì´í„° ë³´ê°• ìŠ¤í¬ë¦½íŠ¸
- Keywords ì¶”ì¶œ ë° ì €ì¥
- Bias ë¶„ì„ ë° í†µê³„ ê³„ì‚°
- Subtitle ê°œì„ 
"""

import json
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.supabase_manager_unified import UnifiedSupabaseManager
from sklearn.feature_extraction.text import TfidfVectorizer


class IssuesEnricher:
    def __init__(self):
        self.sm = UnifiedSupabaseManager()
        
    def enrich_issues(self):
        """Issues í…Œì´ë¸”ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤."""
        print("ğŸ” Issues í…Œì´ë¸” ë°ì´í„° ë³´ê°• ì‹œì‘")
        
        # 1. ê¸°ì‚¬ì™€ í´ëŸ¬ìŠ¤í„° ì •ë³´ ë¡œë“œ
        articles, cluster_data = self._load_data()
        if not articles or not cluster_data:
            print("âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨")
            return False
            
        # 2. ê° í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë°ì´í„° ë³´ê°•
        for cluster_id, cluster_info in cluster_data.items():
            print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„° {cluster_id} ì²˜ë¦¬ ì¤‘...")
            
            # í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ê¸°ì‚¬ë“¤
            cluster_articles = [a for a in articles if a['issue_id'] == cluster_id]
            if not cluster_articles:
                continue
                
            # Keywords ì¶”ì¶œ
            keywords = self._extract_keywords(cluster_articles)
            
            # Bias ë¶„ì„
            bias_stats = self._analyze_bias(cluster_articles)
            
            # Subtitle ê°œì„ 
            subtitle = self._generate_subtitle(cluster_articles, keywords)
            
            # Issues í…Œì´ë¸” ì—…ë°ì´íŠ¸
            self._update_issue(cluster_id, keywords, bias_stats, subtitle, cluster_articles)
            
        print("\nâœ… Issues í…Œì´ë¸” ë°ì´í„° ë³´ê°• ì™„ë£Œ!")
        return True
        
    def _load_data(self) -> Tuple[List[Dict], Dict]:
        """ê¸°ì‚¬ì™€ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
            articles_result = self.sm.client.table('articles').select('*').execute()
            articles = articles_result.data
            
            # í´ëŸ¬ìŠ¤í„° ë°ì´í„° ë¡œë“œ (ID 1 ì œì™¸ - ê¸°ë³¸ ì´ìŠˆ)
            issues_result = self.sm.client.table('issues').select('*').gt('id', 1).execute()
            cluster_data = {issue['id']: issue for issue in issues_result.data}
            
            print(f"ğŸ“Š {len(articles)}ê°œ ê¸°ì‚¬, {len(cluster_data)}ê°œ í´ëŸ¬ìŠ¤í„° ë¡œë“œ ì™„ë£Œ")
            return articles, cluster_data
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return [], {}
            
    def _extract_keywords(self, articles: List[Dict], n_top: int = 10) -> List[str]:
        """í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì‚¬ë“¤ì—ì„œ TF-IDF í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ì œëª©ê³¼ ë‚´ìš© ê²°í•©
            texts = []
            for article in articles:
                title = article.get('title', '')
                content = article.get('content', '')
                if title and content:
                    texts.append(f"{title} {content}")
                    
            if not texts:
                return []
                
            # TF-IDF ë²¡í„°í™”
            cluster_size = len(texts)
            if cluster_size < 3:
                ngram_range = (1, 1)
                min_df = 1
            else:
                ngram_range = (1, 2)
                min_df = 2
                
            vectorizer = TfidfVectorizer(
                ngram_range=ngram_range,
                max_features=5000,
                min_df=min_df,
                stop_words=None
            )
            
            X = vectorizer.fit_transform(texts)
            vocab = np.array(vectorizer.get_feature_names_out())
            
            # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
            tfidf_scores = np.array(X.mean(axis=0)).flatten()
            
            # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
            top_indices = tfidf_scores.argsort()[-n_top:][::-1]
            keywords = vocab[top_indices].tolist()
            
            return keywords
            
        except Exception as e:
            print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
            
    def _analyze_bias(self, articles: List[Dict]) -> Dict:
        """í´ëŸ¬ìŠ¤í„°ì˜ í¸í–¥ì„± í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            bias_counts = Counter()
            total_articles = len(articles)
            
            for article in articles:
                bias = article.get('bias', 'center')
                bias_counts[bias.lower()] += 1
                
            # í¸í–¥ì„± ë¹„ìœ¨ ê³„ì‚° (ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ)
            bias_left_pct = round((bias_counts.get('left', 0) / total_articles) * 100, 1)
            bias_center_pct = round((bias_counts.get('center', 0) / total_articles) * 100, 1)
            bias_right_pct = round((bias_counts.get('right', 0) / total_articles) * 100, 1)
            
            # ì£¼ìš” í¸í–¥ì„± ê²°ì •
            dominant_bias = bias_counts.most_common(1)[0][0] if bias_counts else 'center'
            
            return {
                'bias_left_pct': bias_left_pct,
                'bias_center_pct': bias_center_pct,
                'bias_right_pct': bias_right_pct,
                'dominant_bias': dominant_bias.title()
            }
            
        except Exception as e:
            print(f"âŒ í¸í–¥ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'bias_left_pct': 0.0,
                'bias_center_pct': 100.0,
                'bias_right_pct': 0.0,
                'dominant_bias': 'Center'
            }
            
    def _generate_subtitle(self, articles: List[Dict], keywords: List[str]) -> str:
        """í´ëŸ¬ìŠ¤í„°ì˜ ì˜ë¯¸ìˆëŠ” ë¶€ì œëª©ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            if not keywords:
                return f"í´ëŸ¬ìŠ¤í„° {len(articles)}ê°œ ê¸°ì‚¬"
                
            # ì£¼ìš” í‚¤ì›Œë“œ 3ê°œë¡œ ë¶€ì œëª© ìƒì„±
            main_keywords = keywords[:3]
            subtitle = f"{', '.join(main_keywords)} ê´€ë ¨ ê¸°ì‚¬"
            
            return subtitle
            
        except Exception as e:
            print(f"âŒ ë¶€ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
            return f"í´ëŸ¬ìŠ¤í„° {len(articles)}ê°œ ê¸°ì‚¬"
            
    def _update_issue(self, cluster_id: int, keywords: List[str], bias_stats: Dict, subtitle: str, articles: List[Dict]):
        """Issues í…Œì´ë¸”ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            from datetime import datetime
            update_data = {
                'subtitle': subtitle,
                'dominant_bias': bias_stats['dominant_bias'],
                'bias_left_pct': bias_stats['bias_left_pct'],
                'bias_center_pct': bias_stats['bias_center_pct'],
                'bias_right_pct': bias_stats['bias_right_pct'],
                'source_count': len(articles),
                'updated_at': datetime.now().isoformat()
            }
                
            result = self.sm.client.table('issues').update(update_data).eq('id', cluster_id).execute()
            
            if result.data:
                print(f"âœ… í´ëŸ¬ìŠ¤í„° {cluster_id} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                print(f"   í‚¤ì›Œë“œ: {len(keywords)}ê°œ")
                print(f"   ì£¼ìš” í¸í–¥: {bias_stats['dominant_bias']}")
                print(f"   ë¶€ì œëª©: {subtitle}")
            else:
                print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° {cluster_id} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Issues í…Œì´ë¸” ë°ì´í„° ë³´ê°• ì‹œì‘")
    print("=" * 50)
    
    enricher = IssuesEnricher()
    success = enricher.enrich_issues()
    
    if success:
        print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì¼ë¶€ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
