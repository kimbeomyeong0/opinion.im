import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.table import Table
from rich.panel import Panel
import hashlib
from typing import List, Dict, Set
import logging

class NewsCrawler:
    def __init__(self, max_workers: int = 10, timeout: int = 10):
        self.max_workers = max_workers
        self.timeout = timeout
        self.console = Console()
        self.session = None
        self.visited_urls: Set[str] = set()
        self.news_data: List[Dict] = []
        self.duplicate_checker = set()
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    async def __aenter__(self):
        timeout_config = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(timeout=timeout_config)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _generate_hash(self, title: str, content: str) -> str:
        """ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±"""
        text = f"{title}{content}".lower().strip()
        return hashlib.md5(text.encode()).hexdigest()
    
    def _is_duplicate(self, title: str, content: str) -> bool:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì²´í¬"""
        content_hash = self._generate_hash(title, content)
        if content_hash in self.duplicate_checker:
            return True
        self.duplicate_checker.add(content_hash)
        return False
    
    async def fetch_page(self, url: str) -> str:
        """ì›¹í˜ì´ì§€ ë¹„ë™ê¸° í˜ì¹­"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    self.logger.warning(f"HTTP {response.status}: {url}")
                    return ""
        except asyncio.TimeoutError:
            self.logger.error(f"íƒ€ì„ì•„ì›ƒ: {url}")
            return ""
        except Exception as e:
            self.logger.error(f"ì—ëŸ¬ ë°œìƒ {url}: {str(e)}")
            return ""
    
    def parse_news(self, html: str, base_url: str) -> List[Dict]:
        """HTMLì—ì„œ ë‰´ìŠ¤ ë°ì´í„° íŒŒì‹±"""
        soup = BeautifulSoup(html, 'html.parser')
        news_items = []
        
        # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ êµ¬ì¡° íŒ¨í„´ë“¤
        selectors = [
            'article', '.news-item', '.article', '.post',
            '[class*="news"]', '[class*="article"]', '[class*="post"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    try:
                        # ì œëª© ì¶”ì¶œ
                        title_elem = element.find(['h1', 'h2', 'h3', 'h4']) or element.find(class_=lambda x: x and ('title' in x.lower() or 'headline' in x.lower()))
                        title = title_elem.get_text(strip=True) if title_elem else ""
                        
                        # ë‚´ìš© ì¶”ì¶œ
                        content_elem = element.find(['p', 'div']) or element.find(class_=lambda x: x and ('content' in x.lower() or 'body' in x.lower()))
                        content = content_elem.get_text(strip=True) if content_elem else ""
                        
                        # ë§í¬ ì¶”ì¶œ
                        link_elem = element.find('a')
                        link = urljoin(base_url, link_elem.get('href', '')) if link_elem else ""
                        
                        if title and content and not self._is_duplicate(title, content):
                            news_items.append({
                                'title': title,
                                'content': content[:200] + '...' if len(content) > 200 else content,
                                'link': link,
                                'source': urlparse(base_url).netloc
                            })
                    except Exception as e:
                        self.logger.debug(f"íŒŒì‹± ì—ëŸ¬: {str(e)}")
                        continue
        
        return news_items
    
    async def crawl_urls(self, urls: List[str]) -> List[Dict]:
        """URL ëª©ë¡ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        start_time = time.time()
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console
        ) as progress:
            
            task = progress.add_task("í¬ë¡¤ë§ ì§„í–‰ë¥ ...", total=len(urls))
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # ë¹„ë™ê¸° ì‘ì—…ì„ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                futures = []
                
                for url in urls:
                    if url not in self.visited_urls:
                        self.visited_urls.add(url)
                        future = loop.run_in_executor(executor, lambda u=url: asyncio.run(self._crawl_single_url(u)))
                        futures.append(future)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in asyncio.as_completed(futures):
                    try:
                        result = await future
                        if result:
                            self.news_data.extend(result)
                        progress.advance(task)
                    except Exception as e:
                        self.logger.error(f"í¬ë¡¤ë§ ì—ëŸ¬: {str(e)}")
                        progress.advance(task)
        
        elapsed_time = time.time() - start_time
        self.console.print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        return self.news_data
    
    async def _crawl_single_url(self, url: str) -> List[Dict]:
        """ë‹¨ì¼ URL í¬ë¡¤ë§"""
        html = await self.fetch_page(url)
        if html:
            return self.parse_news(html, url)
        return []
    
    def display_results(self):
        """ê²°ê³¼ë¥¼ Richë¡œ í‘œì‹œ"""
        if not self.news_data:
            self.console.print("âŒ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í†µê³„ ì •ë³´
        stats_panel = Panel(
            f"ì´ {len(self.news_data)}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘\n"
            f"ì¤‘ë³µ ì œê±°: {len(self.duplicate_checker)}ê°œ\n"
            f"ë°©ë¬¸í•œ URL: {len(self.visited_urls)}ê°œ",
            title="ğŸ“Š í¬ë¡¤ë§ í†µê³„",
            border_style="blue"
        )
        self.console.print(stats_panel)
        
        # ë‰´ìŠ¤ í…Œì´ë¸”
        table = Table(title="ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤")
        table.add_column("ì œëª©", style="cyan", width=40)
        table.add_column("ë‚´ìš©", style="white", width=60)
        table.add_column("ì¶œì²˜", style="green", width=20)
        table.add_column("ë§í¬", style="blue", width=30)
        
        for news in self.news_data[:20]:  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
            table.add_row(
                news['title'][:40] + '...' if len(news['title']) > 40 else news['title'],
                news['content'][:60] + '...' if len(news['content']) > 60 else news['content'],
                news['source'],
                news['link'][:30] + '...' if len(news['link']) > 30 else news['link']
            )
        
        self.console.print(table)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ìš© ì–¸ë¡ ì‚¬ URLë“¤
    test_urls = [
        "https://www.yna.co.kr/",
        "https://www.hani.co.kr/",
        "https://www.khan.co.kr/",
        "https://www.donga.com/",
        "https://www.chosun.com/",
        "https://www.joongang.co.kr/",
        "https://www.seoul.co.kr/",
        "https://www.kmib.co.kr/",
        "https://www.munhwa.com/",
        "https://www.kyunghyang.com/"
    ]
    
    console = Console()
    console.print(Panel("ğŸš€ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘", style="bold blue"))
    
    async with NewsCrawler(max_workers=8, timeout=10) as crawler:
        news_data = await crawler.crawl_urls(test_urls)
        crawler.display_results()
        
        # ê²°ê³¼ ì €ì¥
        if news_data:
            import json
            with open('crawled_news.json', 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            console.print("ğŸ’¾ ê²°ê³¼ê°€ 'crawled_news.json' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main())

