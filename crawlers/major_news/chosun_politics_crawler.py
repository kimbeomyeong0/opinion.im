import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.layout import Layout
from rich.columns import Columns
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
from utils.supabase_manager_unified import UnifiedSupabaseManager
import json
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ChosunPoliticsCrawler:
    def __init__(self):
        self.console = Console()
        self.supabase = UnifiedSupabaseManager()
        self.base_url = "https://www.chosun.com"
        self.politics_url = "https://www.chosun.com/politics/"
        self.session: Optional[aiohttp.ClientSession] = None
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.max_articles = 100
        self.max_workers = 20  # ë™ì‹œ ìš”ì²­ ìˆ˜
        self.timeout = 5  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ
        self.delay = 0.1  # ìš”ì²­ ê°„ ì§€ì—°
        
        # ê²°ê³¼ ì €ì¥
        self.articles = []
        self.stats = {
            'total_found': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        connector = aiohttp.TCPConnector(limit=self.max_workers, limit_per_host=10)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def get_politics_article_links(self) -> List[str]:
        """ì •ì¹˜ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ì—¬ëŸ¬ ì¹´í…Œê³ ë¦¬ í¬í•¨)"""
        all_article_links = []
        
        # í¬ë¡¤ë§í•  ì •ì¹˜ ì¹´í…Œê³ ë¦¬ë“¤
        politics_categories = [
            '',  # ë©”ì¸ ì •ì¹˜ í˜ì´ì§€
            'politics_general/',  # ì •ì¹˜ì¼ë°˜
            'blue_house/',        # ì²­ì™€ëŒ€  
            'assembly/',          # êµ­íšŒ
            'diplomacy-defense/', # ì™¸êµêµ­ë°©
            'north_korea/',       # ë¶í•œ
        ]
        
        # Playwrightë¥¼ ì‚¬ìš©í•œ "ê¸°ì‚¬ ë”ë³´ê¸°" ê¸°ëŠ¥ í™œìš©
        self.console.print(f"[cyan]ğŸ” Playwrightë¡œ 'ê¸°ì‚¬ ë”ë³´ê¸°' ê¸°ëŠ¥ í™œìš© ì¤‘...[/cyan]")
        try:
            more_links = await self._get_more_articles_with_playwright()
            all_article_links.extend(more_links)
            self.console.print(f"[green]  - Playwright 'ë”ë³´ê¸°'ì—ì„œ {len(more_links)}ê°œ ì¶”ê°€ ë§í¬ ë°œê²¬[/green]")
        except Exception as e:
            self.console.print(f"[red]  - Playwright 'ë”ë³´ê¸°' í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}[/red]")
        
        # ë‚ ì§œë³„ ì¶”ê°€ í˜ì´ì§€ë“¤ (ìµœê·¼ 2ì£¼)
        from datetime import datetime, timedelta
        today = datetime.now()
        for i in range(14):  # ìµœê·¼ 2ì£¼
            date = today - timedelta(days=i)
            date_str = date.strftime('%Y/%m/%d')
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‚ ì§œ í˜ì´ì§€ ì¶”ê°€
            for main_category in ['politics_general', 'blue_house', 'assembly', 'diplomacy-defense']:
                date_url = f"{main_category}/{date_str}/"
                politics_categories.append(date_url)
                
            # ë©”ì¸ ì •ì¹˜ í˜ì´ì§€ì˜ ë‚ ì§œë³„ í˜ì´ì§€ë„ ì‹œë„
            main_date_url = f"{date_str}/"
            politics_categories.append(main_date_url)
        
        for category in politics_categories:
            category_url = f"{self.politics_url}{category}"
            self.console.print(f"[cyan]ğŸ” {category_url} í¬ë¡¤ë§ ì¤‘...[/cyan]")
            
            try:
                category_links = await self._get_links_from_page(category_url)
                all_article_links.extend(category_links)
                self.console.print(f"[green]  - {len(category_links)}ê°œ ë§í¬ ë°œê²¬[/green]")
                
                # ìš”ì²­ ê°„ ë”œë ˆì´
                await asyncio.sleep(self.delay)
                
            except Exception as e:
                self.console.print(f"[red]  - {category} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}[/red]")
                logger.error(f"{category} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_links = list(set(all_article_links))
        valid_links = [link for link in unique_links if self._is_valid_article_url(link)]
        
        # ìµœì‹  ê¸°ì‚¬ ìš°ì„  ì •ë ¬ (URLì— ë‚ ì§œê°€ í¬í•¨ë˜ì–´ ìˆìœ¼ë¯€ë¡œ)
        valid_links.sort(reverse=True)
        
        self.console.print(f"[bold green]ì´ {len(valid_links)}ê°œ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬[/bold green]")
        return valid_links[:self.max_articles]
    
    async def _get_links_from_page(self, url: str) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ë”ë³´ê¸° ê¸°ëŠ¥ í¬í•¨)"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                article_links = []
                
                # ë°©ë²• 1: feed-item ì•ˆì˜ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ê°€ì¥ ì •í™•)
                feed_items = soup.select('.feed-item')
                for item in feed_items:
                    headline_link = item.select_one('.story-card__headline[href*="/politics/"]')
                    if headline_link:
                        href = headline_link.get('href')
                        if href:
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            else:
                                full_url = href
                            if full_url not in article_links and self._is_valid_article_url(full_url):
                                article_links.append(full_url)
                
                # ë°©ë²• 2: story-card-component í´ë˜ìŠ¤ë¥¼ ê°€ì§„ div ì•ˆì˜ ë§í¬
                story_cards = soup.select('.story-card-component a[href*="/politics/"]')
                for link in story_cards:
                    href = link.get('href')
                    if href and '/politics/' in href:
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        else:
                            full_url = href
                        if full_url not in article_links:
                            article_links.append(full_url)
                
                # ë°©ë²• 3: ëª¨ë“  ì •ì¹˜ ê´€ë ¨ ë§í¬
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    href = link.get('href')
                    if href and '/politics/' in href and len(href) > 20:
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        else:
                            full_url = href
                        if full_url not in article_links and self._is_valid_article_url(full_url):
                            article_links.append(full_url)
                
                # ë°©ë²• 4: íŠ¹ì • í´ë˜ìŠ¤ë‚˜ êµ¬ì¡° ê¸°ë°˜ ë§í¬ ì¶”ì¶œ
                content_links = soup.select('a[href*="/2025/"], a[href*="/2024/"]')
                for link in content_links:
                    href = link.get('href')
                    if href and '/politics/' in href:
                        if href.startswith('/'):
                            full_url = urljoin(self.base_url, href)
                        else:
                            full_url = href
                        if full_url not in article_links and self._is_valid_article_url(full_url):
                            article_links.append(full_url)
                
                return article_links
                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {url} ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    

    
    async def _get_more_articles_with_playwright(self) -> List[str]:
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ 'ê¸°ì‚¬ ë”ë³´ê¸°' ë²„íŠ¼ì„ í´ë¦­í•˜ê³  ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘"""
        additional_links = []
        
        try:
            async with async_playwright() as p:
                # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless ëª¨ë“œ)
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # ì¡°ì„ ì¼ë³´ ì •ì¹˜ í˜ì´ì§€ ì ‘ì†
                await page.goto(self.politics_url, wait_until='networkidle')
                
                # ì´ˆê¸° ê¸°ì‚¬ ìˆ˜ í™•ì¸
                initial_articles = await page.query_selector_all('.feed-item')
                self.console.print(f"[cyan]  - ì´ˆê¸° ê¸°ì‚¬ ìˆ˜: {len(initial_articles)}ê°œ[/cyan]")
                
                # "ê¸°ì‚¬ ë”ë³´ê¸°" ë²„íŠ¼ì„ ì—¬ëŸ¬ ë²ˆ í´ë¦­í•˜ì—¬ ë” ë§ì€ ê¸°ì‚¬ ë¡œë“œ
                max_clicks = 10  # ìµœëŒ€ 10ë²ˆ í´ë¦­ ì‹œë„
                click_count = 0
                
                for i in range(max_clicks):
                    try:
                        # "ê¸°ì‚¬ ë”ë³´ê¸°" ë²„íŠ¼ ì°¾ê¸°
                        load_more_button = await page.query_selector('#load-more-stories')
                        
                        if load_more_button:
                            # ë²„íŠ¼ì´ ë³´ì´ëŠ”ì§€ í™•ì¸
                            is_visible = await load_more_button.is_visible()
                            
                            if is_visible:
                                # ë²„íŠ¼ í´ë¦­
                                await load_more_button.click()
                                click_count += 1
                                
                                # ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                                await page.wait_for_timeout(2000)  # 2ì´ˆ ëŒ€ê¸°
                                
                                # í˜„ì¬ ê¸°ì‚¬ ìˆ˜ í™•ì¸
                                current_articles = await page.query_selector_all('.feed-item')
                                self.console.print(f"[cyan]  - {click_count}ë²ˆì§¸ í´ë¦­ í›„: {len(current_articles)}ê°œ ê¸°ì‚¬[/cyan]")
                                
                                # ë” ì´ìƒ ê¸°ì‚¬ê°€ ëŠ˜ì–´ë‚˜ì§€ ì•Šìœ¼ë©´ ì¤‘ë‹¨
                                if len(current_articles) <= len(initial_articles):
                                    break
                                    
                                initial_articles = current_articles
                            else:
                                break
                        else:
                            break
                    
            except Exception as e:
                        self.console.print(f"[yellow]  - {i+1}ë²ˆì§¸ í´ë¦­ ì‹¤íŒ¨: {str(e)}[/yellow]")
                        break
                
                # ìµœì¢…ì ìœ¼ë¡œ ëª¨ë“  ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
                final_articles = await page.query_selector_all('.feed-item')
                self.console.print(f"[green]  - ìµœì¢… ê¸°ì‚¬ ìˆ˜: {len(final_articles)}ê°œ[/green]")
                
                # ê° ê¸°ì‚¬ì—ì„œ ë§í¬ ì¶”ì¶œ
                for article in final_articles:
                    try:
                        headline_link = await article.query_selector('.story-card__headline[href*="/politics/"]')
                        if headline_link:
                            href = await headline_link.get_attribute('href')
                            if href:
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                else:
                                    full_url = href
                                if full_url not in additional_links and self._is_valid_article_url(full_url):
                                    additional_links.append(full_url)
                    except Exception as e:
                continue
                
                await browser.close()
                
        except Exception as e:
            self.console.print(f"[red]  - Playwright ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}[/red]")
            logger.error(f"Playwright ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        
        return additional_links
    
    def _is_valid_article_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
        # ì œì™¸í•  íŒ¨í„´ë“¤
        exclude_patterns = [
            '#', 'javascript:', 'mailto:', 'tel:',
            '/tag/', '/author/', '/category/',
            '/search', '/archive', '/print'
        ]
        
        for pattern in exclude_patterns:
            if pattern in url.lower():
                return False
        
        # í¬í•¨í•´ì•¼ í•  íŒ¨í„´
        include_patterns = ['/politics/', '/article/']
        has_valid_pattern = any(pattern in url for pattern in include_patterns)
        
        return has_valid_pattern and len(url) > 30
    
    async def crawl_article(self, url: str) -> Optional[Dict]:
        """ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
            # ì œëª© ì¶”ì¶œ
                title = self._extract_title(soup)
                if not title:
                    return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
                content = self._extract_content(soup)
                if not content:
                    return None
                
                # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
                published_at = self._extract_published_time(soup)
                
                article_data = {
                'title': title,
                    'url': url,
                'content': content,
                    'published_at': published_at,
                    'media_name': 'ì¡°ì„ ì¼ë³´'
            }
            
                return article_data
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        # 1. title íƒœê·¸ì—ì„œ ì¶”ì¶œ
        title_elem = soup.find('title')
        if title_elem:
            title = title_elem.get_text(strip=True)
            if title and len(title) > 5:
                return title
        
        # 2. meta og:titleì—ì„œ ì¶”ì¶œ
        og_title = soup.find('meta', property='og:title')
        if og_title:
            title = og_title.get('content', '')
            if title and len(title) > 5:
                return title
        
        # 3. ê¸°íƒ€ ì œëª© ìš”ì†Œë“¤
        title_selectors = [
            'h1.article-title',
            'h1.title',
            '.article-header h1',
            'h1',
            '.headline h1'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title and len(title) > 5:
                    return title
        
        return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        # 1. JavaScript ë³€ìˆ˜ì—ì„œ ì½˜í…ì¸  ì¶”ì¶œ ì‹œë„
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string and 'Fusion.globalContent' in script.string:
                try:
                    # content_elements ë°°ì—´ ë¶€ë¶„ ì¶”ì¶œ
                    start_idx = script.string.find('"content_elements":[')
                    if start_idx != -1:
                        # ë°°ì—´ì˜ ë ì°¾ê¸° (ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶”ê¸°)
                        bracket_count = 0
                        end_idx = start_idx
                        in_string = False
                        escape_next = False
                        
                        for j in range(start_idx, len(script.string)):
                            char = script.string[j]
                            
                            if escape_next:
                                escape_next = False
                                continue
                            
                            if char == '\\':
                                escape_next = True
                                continue
                            
                            if char == '"' and not escape_next:
                                in_string = not in_string
                                continue
                            
                            if not in_string:
                                if char == '[':
                                    bracket_count += 1
                                elif char == ']':
                                    bracket_count -= 1
                                    if bracket_count == 0:
                                        end_idx = j + 1
                        break
                    
                        if end_idx > start_idx:
                            content_elements_str = script.string[start_idx:end_idx]
                            
                            # JSON íŒŒì‹± ì‹œë„
                            try:
                                # content_elements ë¶€ë¶„ì„ JSONìœ¼ë¡œ íŒŒì‹±
                                content_elements_json = '{' + content_elements_str + '}'
                                parsed = json.loads(content_elements_json)
                                
                                if 'content_elements' in parsed:
                                    elements = parsed['content_elements']
                                    
                                    # í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì¶”ì¶œ
                                    text_contents = []
                                    for elem in elements:
                                        if isinstance(elem, dict) and elem.get('type') == 'text':
                                            content = elem.get('content', '')
                                            if content and len(content) > 10:
                                                text_contents.append(content)
                                    
                                    if text_contents:
                                        content = '\n\n'.join(text_contents)
                                        if len(content) > 100:
                                            return content
                                            
                            except json.JSONDecodeError:
                                continue
                    
                except Exception as e:
                    logger.debug(f"JavaScript íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    continue
        
        # 2. HTML ìš”ì†Œì—ì„œ ë³¸ë¬¸ ì°¾ê¸°
        content_selectors = [
            '.content', '.body', '.article-content', '.article-body',
            '.text', '.story-content', '.article-text',
            '.content-text', '.story-body',
            'article', '.main-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for elem in content_elem.select('script, style, .ad, .advertisement, .related-articles'):
                    elem.decompose()
                
                content = content_elem.get_text(separator='\n', strip=True)
                if content and len(content) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                    return content
        
        return None
    
    def _extract_published_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ë°œí–‰ ì‹œê°„ ì¶”ì¶œ"""
        # 1. meta íƒœê·¸ì—ì„œ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
        time_meta = soup.find('meta', {'name': 'article:published_time'})
        if time_meta:
            time_str = time_meta.get('content', '')
            if time_str:
                try:
                    # ISO 8601 í˜•ì‹ íŒŒì‹±
                    dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                    return dt
                except Exception as e:
                    logger.debug(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
                    return None
        
        # 2. ê¸°íƒ€ ì‹œê°„ ìš”ì†Œë“¤
        time_selectors = [
            '.time', '.date', '.published-time', '.article-time',
            '.story-time', '.timestamp', '.publish-date',
            'time', '.upDate', '.story-date'
        ]
        
        for selector in time_selectors:
            time_elem = soup.select_one(selector)
            if time_elem:
                time_text = time_elem.get_text(strip=True)
                if time_text:
                    # ê°„ë‹¨í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹± ì‹œë„
                    try:
                        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                        if 'T' in time_text and 'Z' in time_text:
                            # ISO í˜•ì‹: 2025-08-20T06:05:57.039Z
                            dt = datetime.fromisoformat(time_text.replace('Z', '+00:00'))
                            return dt
                        elif len(time_text) == 19 and time_text.count('-') == 2 and time_text.count(':') == 2:
                            # YYYY-MM-DD HH:MM:SS í˜•ì‹
                            dt = datetime.strptime(time_text, '%Y-%m-%d %H:%M:%S')
                            return dt
                        elif len(time_text) == 10 and time_text.count('-') == 2:
                            # YYYY-MM-DD í˜•ì‹
                            dt = datetime.strptime(time_text, '%Y-%m-%d')
                            return dt
                    except Exception as e:
                        logger.debug(f"ì‹œê°„ í˜•ì‹ íŒŒì‹± ì‹¤íŒ¨: {time_text} - {str(e)}")
                        continue
        
        return None
    
    async def crawl_all_articles(self, article_links: List[str]):
        """ëª¨ë“  ê¸°ì‚¬ í¬ë¡¤ë§"""
        self.stats['start_time'] = time.time()
        
        # ì§„í–‰ë¥  í‘œì‹œ
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            
            task = progress.add_task("ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...", total=len(article_links))
            
            # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
            semaphore = asyncio.Semaphore(self.max_workers)
            
            async def crawl_with_semaphore(url):
                async with semaphore:
                    article = await self.crawl_article(url)
                    if article:
                        self.articles.append(article)
                        self.stats['successful'] += 1
                    else:
                        self.stats['failed'] += 1
                    
                    progress.advance(task)
                    
                    # ì§€ì—° ì‹œê°„
                    await asyncio.sleep(self.delay)
            
            # ëª¨ë“  ê¸°ì‚¬ í¬ë¡¤ë§
            tasks = [crawl_with_semaphore(url) for url in article_links]
            await asyncio.gather(*tasks)
        
        self.stats['end_time'] = time.time()
        self.stats['total_found'] = len(article_links)
    
    def display_results(self):
        """ê²°ê³¼ í‘œì‹œ"""
        duration = self.stats['end_time'] - self.stats['start_time']
        
        # í†µê³„ íŒ¨ë„
        stats_panel = Panel(
            f"â±ï¸  í¬ë¡¤ë§ ì‹œê°„: {duration:.2f}ì´ˆ\n"
            f"ğŸ“° ë°œê²¬ëœ ê¸°ì‚¬: {self.stats['total_found']}ê°œ\n"
            f"âœ… ì„±ê³µ: {self.stats['successful']}ê°œ\n"
            f"âŒ ì‹¤íŒ¨: {self.stats['failed']}ê°œ\n"
            f"ğŸš€ ì†ë„: {self.stats['successful']/duration:.1f} ê¸°ì‚¬/ì´ˆ",
            title="ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼",
            border_style="green"
        )
        
        self.console.print(stats_panel)
        
        # ì„±ê³µí•œ ê¸°ì‚¬ ëª©ë¡
        if self.articles:
            table = Table(title="ğŸ“° í¬ë¡¤ë§ëœ ê¸°ì‚¬ ëª©ë¡", show_header=True, header_style="bold magenta")
            table.add_column("ë²ˆí˜¸", style="cyan", width=5)
            table.add_column("ì œëª©", style="white", width=60)
            table.add_column("ê¸¸ì´", style="green", width=10)
            table.add_column("ì‹œê°„", style="yellow", width=15)
            
            for i, article in enumerate(self.articles[:20], 1):  # ìƒìœ„ 20ê°œë§Œ í‘œì‹œ
                title = article['title'][:55] + "..." if len(article['title']) > 55 else article['title']
                content_length = len(article['content'])
                
                # ì‹œê°„ í‘œì‹œ ì²˜ë¦¬
                if article['published_at']:
                    if isinstance(article['published_at'], datetime):
                        time_str = article['published_at'].strftime('%Y-%m-%d %H:%M')
                    else:
                        time_str = str(article['published_at'])
            else:
                    time_str = "ì‹œê°„ ì •ë³´ ì—†ìŒ"
                
                table.add_row(
                    str(i),
                    title,
                    f"{content_length:,}ì",
                    time_str
                )
            
            self.console.print(table)
            
            if len(self.articles) > 20:
                self.console.print(f"[dim]... ë° {len(self.articles) - 20}ê°œ ë”[/dim]")
    
    async def save_to_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        if not self.articles:
            self.console.print("[yellow]ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return
        
        self.console.print("\n[blue]ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...[/blue]")
        
        # ì¡°ì„ ì¼ë³´ ì–¸ë¡ ì‚¬ ID ê°€ì ¸ì˜¤ê¸°
        media_outlet = self.supabase.get_media_outlet('ì¡°ì„ ì¼ë³´')
        if not media_outlet:
            self.console.print("[red]ì¡°ì„ ì¼ë³´ ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return
        
        media_id = media_outlet['id']
        
        # í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œëŠ” issue_idë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ (í´ëŸ¬ìŠ¤í„°ë§ í›„ ì„¤ì •)
        # ì„ì‹œ ì´ìŠˆ ID 6 ì‚¬ìš© (ë°ì´í„°ë² ì´ìŠ¤ ì œì•½ì¡°ê±´ ì¤€ìˆ˜)
        issue_id = 6
        
        # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
        articles_to_save = []
        for article in self.articles:
            article_data = {
                'issue_id': issue_id,
                'media_id': media_id,
                'title': article['title'],
                'url': article['url'],
                'content': article['content'],
                'bias': media_outlet['bias'],
                'published_at': article['published_at']
            }
            articles_to_save.append(article_data)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        result = self.supabase.insert_articles_batch(articles_to_save)
        
        if result['success'] > 0:
            self.console.print(f"[green]âœ… {result['success']}ê°œ ê¸°ì‚¬ ì €ì¥ ì„±ê³µ![/green]")
        if result['failed'] > 0:
            self.console.print(f"[red]âŒ {result['failed']}ê°œ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨[/red]")
        
        # ì´ìŠˆ í¸í–¥ì„± ì—…ë°ì´íŠ¸
        bias_data = {'right': result['success']}  # ì¡°ì„ ì¼ë³´ëŠ” ìš°íŒŒ
        self.supabase.update_issue_bias(issue_id, bias_data)

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    console = Console()
    
    # ì‹œì‘ ë©”ì‹œì§€
    console.print(Panel(
        "[bold blue]ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬[/bold blue]\n"
        "ğŸš€ ìµœì‹  ì •ì¹˜ ê¸°ì‚¬ 100ê°œë¥¼ ë¹ ë¥´ê²Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
        border_style="blue"
    ))
    
    async with ChosunPoliticsCrawler() as crawler:
        # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        console.print("\n[cyan]ğŸ” ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...[/cyan]")
        article_links = await crawler.get_politics_article_links()
        
        if not article_links:
            console.print("[red]ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return
        
        # 2. ê¸°ì‚¬ í¬ë¡¤ë§
        console.print(f"\n[cyan]ğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...[/cyan]")
        await crawler.crawl_all_articles(article_links)
        
        # 3. ê²°ê³¼ í‘œì‹œ
        crawler.display_results()
        
        # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        await crawler.save_to_database()
        
        # ì™„ë£Œ ë©”ì‹œì§€
        console.print("\n[bold green]ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ![/bold green]")

if __name__ == "__main__":
    asyncio.run(main())
