#!/usr/bin/env python3
"""
ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ìµœì í™”ëœ ë²„ì „)
í’ˆì§ˆ ìš°ì„ : Playwrightë¥¼ ì‚¬ìš©í•œ ë³¸ë¬¸ ìˆ˜ì§‘
"""

import asyncio
import json
import re
import httpx
import random
import traceback
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table

# í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ
from utils.supabase_manager_unified import UnifiedSupabaseManager
from utils.common import make_request

console = Console()

class ChosunPoliticsCollector:
    """ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘ê¸° - í’ˆì§ˆ ìš°ì„ """
    
    def __init__(self):
        self.base_url = "https://www.chosun.com"
        self.politics_url = "https://www.chosun.com/politics/"
        self.media_name = "ì¡°ì„ ì¼ë³´"
        self.media_bias = "Right"
        
        # ì„¤ì •
        self.CONFIG = {
            "target_count": 100,
            "timeout": 10,
            "max_retries": 3,
            "concurrent_limit": 3
        }
        
        # HTTP í—¤ë”
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # ìƒíƒœ ë³€ìˆ˜
        self.articles = []
        self.today = datetime.now().strftime('%Y-%m-%d')
        
        # ë‚ ì§œ ë²”ìœ„ (ìµœê·¼ 7ì¼)
        self.date_range = []
        for i in range(7):
            date = (datetime.now() - timedelta(days=i)).strftime('%Y-%m-%d')
            self.date_range.append(date)
        
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.supabase_manager = UnifiedSupabaseManager()
        
        # Playwright ê´€ë ¨
        self._playwright = None
        self._browser = None

    async def _make_request(self, url: str, params: Optional[Dict] = None) -> Optional[str]:
        """HTTP GET ìš”ì²­ ìˆ˜í–‰"""
        return await make_request(url, "httpx", "GET", params=params, headers=self.headers, timeout=self.CONFIG["timeout"])

    async def _collect_from_html(self):
        """HTMLì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        try:
            console.print("ğŸ“° HTMLì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
            html = await self._make_request(self.politics_url)
            if not html:
                console.print("âŒ HTML ìˆ˜ì§‘ ì‹¤íŒ¨")
                return
            
            # JSON ë°ì´í„° ì¶”ì¶œ ì‹œë„
            articles = self._extract_json_data_from_html(html)
            
            # JSONì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ HTML ì§ì ‘ íŒŒì‹±
            if not articles:
                articles = self._extract_articles_from_html_direct(html)
            
            # ê¸°ì‚¬ ì¶”ê°€
            for article in articles:
                if self._add_article_to_collection(article):
                    continue
            
            console.print(f"âœ… HTML ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ (ì´ {len(self.articles)}ê°œ)")
            
        except Exception as e:
            console.print(f"âŒ HTML ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")

    def _extract_json_data_from_html(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ JSON ë°ì´í„° ì¶”ì¶œ"""
        articles = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            scripts = soup.find_all('script')
            
            for script in scripts:
                if not script.string:
                    continue
                
                script_text = script.string
                
                if 'content_elements' in script_text and 'headlines' in script_text:
                    try:
                        start_idx = script_text.find('{')
                        end_idx = script_text.rfind('}') + 1
                        
                        if start_idx != -1 and end_idx != -1:
                            json_str = script_text[start_idx:end_idx]
                            data = json.loads(json_str)
                            articles.extend(self._parse_content_elements(data))
                    except (json.JSONDecodeError, Exception) as e:
                        continue
            
            return articles
            
        except Exception as e:
            console.print(f"âŒ JSON ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []

    def _extract_articles_from_html_direct(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ì§ì ‘ ê¸°ì‚¬ ì¶”ì¶œ"""
        articles = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ìƒë‹¨ ê³ ì • ê¸°ì‚¬
            flex_chains = soup.find_all('section', class_='flex-chain')
            for chain in flex_chains:
                story_cards = chain.find_all('div', class_='story-card-container')
                for card in story_cards:
                    article = self._extract_single_article(card)
                    if article:
                        articles.append(article)
            
            # ì¼ë°˜ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            feed_items = soup.find_all('div', class_='feed-item')
            for item in feed_items:
                article = self._extract_single_article(item)
                if article:
                    articles.append(article)
            
            return articles
            
        except Exception as e:
            console.print(f"âŒ HTML ì§ì ‘ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
            return []

    def _extract_single_article(self, container) -> Optional[Dict]:
        """ê°œë³„ ê¸°ì‚¬ ì¶”ì¶œ"""
        try:
            headline = container.find('a', class_='story-card__headline')
            if not headline:
                return None
            
            title = self._clean_title(headline.get_text(strip=True))
            if not title or len(title) < 5:
                return None
            
            url = headline.get('href', '')
            if not url:
                return None
            
            if url.startswith('/'):
                url = urljoin(self.base_url, url)
            
            # ìš”ì•½ ì¶”ì¶œ
            deck = container.find('div', class_='story-card__deck')
            content = deck.get_text(strip=True) if deck else ""
            
            # ë‚ ì§œ ì¶”ì¶œ
            datetime_elem = container.find('div', class_='story-card__sigline-datetime')
            date = ""
            if datetime_elem:
                time_text = datetime_elem.find('div', class_='text')
                if time_text:
                    date = self._parse_relative_time(time_text.get_text(strip=True))
            
            if not date:
                date = self.today
            
            # ê¸°ì ì¶”ì¶œ
            author_elem = container.find('span', class_='story-card__sigline-author')
            author = author_elem.get_text(strip=True) if author_elem else ""
            
            # ìƒë‹¨ ê³ ì • ê¸°ì‚¬ ì—¬ë¶€
            is_top = container.find_parent('section', class_='flex-chain') is not None
            
            return {
                'title': title,
                'url': url,
                'content': content,
                'date': date,
                'author': author,
                'is_top': is_top
            }
            
        except Exception as e:
            return None

    async def _collect_from_api(self):
        """APIë¥¼ í†µí•œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘"""
        try:
            console.print("ğŸ”Œ APIë¥¼ í†µí•œ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹œì‘...")
            
            api_base = "https://www.chosun.com/pf/api/v3/content/fetch/story-feed"
            offset = 20
            size = 50
            
            while len(self.articles) < self.CONFIG["target_count"] and offset < 2000:
                try:
                    query_params = {
                        "query": json.dumps({
                            "excludeContentTypes": "gallery, video",
                            "includeContentTypes": "story",
                            "includeSections": "/politics",
                            "offset": offset,
                            "size": size
                        }),
                        "filter": "{content_elements{_id,canonical_url,credits{by{_id,additional_properties{original{affiliations,byline}},name,org,url}},description{basic},display_date,headlines{basic,mobile},label{membership_icon{text}},last_updated_date,promo_items{basic{_id,additional_properties{focal_point{max,min}},alt_text,caption,content,content_elements{_id,alignment,alt_text,caption,content,credits{affiliation{name},by{_id,byline,name,org}},height,resizedUrls{16x9_lg,16x9_md,16x9_sm,16x9_xxl,4x3_lg,4x3_md,4x3_sm,4x3_xxl},subtype,type,url,width},credits{affiliation{byline,name},by{byline,name}},description{basic},embed_html,focal_point{x,y},headlines{basic},height,promo_items{basic{_id,height,resizedUrls{16x9_lg,16x9_md,16x9_sm,16x9_xxl,4x3_lg,4x3_md,4x3_sm,4x3_xxl},subtype,type,url,width}},resizedUrls{16x9_lg,16x9_md,16x9_sm,16x9_xxl,4x3_lg,4x3_md,4x3_sm,4x3_xxl},streams{height,width},subtype,type,url,websites,width},lead_art{duration,type}},related_content{basic{_id,absolute_canonical_url,headlines{basic,mobile},referent{id,type},type}},subtype,taxonomy{primary_section{_id,name},tags{slug,text}},type,website_url},count,next}",
                        "d": "1912",
                        "mxId": "00000000",
                        "_website": "chosun"
                    }
                    
                    async with httpx.AsyncClient(timeout=10.0) as client:
                        response = await client.get(api_base, params=query_params)
                        response.raise_for_status()
                        data = response.json()
                        
                        content_elements = data.get('content_elements', [])
                        if not content_elements:
                            break
                        
                        new_articles = 0
                        for element in content_elements:
                            if len(self.articles) >= self.CONFIG["target_count"]:
                                break
                            
                            article = self._parse_api_article(element)
                            if article and self._add_article_to_collection(article):
                                new_articles += 1
                        
                        console.print(f"âœ… API í˜¸ì¶œ (offset: {offset}): {new_articles}ê°œ ê¸°ì‚¬ ì¶”ê°€ (ì´ {len(self.articles)}ê°œ)")
                        
                        if new_articles == 0:
                            break
                        
                        offset += size
                        await asyncio.sleep(0.05)
                        
                except Exception as e:
                    console.print(f"âŒ API í˜¸ì¶œ ì˜¤ë¥˜ (offset: {offset}): {str(e)}")
                    offset += size
                    continue
            
            console.print(f"ğŸ¯ API ìˆ˜ì§‘ ì™„ë£Œ: ì´ {len(self.articles)}ê°œ ê¸°ì‚¬")
            
        except Exception as e:
            console.print(f"âŒ API ìˆ˜ì§‘ ì „ì²´ ì˜¤ë¥˜: {str(e)}")

    def _parse_api_article(self, element: Dict) -> Optional[Dict]:
        """API ì‘ë‹µì˜ ê¸°ì‚¬ ìš”ì†Œë¥¼ íŒŒì‹±"""
        try:
            headlines = element.get('headlines', {})
            title = headlines.get('basic', '')
            if not title or len(title) < 5:
                return None
            
            canonical_url = element.get('canonical_url', '')
            if not canonical_url:
                return None
            
            if canonical_url.startswith('/'):
                url = urljoin(self.base_url, canonical_url)
            else:
                url = canonical_url
            
            description = element.get('description', {})
            content = description.get('basic', '')
            
            display_date = element.get('display_date', '')
            date = self._parse_date(display_date)
            if not date:
                date = self.today
            
            credits = element.get('credits', {})
            by_list = credits.get('by', [])
            author = ""
            if by_list and len(by_list) > 0:
                by_info = by_list[0]
                additional_props = by_info.get('additional_properties', {})
                original = additional_props.get('original', {})
                author = original.get('byline', '')
                if not author:
                    author = by_info.get('name', '')
            
            return {
                'title': title,
                'url': url,
                'content': content,
                'date': date,
                'author': author,
                'source': 'api'
            }
            
        except Exception as e:
            return None

    async def _collect_article_contents(self, articles: List[Dict]):
        """ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (í’ˆì§ˆ ìš°ì„ )"""
        if not articles:
            return
        
        console.print(f"ğŸ“– {len(articles)}ê°œ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        
        semaphore = asyncio.Semaphore(self.CONFIG["concurrent_limit"])
        
        async def process_article(article):
            async with semaphore:
                try:
                    url = article['url']
                    
                    # Playwrightë¡œ ë³¸ë¬¸ ì¶”ì¶œ
                    content = await self._extract_content_with_playwright(url)
                    
                    if content and len(content.strip()) > 50:
                        article['content'] = content
                        return True
                    else:
                        # HTML íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´
                        content = await self._extract_content_from_html(url)
                        if content and len(content.strip()) > 50:
                            article['content'] = content
                            return True
                    
                    return False
                    
                except Exception as e:
                    console.print(f"âš ï¸ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ({article.get('title', 'Unknown')}): {str(e)}")
                    return False
        
        tasks = [process_article(article) for article in articles]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        success_count = sum(1 for r in results if r is True)
        console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(articles)}ê°œ ì„±ê³µ")

    async def _extract_content_with_playwright(self, url: str) -> str:
        """Playwrightë¥¼ í™œìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            from playwright.async_api import async_playwright
            
            if not hasattr(self, '_browser'):
                self._playwright = await async_playwright().start()
                self._browser = await self._playwright.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
                )
            
            page = await self._browser.new_page()
            
            try:
                await page.goto(url, wait_until='domcontentloaded', timeout=10000)
                await page.wait_for_timeout(1000)
                
                try:
                    await page.wait_for_selector('section.article-body', timeout=5000)
                except:
                    selectors = [
                        'section[itemprop="articleBody"]',
                        'article.article-body',
                        'div.article-body',
                        'div#article-body'
                    ]
                    
                    for selector in selectors:
                        try:
                            await page.wait_for_selector(selector, timeout=4000)
                            break
                        except:
                            continue
                
                content = await page.evaluate('''() => {
                    let paragraphs = document.querySelectorAll('p.article-body__content.article-body__content-text');
                    
                    if (paragraphs.length === 0) {
                        paragraphs = document.querySelectorAll('p.article-body__content');
                    }
                    
                    if (paragraphs.length === 0) {
                        const articleBody = document.querySelector('section.article-body');
                        if (articleBody) {
                            paragraphs = articleBody.querySelectorAll('p');
                        }
                    }
                    
                    if (paragraphs.length === 0) {
                        paragraphs = document.querySelectorAll('article p, .content p, .article p');
                    }
                    
                    const textContent = Array.from(paragraphs)
                        .map(p => p.textContent.trim())
                        .filter(text => text.length > 10)
                        .join('\\n\\n');
                    
                    return textContent;
                }''')
                
                if content and len(content.strip()) > 50:
                    return content.strip()
                
                return ""
                
            finally:
                await page.close()
                
        except Exception as e:
            return ""

    async def _extract_content_from_html(self, url: str) -> str:
        """HTMLì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)"""
        try:
            html = await self._make_request(url)
            if not html:
                return ""
            
            soup = BeautifulSoup(html, 'html.parser')
            
            content_elem = (
                soup.find('section', class_='article-body') or
                soup.find('section', {'itemprop': 'articleBody'}) or
                soup.find('article', class_='article-body') or
                soup.find('div', class_='article-body')
            )
            
            if content_elem:
                p_tags = content_elem.find_all('p', class_='article-body__content')
                
                if not p_tags:
                    p_tags = content_elem.find_all('p')
                
                if p_tags:
                    content = '\n'.join(p.get_text(strip=True) for p in p_tags if p.get_text(strip=True))
                    content = re.sub(r'\n\s*\n', '\n\n', content)
                    return content.strip()
            
            return ""
            
        except Exception as e:
            return ""

    def _add_article_to_collection(self, article: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ ì»¬ë ‰ì…˜ì— ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)"""
        if not article or not article.get('title') or not article.get('url'):
            return False
        
        # ì¤‘ë³µ ì²´í¬
        for existing in self.articles:
            if existing['url'] == article['url']:
                return False
        
        self.articles.append(article)
        return True

    def _clean_title(self, title: str) -> str:
        """ì œëª© ì •ë¦¬"""
        if not title:
            return ""
        
        title = re.sub(r'\s+', ' ', title.strip())
        title = re.sub(r'^[^\wê°€-í£]+', '', title)
        title = re.sub(r'[^\wê°€-í£]+$', '', title)
        
        return title

    def _parse_relative_time(self, time_str: str) -> str:
        """ìƒëŒ€ ì‹œê°„ì„ ì ˆëŒ€ ì‹œê°„ìœ¼ë¡œ ë³€í™˜"""
        try:
            now = datetime.now()
            
            if 'ë¶„ ì „' in time_str:
                minutes = int(re.search(r'(\d+)ë¶„ ì „', time_str).group(1))
                target_time = now - timedelta(minutes=minutes)
            elif 'ì‹œê°„ ì „' in time_str:
                hours = int(re.search(r'(\d+)ì‹œê°„ ì „', time_str).group(1))
                target_time = now - timedelta(hours=hours)
            elif 'ì¼ ì „' in time_str or 'ì¼ì „' in time_str:
                days = int(re.search(r'(\d+)ì¼', time_str).group(1))
                target_time = now - timedelta(days=days)
            else:
                return self.today
            
            return target_time.strftime('%Y-%m-%d')
            
        except:
            return self.today

    def _parse_date(self, date_str: str) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            if not date_str:
                return self.today
            
            # ISO í˜•ì‹
            if 'T' in date_str:
                dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                return dt.strftime('%Y-%m-%d')
            
            return self.today
            
        except:
            return self.today

    def _parse_content_elements(self, data: Dict) -> List[Dict]:
        """JSON ë°ì´í„°ì—ì„œ content_elements íŒŒì‹±"""
        articles = []
        
        try:
            content_elements = data.get('content_elements', [])
            
            for element in content_elements:
                article = self._parse_api_article(element)
                if article:
                    articles.append(article)
            
            return articles
            
        except Exception as e:
            return []

    async def _cleanup_playwright(self):
        """Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, '_browser') and self._browser:
                await self._browser.close()
            
            if hasattr(self, '_playwright') and self._playwright:
                await self._playwright.stop()
                
        except Exception as e:
            console.print(f"âš ï¸ Playwright ì •ë¦¬ ì˜¤ë¥˜: {str(e)}")

    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (ë©”ì¸ ë©”ì„œë“œ)"""
        start_time = datetime.now()
        
        console.print("ğŸš€ ì¡°ì„ ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘ (í’ˆì§ˆ ìš°ì„ )")
        console.print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ ë²”ìœ„: {self.date_range}")
        console.print(f"ğŸ¯ ëª©í‘œ: {self.CONFIG['target_count']}ê°œ ê¸°ì‚¬")
        console.print(f"â±ï¸ ëª©í‘œ ì‹œê°„: í’ˆì§ˆ ìš°ì„  (ë³¸ë¬¸ ìˆ˜ì§‘)")
        console.print("=" * 50)
        
        try:
            # 1ì°¨: HTMLì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
            await self._collect_from_html()
            
            # 2ì°¨: APIë¥¼ í†µí•œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘
            if len(self.articles) < self.CONFIG["target_count"]:
                console.print("ğŸ“° APIë¥¼ í†µí•œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
                await self._collect_from_api()
            
            # ê²°ê³¼ ì œí•œ
            if len(self.articles) > self.CONFIG["target_count"]:
                self.articles = self.articles[:self.CONFIG["target_count"]]
            
            # í’ˆì§ˆ ê²€ì¦: ì œëª©ì´ ë„ˆë¬´ ì§§ì€ ê¸°ì‚¬ ì œì™¸
            self.articles = [article for article in self.articles if len(article['title'].strip()) >= 5]
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ (í’ˆì§ˆ ìš°ì„ )
            console.print(f"ğŸ“– ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘... (í’ˆì§ˆ ìš°ì„ )")
            await self._collect_article_contents(self.articles)
            
            # í’ˆì§ˆ ê²€ì¦: ë³¸ë¬¸ì´ ì—†ëŠ” ê¸°ì‚¬ ì œì™¸
            self.articles = [article for article in self.articles if article.get('content', '').strip()]
            console.print(f"âœ… ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ: {len(self.articles)}ê°œ ê¸°ì‚¬ (ë³¸ë¬¸ ìˆìŒ)")
            
            # Playwright ë¦¬ì†ŒìŠ¤ ì •ë¦¬
            await self._cleanup_playwright()
            
            end_time = datetime.now()
            elapsed = (end_time - start_time).total_seconds()
            
            console.print("=" * 50)
            console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            console.print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
            console.print(f"   - ì´ ìˆ˜ì§‘: {len(self.articles)}ê°œ")
            console.print(f"   - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
            
            if elapsed > 0:
                speed = len(self.articles) / elapsed
                console.print(f"   - í‰ê·  ì†ë„: {speed:.1f} ê¸°ì‚¬/ì´ˆ")
            
            if elapsed <= 120:  # í’ˆì§ˆ ìš°ì„ ì´ë¯€ë¡œ 2ë¶„ ëª©í‘œ
                console.print("âœ… ëª©í‘œ ì‹œê°„ ë‹¬ì„±! (2ë¶„ ì´ë‚´)")
            else:
                console.print("â° ëª©í‘œ ì‹œê°„ ì´ˆê³¼")
            
            return self.articles
            
        except Exception as e:
            console.print(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
            await self._cleanup_playwright()
            return []

    def display_results(self, articles: List[Dict]):
        """ìˆ˜ì§‘ ê²°ê³¼ í‘œì‹œ"""
        if not articles:
            console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        console.print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì˜ˆì‹œ:")
        
        table = Table(title="ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")
        table.add_column("ë²ˆí˜¸", justify="right", style="cyan", no_wrap=True)
        table.add_column("ì œëª©", style="magenta", max_width=50)
        table.add_column("ë‚ ì§œ", justify="center", style="green")
        table.add_column("URL", style="blue", max_width=50)
        
        for i, article in enumerate(articles[:10], 1):
            title = article['title']
            if len(title) > 50:
                title = title[:47] + "..."
            
            url = article['url']
            if len(url) > 50:
                url = url[:47] + "..."
            
            table.add_row(
                str(i),
                title,
                article['date'],
                url
            )
        
        console.print(table)

    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        console.print(f"\nğŸ’¾ Supabaseì— {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ ì¡°íšŒ
        media_outlet = self.supabase_manager.get_media_outlet('ì¡°ì„ ì¼ë³´')
        if not media_outlet:
            console.print("âŒ ì¡°ì„ ì¼ë³´ ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": 0, "failed": len(articles)}
        
        # ì„ì˜ì˜ ì´ìŠˆ ID ê°€ì ¸ì˜¤ê¸°
        issue_id = self.supabase_manager.get_random_issue_id()
        if not issue_id:
            console.print("âŒ ì´ìŠˆ IDë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"success": 0, "failed": len(articles)}
        
        console.print(f"âœ… ë¯¸ë””ì–´: {media_outlet['name']} (ID: {media_outlet['id']})")
        console.print(f"âœ… ì´ìŠˆ ID: {issue_id}")
        
        # ê¸°ì‚¬ ì €ì¥
        success_count = 0
        failed_count = 0
        
        for i, article in enumerate(articles, 1):
            try:
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article.get("content", ""),
                    "published_at": article["date"],
                    "media_id": media_outlet["id"],
                    "issue_id": issue_id,
                    "bias": media_outlet.get("bias", self.media_bias)
                }
                
                if self.supabase_manager.insert_article(article_data):
                    success_count += 1
                    console.print(f"âœ… [{i}/{len(articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                else:
                    failed_count += 1
                    console.print(f"âŒ [{i}/{len(articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                    
            except Exception as e:
                failed_count += 1
                console.print(f"âŒ [{i}/{len(articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"   - ì„±ê³µ: {success_count}ê°œ")
        console.print(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
        console.print(f"   - ì„±ê³µë¥ : {(success_count / len(articles) * 100):.1f}%")
        
        return {"success": success_count, "failed": failed_count}

    async def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            # ê¸°ì‚¬ ìˆ˜ì§‘
            articles = await self.collect_all_articles()
            
            if not articles:
                console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ê²°ê³¼ í‘œì‹œ
            self.display_results(articles)
            
            # Supabaseì— ì €ì¥
            save_result = await self.save_to_supabase(articles)
            
            # ìµœì¢… ê²°ê³¼
            total_time = datetime.now().strftime('%H:%M:%S')
            console.print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼: [ì„±ê³µ {len(articles)}ê°œ / ì‹¤íŒ¨ {save_result['failed']}ê°œ / ì´ ì†Œìš”ì‹œê°„ {total_time}]")
            
        except Exception as e:
            console.print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            traceback.print_exc()


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = ChosunPoliticsCollector()
    await collector.run()


if __name__ == "__main__":
    asyncio.run(main())

