#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤1 ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- Ajax APIë¥¼ í™œìš©í•˜ì—¬ ë¹ ë¥¸ ê¸°ì‚¬ ìˆ˜ì§‘
- 20ì´ˆ ë‚´ì— 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ëª©í‘œ
- ë³¸ë¬¸ì„ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ (êµ°ë”ë”ê¸° ì œê±°)
- biasëŠ” media_outlets.biasë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich import box
import re
import json
import sys
import os
from playwright.async_api import async_playwright
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from legacy.supabase_manager_v2 import SupabaseManagerV2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rich ì½˜ì†” ì„¤ì •
console = Console()

class News1PoliticsCrawler:
    def __init__(self):
        self.base_url = "https://www.news1.kr"
        self.politics_url = "https://www.news1.kr/politics"
        self.supabase_manager = SupabaseManagerV2()
        self.media_outlet = "ë‰´ìŠ¤1"
        self.media_bias = "left"  # ë‰´ìŠ¤1ì€ ì¢Œí¸í–¥ ì„±í–¥
        
        # media_outletsì—ì„œ ë‰´ìŠ¤1 ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        self.media_id = None
        self._init_media_outlet()
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.max_articles = 100
        self.max_workers = 20
        self.timeout = 10
        self.delay = 0.05
        
    def _init_media_outlet(self):
        """media_outletsì—ì„œ ë‰´ìŠ¤1 ì •ë³´ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            media_outlet = self.supabase_manager.get_media_outlet(self.media_outlet)
            if media_outlet:
                self.media_id = media_outlet['id']
                console.print(f"âœ… ë‰´ìŠ¤1 media_id: {self.media_id}, bias: {self.media_bias}")
            else:
                # ë‰´ìŠ¤1ì´ ì—†ìœ¼ë©´ ìƒì„±
                self.media_id = self.supabase_manager.create_media_outlet(self.media_outlet, self.media_bias)
                console.print(f"âœ… ë‰´ìŠ¤1 ìƒì„±ë¨ - media_id: {self.media_id}, bias: {self.media_bias}")
        except Exception as e:
            console.print(f"[red]ë‰´ìŠ¤1 media_outlet ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}[/red]")
            self.media_id = 20  # ê¸°ë³¸ê°’ ì‚¬ìš©
    
        # Ajax API ì„¤ì •
        self.api_url = "https://www.news1.kr/api/article/list"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Referer': 'https://www.news1.kr/politics',
            'X-Requested-With': 'XMLHttpRequest'
        }

    async def get_page_content(self, session, url):
        """í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
            return None

    async def collect_article_links_via_api(self, session) -> list:
        """Ajax APIë¥¼ í†µí•´ ê¸°ì‚¬ ë§í¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        console.print("ğŸ” ë‰´ìŠ¤1 Ajax APIë¡œ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = []
        start_page = 1
        max_pages = 10  # 100ê°œ ê¸°ì‚¬ = 10í˜ì´ì§€ Ã— 10ê°œì”©
        
        try:
            # ì˜¬ë°”ë¥¸ ë‰´ìŠ¤1 API ì—”ë“œí¬ì¸íŠ¸
            api_url = "https://rest.news1.kr/v6/section/politics/latest"
            
            for page in range(start_page, start_page + max_pages):
                params = {
                    'start': page,
                    'limit': 10
                }
                
                console.print(f"ğŸ“„ API í˜ì´ì§€ {page} ìš”ì²­ ì¤‘... (start={page}, limit=10)")
                
                async with session.get(api_url, params=params, headers=self.headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        if isinstance(data, list) and len(data) > 0:
                            page_links = []
                            for article in data:
                                if 'url' in article and '/politics/' in article['url']:
                                    if article['url'].startswith('/'):
                                        full_url = f"{self.base_url}{article['url']}"
                                    else:
                                        full_url = article['url']
                                    page_links.append(full_url)
                            
                            # ìƒˆë¡œ ë°œê²¬ëœ ë§í¬ë§Œ ì¶”ê°€
                            new_links = [link for link in page_links if link not in all_links]
                            all_links.extend(new_links)
                            
                            console.print(f"ğŸ“„ API í˜ì´ì§€ {page}: {len(new_links)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬ (ì´ {len(all_links)}ê°œ)")
                            
                            # ë” ì´ìƒ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                            if len(new_links) == 0:
                                console.print(f"ğŸ“„ API í˜ì´ì§€ {page}: ë” ì´ìƒ ìƒˆ ê¸°ì‚¬ ì—†ìŒ, ìˆ˜ì§‘ ì™„ë£Œ")
                                break
                            
                            # API í˜¸ì¶œ ê°„ ë”œë ˆì´
                            await asyncio.sleep(0.2)
                        else:
                            console.print(f"ğŸ“„ API í˜ì´ì§€ {page}: ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜")
                            break
                    else:
                        console.print(f"âŒ API í˜ì´ì§€ {page} í˜¸ì¶œ ì‹¤íŒ¨: {response.status}")
                        break
            
            console.print(f"âœ… APIë¡œ ì´ {len(all_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
            return all_links
                    
        except Exception as e:
            console.print(f"âŒ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return all_links

    async def collect_article_links_fallback(self, session) -> list:
        """HTML íŒŒì‹±ìœ¼ë¡œ ê¸°ì‚¬ ë§í¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤ (100ê°œ ë‹¬ì„±ê¹Œì§€)"""
        console.print("ğŸ” HTML íŒŒì‹±ìœ¼ë¡œ ë‰´ìŠ¤1 ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = []
        target_count = 100
        
        # 1ë‹¨ê³„: ë©”ì¸ ì •ì¹˜ í˜ì´ì§€ì—ì„œ 3ê°œ ì„¹ì…˜ ëª¨ë‘ íŒŒì‹±
        try:
            links = await self._collect_all_sections(session, self.politics_url)
            all_links.extend(links)
            # ì¤‘ë³µ ì œê±°
            all_links = list(set(all_links))
            console.print(f"ğŸ“„ 1ë‹¨ê³„ ì™„ë£Œ: {len(all_links)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        except Exception as e:
            console.print(f"[red]1ë‹¨ê³„ ì‹¤íŒ¨: {str(e)}[/red]")
        
        # 2ë‹¨ê³„: 100ê°œ ë¯¸ë‹¬ì‹œ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¶”ê°€ ìˆ˜ì§‘
        if len(all_links) < target_count:
            shortage = target_count - len(all_links)
            console.print(f"ğŸ” 2ë‹¨ê³„: {shortage}ê°œ ë¶€ì¡±, í•˜ìœ„ ì¹´í…Œê³ ë¦¬ì—ì„œ ì¶”ê°€ ìˆ˜ì§‘...")
            
            sub_categories = [
                '/politics/president',
                '/politics/assembly', 
                '/politics/pm-bai-comm',
                '/politics/general-politics'
            ]
            
            for category in sub_categories:
                if len(all_links) >= target_count:
                    break
                    
                try:
                    category_url = f"{self.base_url}{category}"
                    additional_links = await self._collect_with_playwright_enhanced(category_url, target_count - len(all_links))
                    
                    # ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
                    new_links = [link for link in additional_links if link not in all_links]
                    all_links.extend(new_links)
                    
                    console.print(f"ğŸ“„ {category}: {len(new_links)}ê°œ ìƒˆ ê¸°ì‚¬ ì¶”ê°€ (ì´ {len(all_links)}ê°œ)")
                    
                except Exception as e:
                    console.print(f"[red]{category} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}[/red]")
        
        # ìµœì¢… ì¤‘ë³µ ì œê±°
        all_links = list(set(all_links))
        
        if len(all_links) >= target_count:
            console.print(f"âœ… ëª©í‘œ ë‹¬ì„±! ì´ {len(all_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            console.print(f"âš ï¸  ëª©í‘œ ë¯¸ë‹¬: {len(all_links)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ({target_count - len(all_links)}ê°œ ë¶€ì¡±)")
        
        return all_links
    
    async def _collect_all_sections(self, session, url: str) -> list:
        """3ê°œ ì„¹ì…˜ì„ ëª¨ë‘ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_links = []
        
        try:
            # ê¸°ë³¸ HTML íŒŒì‹±ìœ¼ë¡œ 3ê°œ ì„¹ì…˜ ëª¨ë‘ ìˆ˜ì§‘
            html_content = await self.get_page_content(session, url)
            if html_content:
                soup = BeautifulSoup(html_content, 'html.parser')
                links = self._extract_all_sections_from_html(soup)
                all_links.extend(links)
                console.print(f"ğŸ“„ ê¸°ë³¸ HTML íŒŒì‹±: {len(links)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            # Playwrightë¡œ ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­í•˜ì—¬ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘
            playwright_links = await self._collect_with_playwright(url)
            all_links.extend(playwright_links)
            console.print(f"ğŸ“„ Playwright ë”ë³´ê¸°: {len(playwright_links)}ê°œ ì¶”ê°€ ê¸°ì‚¬ ë°œê²¬")
            
        except Exception as e:
            console.print(f"[red]ì„¹ì…˜ë³„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}[/red]")
        
        return all_links
    
    async def _collect_with_playwright(self, url: str) -> list:
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ê³  ì¶”ê°€ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_links = []
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle')
                await asyncio.sleep(2)
                
                # ì´ˆê¸° ê¸°ì‚¬ ìˆ˜ í™•ì¸
                initial_count = len(await page.query_selector_all('h2.n1-header-title-1-2 a'))
                console.print(f"ğŸ“„ ì´ˆê¸° ê¸°ì‚¬ ìˆ˜: {initial_count}ê°œ")
                
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ë°˜ë³µ (ìµœëŒ€ 15íšŒë¡œ ì¦ê°€)
                click_count = 0
                max_clicks = 15
                
                while click_count < max_clicks:
                    try:
                        # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                        more_button = await page.query_selector('button.read-more, .read-more, [class*="more"]')
                        if not more_button:
                            console.print(f"ğŸ“„ ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ, ìˆ˜ì§‘ ì™„ë£Œ")
                            break
                        
                        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
                        await more_button.click()
                        await asyncio.sleep(2)  # ë¡œë”© ëŒ€ê¸°
                        
                        # ìƒˆë¡œìš´ ê¸°ì‚¬ ìˆ˜ í™•ì¸
                        new_count = len(await page.query_selector_all('h2.n1-header-title-1-2 a'))
                        new_articles = new_count - initial_count
                        
                        if new_articles > 0:
                            console.print(f"ğŸ“„ ë”ë³´ê¸° í´ë¦­ {click_count + 1}: {new_articles}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬ (ì´ {new_count}ê°œ)")
                            initial_count = new_count
                        else:
                            console.print(f"ğŸ“„ ë”ë³´ê¸° í´ë¦­ {click_count + 1}: ìƒˆ ê¸°ì‚¬ ì—†ìŒ")
                        
                        click_count += 1
                        
                        # 150ê°œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨ (ì—¬ìœ ë¶„ í™•ë³´)
                        if new_count >= 150:
                            console.print(f"ğŸ“„ 150ê°œ ê¸°ì‚¬ ë‹¬ì„±! ìˆ˜ì§‘ ì™„ë£Œ")
                            break
                            
                    except Exception as e:
                        console.print(f"[red]ë”ë³´ê¸° í´ë¦­ {click_count + 1} ì‹¤íŒ¨: {str(e)}[/red]")
                        break
                
                # ìµœì¢… HTMLì—ì„œ ëª¨ë“  ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                final_html = await page.content()
                soup = BeautifulSoup(final_html, 'html.parser')
                final_links = self._extract_all_sections_from_html(soup)
                
                await browser.close()
                return final_links
                
        except Exception as e:
            console.print(f"[red]Playwright ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}[/red]")
            return []
    
    async def _collect_with_playwright_enhanced(self, url: str, needed_count: int) -> list:
        """í–¥ìƒëœ Playwrightë¡œ í•„ìš”í•œ ë§Œí¼ë§Œ ì¶”ê°€ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_links = []
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()
                
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle')
                await asyncio.sleep(2)
                
                # ì´ˆê¸° ê¸°ì‚¬ ìˆ˜ í™•ì¸
                initial_count = len(await page.query_selector_all('h2.n1-header-title-1-2 a'))
                console.print(f"ğŸ“„ {url} ì´ˆê¸° ê¸°ì‚¬: {initial_count}ê°œ")
                
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ë°˜ë³µ (í•„ìš”í•œ ë§Œí¼)
                click_count = 0
                max_clicks = 10
                target_count = initial_count + needed_count + 20  # ì—¬ìœ ë¶„ ì¶”ê°€
                
                while click_count < max_clicks:
                    try:
                        # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸°
                        more_button = await page.query_selector('button.read-more, .read-more, [class*="more"]')
                        if not more_button:
                            break
                        
                        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
                        await more_button.click()
                        await asyncio.sleep(1.5)  # ë¡œë”© ëŒ€ê¸°
                        
                        # ìƒˆë¡œìš´ ê¸°ì‚¬ ìˆ˜ í™•ì¸
                        new_count = len(await page.query_selector_all('h2.n1-header-title-1-2 a'))
                        
                        if new_count > initial_count:
                            console.print(f"ğŸ“„ ë”ë³´ê¸° í´ë¦­ {click_count + 1}: {new_count}ê°œ ê¸°ì‚¬ (ëª©í‘œ: {target_count}ê°œ)")
                            initial_count = new_count
                        
                        click_count += 1
                        
                        # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
                        if new_count >= target_count:
                            break
                            
                    except Exception as e:
                        console.print(f"[red]ë”ë³´ê¸° í´ë¦­ ì‹¤íŒ¨: {str(e)}[/red]")
                        break
                
                # ìµœì¢… HTMLì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                final_html = await page.content()
                soup = BeautifulSoup(final_html, 'html.parser')
                final_links = self._extract_all_sections_from_html(soup)
                
                await browser.close()
                return final_links
                
        except Exception as e:
            console.print(f"[red]í–¥ìƒëœ Playwright ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}[/red]")
            return []
    
    async def _collect_with_more_button(self, session, url: str, page_name: str) -> list:
        """ë”ë³´ê¸° ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë” ë§ì€ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        all_links = []
        current_url = url
        page_count = 1
        
        while True:
            try:
                html_content = await self.get_page_content(session, current_url)
                if not html_content:
                    break
                
                soup = BeautifulSoup(html_content, 'html.parser')
                links = self._extract_article_links_from_html(soup)
                
                # ìƒˆë¡œ ë°œê²¬ëœ ë§í¬ë§Œ ì¶”ê°€
                new_links = [link for link in links if link not in all_links]
                all_links.extend(new_links)
                
                console.print(f"ğŸ“„ {page_name} (í˜ì´ì§€ {page_count}): {len(new_links)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬ (ì´ {len(all_links)}ê°œ)")
                
                # ë”ë³´ê¸° ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                more_button = soup.select_one('button.read-more')
                if not more_button:
                    console.print(f"ğŸ“„ {page_name}: ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ, ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                
                # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë®¬ë ˆì´ì…˜ (URL íŒŒë¼ë¯¸í„° ì¶”ê°€)
                page_count += 1
                if '?' in current_url:
                    current_url = f"{url}&page={page_count}"
                else:
                    current_url = f"{url}?page={page_count}"
                
                # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ë§Œ ìˆ˜ì§‘ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
                if page_count > 10:
                    console.print(f"ğŸ“„ {page_name}: ìµœëŒ€ í˜ì´ì§€ ìˆ˜(10) ë„ë‹¬, ìˆ˜ì§‘ ì™„ë£Œ")
                    break
                
                # í˜ì´ì§€ ê°„ ë”œë ˆì´
                await asyncio.sleep(0.5)
                
            except Exception as e:
                console.print(f"[red]{page_name} í˜ì´ì§€ {page_count} íŒŒì‹± ì‹¤íŒ¨: {str(e)}[/red]")
                break
        
        return all_links
    
    def _extract_all_sections_from_html(self, soup) -> list:
        """3ê°œ ì„¹ì…˜ì„ ëª¨ë‘ íŒŒì‹±í•˜ì—¬ ê¸°ì‚¬ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links = []
        
        # 1. ìµœìƒë‹¨ (ë©”ì¸ ê¸°ì‚¬) - h2.n1-header-subtop-2 a
        main_articles = soup.select('h2.n1-header-subtop-2 a')
        for article in main_articles:
            href = article.get('href')
            if href and '/politics/' in href:
                if href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                if full_url not in links:
                    links.append(full_url)
        
        # 2. ì¤‘ê°„ (ì£¼ìš”ê¸°ì‚¬) - h2.text-limit-2-row.n1-header-title-7 a
        featured_articles = soup.select('h2.text-limit-2-row.n1-header-title-7 a')
        for article in featured_articles:
            href = article.get('href')
            if href and '/politics/' in href:
                if href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                if full_url not in links:
                    links.append(full_url)
        
        # 3. í•˜ë‹¨ (ìµœì‹ ê¸°ì‚¬) - h2.n1-header-title-1-2.text-limit-2-row a
        latest_articles = soup.select('h2.n1-header-title-1-2.text-limit-2-row a')
        for article in latest_articles:
            href = article.get('href')
            if href and '/politics/' in href:
                if href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                if full_url not in links:
                    links.append(full_url)
        
        # 4. ì¶”ê°€ ê¸°ì‚¬ë“¤ (ë”ë³´ê¸° í´ë¦­ í›„ ë¡œë“œëœ ê¸°ì‚¬ë“¤)
        additional_articles = soup.select('h2.n1-header-title-1-2 a')
        for article in additional_articles:
            href = article.get('href')
            if href and '/politics/' in href:
                if href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                if full_url not in links:
                    links.append(full_url)
        
        console.print(f"ğŸ“„ ì„¹ì…˜ë³„ íŒŒì‹± ê²°ê³¼:")
        console.print(f"   - ìµœìƒë‹¨ (ë©”ì¸): {len(main_articles)}ê°œ")
        console.print(f"   - ì¤‘ê°„ (ì£¼ìš”): {len(featured_articles)}ê°œ")
        console.print(f"   - í•˜ë‹¨ (ìµœì‹ ): {len(latest_articles)}ê°œ")
        console.print(f"   - ì¶”ê°€ ê¸°ì‚¬: {len(additional_articles)}ê°œ")
        console.print(f"   - ì´ ì¤‘ë³µ ì œê±° í›„: {len(links)}ê°œ")
        
        return links
    
    def _extract_article_links_from_html(self, soup) -> list:
        """HTMLì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        links = []
        article_elements = soup.find_all('a', href=True)
        
        for element in article_elements:
            href = element.get('href')
            # ì‹¤ì œ ê¸°ì‚¬ URLë§Œ í•„í„°ë§ (ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì œì™¸)
            if (href and '/politics/' in href and 
                any(keyword in href for keyword in ['/president/', '/assembly/', '/pm-bai-comm/', '/general-politics/']) and
                not href.endswith(('/president', '/assembly', '/pm-bai-comm', '/general-politics'))):
                
                if href.startswith('/'):
                    full_url = f"{self.base_url}{href}"
                else:
                    full_url = href
                
                if full_url not in links:
                    links.append(full_url)
        
        return links

    def extract_article_content(self, html_content: str, url: str) -> tuple:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‰´ìŠ¤1ì€ ê¸°ì‚¬ ë‚´ìš©ì´ JSONì— ìˆìŒ - __NEXT_DATA__ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ì¶”ì¶œ
            next_data_script = soup.find('script', {'id': '__NEXT_DATA__'})
            if next_data_script:
                try:
                    import json
                    data = json.loads(next_data_script.string)
                    
                    # JSONì—ì„œ ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ
                    article_view = data.get('props', {}).get('pageProps', {}).get('articleView', {})
                    
                    # ì œëª© ì¶”ì¶œ
                    title = article_view.get('title', '')
                    
                    # ë³¸ë¬¸ ì¶”ì¶œ - contentArrangeì—ì„œ typeì´ "text"ì¸ ê²ƒë“¤
                    content_parts = []
                    content_arrange = article_view.get('contentArrange', [])
                    for item in content_arrange:
                        if item.get('type') == 'text':
                            text_content = item.get('content', '').strip()
                            if text_content and len(text_content) > 10:
                                # ê¸°ì ì´ë©”ì¼ ì œì™¸
                                if '@news1.kr' not in text_content:
                                    content_parts.append(text_content)
                    
                    content = '\n\n'.join(content_parts)
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ
                    publish_date = None
                    pubdate_at = article_view.get('pubdate_at', '')
                    if pubdate_at:
                        publish_date = self.parse_date(pubdate_at)
                    
                    # ì œëª©ê³¼ ë³¸ë¬¸ì´ ëª¨ë‘ ìˆì–´ì•¼ ìœ íš¨í•œ ê¸°ì‚¬
                    if title and content and len(content) > 50:
                        return title, content, publish_date
                
                except json.JSONDecodeError as e:
                    console.print(f"[yellow]JSON íŒŒì‹± ì‹¤íŒ¨, HTML íŒŒì‹±ìœ¼ë¡œ ëŒ€ì²´: {str(e)}[/yellow]")
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ HTML íŒŒì‹± ë°©ì‹ ì‚¬ìš©
            # ì œëª© ì¶”ì¶œ: h1.article-h2-header-title
            title = ""
            title_elem = soup.select_one('h1.article-h2-header-title')
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # ë³¸ë¬¸ ì¶”ì¶œ: div#articleBodyContent p
            content = ""
            content_elem = soup.select_one('div#articleBodyContent')
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in content_elem.select('.ads-article-warp, figure, .article_content_stitle'):
                    unwanted.decompose()
                
                # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                paragraphs = content_elem.find_all('p')
                content_parts = []
                for p in paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 10:
                        content_parts.append(text)
                
                content = '\n\n'.join(content_parts)
            
            # ë°œí–‰ì¼ ì¶”ì¶œ: #article_created time
            publish_date = None
            date_elem = soup.select_one('#article_created time')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                publish_date = self.parse_date(date_text)
            
            # HTML íŒŒì‹±ë„ ì‹¤íŒ¨í•œ ê²½ìš°
            if not title or not content or len(content) < 50:
                return None, None, None
                
            return title, content, publish_date
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return None, None, None

    def parse_date(self, date_text: str) -> str:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            # ë‰´ìŠ¤1 ë‚ ì§œ í˜•ì‹: "2024.01.15 14:30" ë˜ëŠ” "1ì‹œê°„ ì „" ë“±
            if 'ì‹œê°„ ì „' in date_text:
                hours = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
                now = datetime.now()
                date_obj = now.replace(hour=now.hour - hours, minute=0, second=0, microsecond=0)
                return date_obj.isoformat()
            elif 'ë¶„ ì „' in date_text:
                minutes = int(re.search(r'(\d+)ë¶„', date_text).group(1))
                now = datetime.now()
                date_obj = now.replace(minute=now.minute - minutes, second=0, microsecond=0)
                return date_obj.isoformat()
            elif re.match(r'\d{4}\.\d{2}\.\d{2}', date_text):
                # "2024.01.15" í˜•ì‹
                date_obj = datetime.strptime(date_text.split()[0], '%Y.%m.%d')
                return date_obj.isoformat()
            else:
                # í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                return datetime.now().isoformat()
        except Exception as e:
            logger.error(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_text} - {e}")
            return datetime.now().isoformat()

    async def process_single_article(self, session, url, semaphore):
        """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        async with semaphore:
            try:
                # ê¸°ì‚¬ê°€ ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸ (URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬)
                existing_article = self.supabase_manager.client.table('articles').select('id').eq('url', url).execute()
                if existing_article.data:
                    return True  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬
                
                # ê¸°ì‚¬ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                html_content = await self.get_page_content(session, url)
                if not html_content:
                    return False
                
                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                result = self.extract_article_content(html_content, url)
                if not result or len(result) != 3:
                    return False
                
                title, content, publish_date = result
                
                # ì œëª© ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ë„ ì¶”ê°€
                existing_title = self.supabase_manager.client.table('articles').select('id').eq('title', title).execute()
                if existing_title.data:
                    console.print(f"[yellow]ì¤‘ë³µ ì œëª© ë°œê²¬, ê±´ë„ˆëœ€: {title[:50]}...[/yellow]")
                    return True  # ì¤‘ë³µì´ì§€ë§Œ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                
                # ìƒˆ ê¸°ì‚¬ ì‚½ì…
                article_data = {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': publish_date,
                    'media_id': self.media_id,  # ë¯¸ë¦¬ ê°€ì ¸ì˜¨ media_id ì‚¬ìš©
                    'bias': self.media_bias,   # bias ëª…ì‹œì  ì„¤ì •
                    'issue_id': 6  # ì„ì‹œ issue_id
                }
                
                result = self.supabase_manager.insert_article(article_data)
                if result:
                    console.print(f"[green]âœ… ìƒˆ ê¸°ì‚¬ ì €ì¥: {title[:50]}...[/green]")
                    return True
                else:
                    console.print(f"[red]âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {title[:50]}...[/red]")
                    return False
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
                return False

    async def crawl_articles(self, article_links):
        """ê¸°ì‚¬ë“¤ì„ ë™ì‹œì— í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        console.print(f"\nğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì„¸ì…˜ ì„¤ì • - ì„±ëŠ¥ ìµœì í™”
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=self.timeout, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=console
            ) as progress:
                
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                # ë™ì‹œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ìµœëŒ€ 30ê°œ ë™ì‹œ ìš”ì²­)
                semaphore = asyncio.Semaphore(30)
                
                async def process_with_progress(url):
                    result = await self.process_single_article(session, url, semaphore)
                    progress.update(task, advance=1)
                    return result
                
                # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
                results = await asyncio.gather(*[process_with_progress(url) for url in article_links], return_exceptions=True)
                
                success_count = sum(1 for result in results if result is True)
                failed_count = len(results) - success_count
        
        return success_count, failed_count

    async def run(self):
        """í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        start_time = datetime.now()
        
        console.print("ğŸš€ ë‰´ìŠ¤1 ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘!")
        console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ì„¸ì…˜ ì„¤ì •
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=self.timeout, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            all_article_links = []
            
            # 1. HTML íŒŒì‹±ìœ¼ë¡œ ì´ˆê¸° ê¸°ì‚¬ ìˆ˜ì§‘ (100ê°œ ë³´ì¥)
            console.print("ğŸ” 1ë‹¨ê³„: HTML íŒŒì‹±ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘ (100ê°œ ëª©í‘œ)...")
            initial_links = await self.collect_article_links_fallback(session)
            all_article_links.extend(initial_links)
            console.print(f"ğŸ“„ ì´ˆê¸° ê¸°ì‚¬: {len(initial_links)}ê°œ ìˆ˜ì§‘")
            
            # 2. Ajax APIë¡œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘
            console.print("ğŸ” 2ë‹¨ê³„: Ajax APIë¡œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘...")
            api_links = await self.collect_article_links_via_api(session)
            all_article_links.extend(api_links)
            console.print(f"ğŸ“„ API ê¸°ì‚¬: {len(api_links)}ê°œ ìˆ˜ì§‘")
            
            # 3. ì¤‘ë³µ ì œê±°
            all_article_links = list(set(all_article_links))
            console.print(f"ğŸ“„ ì¤‘ë³µ ì œê±° í›„ ì´: {len(all_article_links)}ê°œ ê¸°ì‚¬")
            
            # 4. 100ê°œ ë¯¸ë‹¬ ì‹œ ì¶”ê°€ í¬ë¡¤ë§
            target_count = 100
            if len(all_article_links) < target_count:
                shortage = target_count - len(all_article_links)
                console.print(f"ğŸ” 3ë‹¨ê³„: {shortage}ê°œ ë¶€ì¡±, ì¶”ê°€ í¬ë¡¤ë§ ì‹¤í–‰...")
                
                # ë” ë§ì€ ë”ë³´ê¸° í´ë¦­ìœ¼ë¡œ ì¶”ê°€ ìˆ˜ì§‘
                try:
                    additional_links = await self._collect_with_playwright_enhanced(
                        self.politics_url, 
                        shortage + 20  # ì—¬ìœ ë¶„ ì¶”ê°€
                    )
                    
                    # ì¤‘ë³µ ì²´í¬ í›„ ì¶”ê°€
                    new_links = [link for link in additional_links if link not in all_article_links]
                    all_article_links.extend(new_links)
                    all_article_links = list(set(all_article_links))  # ìµœì¢… ì¤‘ë³µ ì œê±°
                    
                    console.print(f"ğŸ“„ ì¶”ê°€ í¬ë¡¤ë§: {len(new_links)}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘")
                    console.print(f"ğŸ“„ ìµœì¢… ì´: {len(all_article_links)}ê°œ ê¸°ì‚¬")
                    
                except Exception as e:
                    console.print(f"[red]ì¶”ê°€ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}[/red]")
            
            if len(all_article_links) >= target_count:
                console.print(f"âœ… 100ê°œ ëª©í‘œ ë‹¬ì„±! ({len(all_article_links)}ê°œ)")
            else:
                console.print(f"âš ï¸  ëª©í‘œ ë¯¸ë‹¬: {len(all_article_links)}ê°œ ({target_count - len(all_article_links)}ê°œ ë¶€ì¡±)")
                console.print("ğŸ“„ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ì‚¬ë¡œ í¬ë¡¤ë§ ê³„ì† ì§„í–‰...")
            
            if not all_article_links:
                console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 4. ê¸°ì‚¬ í¬ë¡¤ë§
            success_count, failed_count = await self.crawl_articles(all_article_links)
        
        # ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        duration = end_time - start_time
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="ë‰´ìŠ¤1 í¬ë¡¤ë§ ê²°ê³¼", box=box.ROUNDED)
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê°’", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(all_article_links)))
        table.add_row("ì„±ê³µ", f"{success_count}ê°œ", style="green")
        table.add_row("ì‹¤íŒ¨", f"{failed_count}ê°œ", style="red")
        table.add_row("ì„±ê³µë¥ ", f"{(success_count/len(all_article_links)*100):.1f}%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration.total_seconds():.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{len(all_article_links)/duration.total_seconds():.2f} ê¸°ì‚¬/ì´ˆ")
        
        console.print(table)
        console.print(f"âœ… ë‰´ìŠ¤1 í¬ë¡¤ë§ ì™„ë£Œ! ğŸ‰")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = News1PoliticsCrawler()
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
