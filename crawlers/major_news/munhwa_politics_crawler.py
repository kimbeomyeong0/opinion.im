#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë¬¸í™”ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- ìµœì‹  ì •ì¹˜ ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- í˜ì´ì§€ë„¤ì´ì…˜ API í™œìš©
- 20ì´ˆ ë‚´ í¬ë¡¤ë§ ì™„ë£Œ ëª©í‘œ
"""

import asyncio
import aiohttp
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
import logging
from utils.supabase_manager_unified import UnifiedSupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MunhwaPoliticsCrawler:
    """ë¬¸í™”ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100):
        self.max_articles = max_articles
        self.console = Console()
        self.session: Optional[aiohttp.ClientSession] = None
        self.supabase_manager = UnifiedSupabaseManager()
        
        # ë¬¸í™”ì¼ë³´ ì„¤ì •
        self.base_url = "https://www.munhwa.com"
        self.politics_url = "https://www.munhwa.com/politics"
        self.api_url = "https://www.munhwa.com/_CP/43"
        
        # í†µê³„
        self.total_articles = 0
        self.successful_articles = 0
        self.failed_articles = 0
        self.start_time: Optional[datetime] = None
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
        self.page_size = 12  # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜
        self.max_pages = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def get_politics_article_links(self) -> List[str]:
        """ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘"""
        self.console.print("ğŸ” ë¬¸í™”ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = set()
        
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ìƒë‹¨ ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘
            async with self.session.get(self.politics_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ìƒë‹¨ ê³ ì • ê¸°ì‚¬ (section-news-top)
                    top_articles = soup.select('article.section-news-top a[href^="/article/"]')
                    for article in top_articles:
                        href = article.get('href')
                        if href and href.startswith('/article/'):
                            full_url = self.base_url + href
                            all_links.add(full_url)
                    
                    self.console.print(f"âœ… ìƒë‹¨ ê³ ì • ê¸°ì‚¬ {len(top_articles)}ê°œ ë°œê²¬")
            
            # 2. APIë¥¼ í†µí•œ í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
            for page in range(1, self.max_pages + 1):
                if len(all_links) >= self.max_articles:
                    break
                
                api_params = {
                    'page': page,
                    'domainId': '1000',
                    'mKey': 'politicsAll',
                    'keyword': '',
                    'term': '2',
                    'type': 'C'
                }
                
                try:
                    async with self.session.get(self.api_url, params=api_params) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                            article_links = soup.select('div.card-body h4.title.headline a[href^="/article/"]')
                            
                            for link in article_links:
                                href = link.get('href')
                                if href and href.startswith('/article/'):
                                    full_url = self.base_url + href
                                    all_links.add(full_url)
                            
                            self.console.print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(article_links)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                            
                            # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜ê°€ ì ìœ¼ë©´ ë” ë§ì€ í˜ì´ì§€ í™•ì¸
                            if len(article_links) < self.page_size:
                                break
                                
                        else:
                            self.console.print(f"âš ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                            
                except Exception as e:
                    self.console.print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # 3. ìš°ì¸¡ ì‚¬ì´ë“œë°” ê¸°ì‚¬ë„ ìˆ˜ì§‘
            try:
                async with self.session.get(self.politics_url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ìš°ì¸¡ ì‚¬ì´ë“œë°” ê¸°ì‚¬
                        side_articles = soup.select('div.side-card a[href^="/article/"]')
                        for article in side_articles:
                            href = article.get('href')
                            if href and href.startswith('/article/'):
                                full_url = self.base_url + href
                                all_links.add(full_url)
                        
                        self.console.print(f"âœ… ì‚¬ì´ë“œë°” ê¸°ì‚¬ {len(side_articles)}ê°œ ë°œê²¬")
                        
            except Exception as e:
                self.console.print(f"âš ï¸ ì‚¬ì´ë“œë°” ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            unique_links = list(all_links)[:self.max_articles]
            
            self.console.print(f"ğŸ¯ ì´ {len(unique_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ!")
            return unique_links
            
        except Exception as e:
            self.console.print(f"âŒ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _fetch_article_details(self, article_url: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            async with self.session.get(article_url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title = None
                title_selectors = [
                    'h1.headline',
                    'h1.title',
                    'h2.headline',
                    'h2.title',
                    'title'
                ]
                
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title:
                            break
                
                if not title:
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = None
                content_selectors = [
                    'div.article-content',
                    'div.article-body',
                    'div.content',
                    'div.body',
                    'article'
                ]
                
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_elem.select('script, style, .ad, .advertisement, .related, .comment, .ui-control, .font-control, .share-control, .reaction-control, .sidebar, .side-news, .related-news, .trending-news, .popular-news, .comment-section, .advertisement-section, .social-share, .article-tools, .reading-mode, .dark-mode, .font-size, .bookmark, .print, .share, .reaction, .recommend, .like, .dislike, .angry, .sad, .funny'):
                            unwanted.decompose()
                        
                        content = content_elem.get_text(strip=True, separator='\n')
                        if content and len(content) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                            unwanted_patterns = [
                                'ì½ê¸°ëª¨ë“œ',
                                'ë‹¤í¬ëª¨ë“œ', 
                                'í°íŠ¸í¬ê¸°',
                                'ê°€ê°€ê°€ê°€ê°€ê°€',
                                'ë¶ë§ˆí¬',
                                'ê³µìœ í•˜ê¸°',
                                'í”„ë¦°íŠ¸',
                                'ê¸°ì‚¬ë°˜ì‘',
                                'ì¶”ì²œí•´ìš”',
                                'ì¢‹ì•„ìš”',
                                'ê°ë™ì´ì—ìš”',
                                'í™”ë‚˜ìš”',
                                'ìŠ¬í¼ìš”',
                                'My ì¶”ì²œ ê¸°ì‚¬',
                                'ê°€ì¥ ë§ì´ ì½ì€ ê¸°ì‚¬',
                                'ëŒ“ê¸€ ë§ì€ ê¸°ì‚¬',
                                'ì‹¤ì‹œê°„ ìµœì‹  ë‰´ìŠ¤',
                                'ì£¼ìš”ë‰´ìŠ¤',
                                'ì´ìŠˆNOW',
                                'ê´€ë ¨ê¸°ì‚¬',
                                'ê¸°ì‚¬ ì¶”ì²œ',
                                'êµ¬ë…',
                                'ê¸°ì‚¬ í›„ì›í•˜ê¸°',
                                'ë‹¤ë¥¸ ê¸°ì‚¬ ë”ë³´ê¸°',
                                '+ êµ¬ë…',
                                'ê¸°ì‚¬ í›„ì›í•˜ê¸°',
                                # ì¶”ê°€ íŒ¨í„´ë“¤
                                'ì°¸ì‚¬ëŠ” ìˆ˜ìŠµí–ˆì§€ë§Œ',
                                'å°¹ ëŒ€í†µë ¹ íŒŒë©´',
                                '3ëŒ€ íŠ¹ê²€',
                                'ì´ì¬ëª… ì •ë¶€',
                                'ë¯¸êµ­ ê°€ëŠ”',
                                'ì •ì˜ì„ ',
                                'ê½ƒì´ ëœ',
                                'ì„ìœ í™”í•™',
                                'ì •ë¶€, ì—°ë‚´',
                                'ë””ì§€í„¸ì½˜í…ì¸ ë¶€',
                                'jwrepublic@munhwa.com',
                                'ê°€',
                                'ëŒ€í†µë ¹ì‹¤ì€',
                                '20ì¼ ì˜¤í›„ 5ì‹œ',
                                'ì•„ë¦¬ë‘ êµ­ì œë°©ì†¡',
                                'ì¼€ì´íŒ ë” ë„¥ìŠ¤íŠ¸ ì±•í„°',
                                'K-Pop:The Next Chapter',
                                'ì¼€ë°í—Œ ê°ë…',
                                'íŠ¸ì™€ì´ìŠ¤ì˜',
                                'ìŒì•… í”„ë¡œë“€ì„œ',
                                'ë°©ì†¡ì¸ ì¥ì„±ê·œ',
                                'KíŒì˜ í˜„ì¬ì™€ ì•ìœ¼ë¡œì˜ ë¹„ì „',
                                'KíŒì´ ìŒ“ì•„ì˜¨ ì„¸ê³„ì  ìœ„ìƒ',
                                'ê¸€ë¡œë²Œ ì½˜í…ì¸ ê°€ ë³´ì—¬ì¤€ í™•ì¥ì„±',
                                'ìƒˆë¡œìš´ ê°€ëŠ¥ì„±ì„ ì¡°ë§í•˜ê³ ',
                                'ë‹¤ìŒ ë‹¨ê³„ë¡œ ë‚˜ì•„ê°€ê¸° ìœ„í•œ ë¹„ì „',
                                'í˜„ì¥ì˜ ëª©ì†Œë¦¬ì™€ í†µì°°',
                                'ì•ìœ¼ë¡œì˜ ì •ì±… ë°©í–¥ ì„¤ì •',
                                'í™œìš©í•´ ë‚˜ê°ˆ ê³„íš'
                            ]
                            
                            for pattern in unwanted_patterns:
                                content = content.replace(pattern, '')
                            
                            # ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€ ì •ë¦¬
                            import re
                            
                            # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
                            content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', content)
                            
                            # í•´ì‹œíƒœê·¸ ì œê±°
                            content = re.sub(r'#\s*\w+', '', content)
                            
                            # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸ ì œê±°
                            content = re.sub(r'^\d+\s*$', '', content, flags=re.MULTILINE)
                            
                            # ì‹œê°„ í˜•ì‹ ì œê±° (12:10 ë“±)
                            content = re.sub(r'\b\d{1,2}:\d{2}\b', '', content)
                            
                            # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                            content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
                            
                            # ê¸°ì ì •ë³´ ì´í›„ì˜ ëª¨ë“  ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
                            lines = content.split('\n')
                            cleaned_lines = []
                            found_reporter = False
                            
                            for line in lines:
                                line = line.strip()
                                if not line:
                                    continue
                                
                                # ê¸°ì ì •ë³´ë¥¼ ì°¾ì•˜ìœ¼ë©´ ê·¸ ì´í›„ëŠ” ëª¨ë‘ ì œê±°
                                if 'ê¸°ì' in line and len(line) < 50:
                                    cleaned_lines.append(line)
                                    found_reporter = True
                                    break
                                
                                # ê¸°ì ì •ë³´ë¥¼ ì°¾ê¸° ì „ê¹Œì§€ë§Œ ì¶”ê°€
                                if not found_reporter:
                                    cleaned_lines.append(line)
                            
                            content = '\n'.join(cleaned_lines)
                            
                            if content and len(content) > 100:
                                break
                
                if not content:
                    return None
                
                # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                published_at = None
                time_selectors = [
                    'span.date',
                    'p.byline span.date',
                    'time',
                    'meta[property="article:published_time"]'
                ]
                
                for selector in time_selectors:
                    time_elem = soup.select_one(selector)
                    if time_elem:
                        if selector == 'meta[property="article:published_time"]':
                            time_str = time_elem.get('content', '')
                        else:
                            time_str = time_elem.get_text(strip=True)
                        
                        if time_str:
                            try:
                                # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹±
                                if 'T' in time_str:  # ISO í˜•ì‹
                                    published_at = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                                elif len(time_str) == 19:  # YYYY-MM-DD HH:MM:SS
                                    published_at = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                                elif len(time_str) == 16:  # YYYY-MM-DD HH:MM
                                    published_at = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
                                else:
                                    # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                                    published_at = datetime.now()
                                break
                            except ValueError:
                                continue
                
                if not published_at:
                    published_at = datetime.now()
                
                return {
                    'title': title,
                    'url': article_url,
                    'content': content,
                    'published_at': published_at
                }
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {str(e)}")
            return None
    
    async def crawl_articles(self) -> None:
        """ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        self.start_time = datetime.now()
        
        try:
            # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.get_politics_article_links()
            
            if not article_links:
                self.console.print("[red]ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            self.total_articles = len(article_links)
            self.console.print(f"\nğŸ“° {self.total_articles}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
            
            # 2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                for i, article_url in enumerate(article_links):
                    try:
                        article_data = await self._fetch_article_details(article_url)
                        if article_data:
                            # í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œëŠ” issue_idë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ (í´ëŸ¬ìŠ¤í„°ë§ í›„ ì„¤ì •)
                            # ì„ì‹œ ì´ìŠˆ ID 6 ì‚¬ìš© (ë°ì´í„°ë² ì´ìŠ¤ ì œì•½ì¡°ê±´ ì¤€ìˆ˜)
                            issue = {'id': 6}
                            
                            # ì–¸ë¡ ì‚¬ ì¡°íšŒ
                            media_outlet = self.supabase_manager.get_media_outlet("ë¬¸í™”ì¼ë³´")
                            if not media_outlet:
                                media_outlet = self.supabase_manager.create_media_outlet("ë¬¸í™”ì¼ë³´", "center")
                            
                            # ê¸°ì‚¬ ì €ì¥
                            article_insert_data = {
                                'title': article_data['title'],
                                'url': article_data['url'],
                                'content': article_data['content'],
                                'published_at': article_data['published_at'],
                                'issue_id': issue['id'] if isinstance(issue, dict) else issue,
                                'media_id': media_outlet['id'] if isinstance(media_outlet, dict) else media_outlet,
                                'bias': media_outlet.get('bias', 'center') if isinstance(media_outlet, dict) else 'center'
                            }
                            
                            self.supabase_manager.insert_article(article_insert_data)
                            logger.info(f"ìƒˆ ê¸°ì‚¬ ì‚½ì…: {article_data['title']}")
                            
                            self.successful_articles += 1
                        else:
                            self.failed_articles += 1
                            
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({article_url}): {str(e)}")
                        self.failed_articles += 1
                    
                    progress.advance(task)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if (i + 1) % 10 == 0:
                        progress.update(task, description=f"ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i + 1}/{len(article_links)})")
            
            # 3ë‹¨ê³„: ê²°ê³¼ í‘œì‹œ
            self._display_results()
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        finally:
            if self.session:
                await self.session.close()
    
    def _display_results(self) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        if not self.start_time:
            return
        
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="ğŸ“° ë¬¸í™”ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ê²°ê³¼")
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê²°ê³¼", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(self.total_articles))
        table.add_row("ì„±ê³µ", f"{self.successful_articles}ê°œ")
        table.add_row("ì‹¤íŒ¨", f"{self.failed_articles}ê°œ")
        table.add_row("ì„±ê³µë¥ ", f"{(self.successful_articles/self.total_articles*100):.1f}%" if self.total_articles > 0 else "0%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration:.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{self.successful_articles/duration:.2f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "0 ê¸°ì‚¬/ì´ˆ")
        
        self.console.print(table)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ìš”ì•½
        if self.successful_articles > 0:
            self.console.print(f"\n[green]âœ… ì„±ê³µì ìœ¼ë¡œ {self.successful_articles}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤![/green]")
        
        if self.failed_articles > 0:
            self.console.print(f"\n[yellow]âš ï¸ {self.failed_articles}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/yellow]")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_articles()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with MunhwaPoliticsCrawler() as crawler:
        await crawler.crawl_articles()

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))
