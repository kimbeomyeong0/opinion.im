#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- 20ì´ˆ ë‚´ì™¸ ë¹ ë¥¸ í¬ë¡¤ë§
- 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ëª©í‘œ (ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ fallback ì „ëµ)
- 3ë‹¨ê³„ fallback ì „ëµìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥ ë¶„ì„
- ì ì‘í˜• ë”œë ˆì´ ë° ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
"""
import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Tuple
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich import box
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
import sys
import os
import statistics
from dataclasses import dataclass
from contextlib import asynccontextmanager

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from utils.supabase_manager_unified import UnifiedSupabaseManager
from utils.common.html_parser import HTMLParserUtils

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class CrawlingMetrics:
    """í¬ë¡¤ë§ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    start_time: float
    end_time: float
    total_articles: int
    successful_articles: int
    failed_articles: int
    network_errors: int
    parsing_errors: int
    avg_response_time: float
    response_times: List[float]
    
    @property
    def duration(self) -> float:
        return self.end_time - self.start_time
    
    @property
    def success_rate(self) -> float:
        return (self.successful_articles / self.total_articles * 100) if self.total_articles > 0 else 0
    
    @property
    def articles_per_second(self) -> float:
        return self.successful_articles / self.duration if self.duration > 0 else 0

class YnaPoliticsCrawler:
    """ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100, debug: bool = False):
        self.base_url = "https://www.yna.co.kr"
        self.politics_url = "https://www.yna.co.kr/politics/all"
        self.max_articles = max_articles
        self.console = Console()
        self.debug = debug
        
        # ì ì‘í˜• ë”œë ˆì´ ì„¤ì •
        self.initial_delay = 0.01
        self.current_delay = self.initial_delay
        self.min_delay = 0.005
        self.max_delay = 0.1
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = None
        self.response_times = []
        
        # Supabase ë§¤ë‹ˆì €
        self.supabase_manager = None
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        self.connector = None
        
        # ì—ëŸ¬ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.supabase_manager = UnifiedSupabaseManager()
        
        # HTTP ì„¸ì…˜ ì„¤ì • (ìµœì í™”ëœ ì»¤ë„¥í„°)
        self.connector = aiohttp.TCPConnector(
            limit=100,  # ë™ì‹œ ì—°ê²° ìˆ˜
            limit_per_host=20,  # í˜¸ìŠ¤íŠ¸ë‹¹ ìµœëŒ€ ì—°ê²°
            ttl_dns_cache=300,  # DNS ìºì‹œ TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(total=10, connect=5)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    def _adjust_delay(self, response_time: float):
        """ì‘ë‹µ ì‹œê°„ì— ë”°ë¥¸ ë”œë ˆì´ ìë™ ì¡°ì •"""
        if response_time > 2.0:  # ì‘ë‹µì´ ëŠë¦¬ë©´ ë”œë ˆì´ ì¦ê°€
            self.current_delay = min(self.current_delay * 1.2, self.max_delay)
        elif response_time < 0.5:  # ì‘ë‹µì´ ë¹ ë¥´ë©´ ë”œë ˆì´ ê°ì†Œ
            self.current_delay = max(self.current_delay * 0.8, self.min_delay)
    
    async def _make_request(self, url: str, retries: int = 3) -> Optional[str]:
        """HTTP ìš”ì²­ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        for attempt in range(retries):
            try:
                start_time = time.time()
                async with self.session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        response_time = time.time() - start_time
                        self.response_times.append(response_time)
                        self._adjust_delay(response_time)
                        return html
                    else:
                        self.console.print(f"[red]HTTP {response.status}: {url}[/red]")
                        
            except asyncio.TimeoutError:
                self.console.print(f"[yellow]íƒ€ì„ì•„ì›ƒ (ì‹œë„ {attempt + 1}/{retries}): {url}[/yellow]")
                self.network_errors += 1
            except Exception as e:
                self.console.print(f"[red]ìš”ì²­ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}/{retries}): {url} - {str(e)}[/red]")
                self.network_errors += 1
            
            # ì¬ì‹œë„ ì „ ëŒ€ê¸°
            if attempt < retries - 1:
                await asyncio.sleep(0.5 * (attempt + 1))
        
        return None
    
    async def collect_article_links(self) -> List[str]:
        """ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (3ë‹¨ê³„ fallback ì „ëµ)"""
        self.console.print("ğŸ” ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = set()
        page = 1
        max_pages = 20  # ì•ˆì „ì¥ì¹˜
        
        # 1ë‹¨ê³„: ê¸°ë³¸ í˜ì´ì§€ë„¤ì´ì…˜ ìˆ˜ì§‘
        while len(all_links) < self.max_articles * 1.5 and page <= max_pages:
            page_url = f"{self.politics_url}/{page}" if page > 1 else self.politics_url
            
            html = await self._make_request(page_url)
            if not html:
                self.console.print(f"[yellow]í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰[/yellow]")
                break
            
            soup = BeautifulSoup(html, 'html.parser')
            links = self._extract_links_from_page(soup)
            
            if not links:
                self.console.print(f"[yellow]í˜ì´ì§€ {page}ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ[/yellow]")
                break
            
            new_links = [link for link in links if link not in all_links]
            all_links.update(new_links)
            
            self.console.print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(new_links)}ê°œ ìƒˆ ë§í¬ (ì´ {len(all_links)}ê°œ)")
            
            if len(new_links) == 0:  # ë” ì´ìƒ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                break
            
            page += 1
            await asyncio.sleep(self.current_delay)
        
        # 2ë‹¨ê³„: ì¶”ê°€ í˜ì´ì§€ ìˆ˜ì§‘ (ëª©í‘œ ë‹¬ì„±í•˜ì§€ ëª»í•œ ê²½ìš°)
        if len(all_links) < self.max_articles:
            self.console.print(f"[yellow]1ë‹¨ê³„ì—ì„œ {len(all_links)}ê°œë§Œ ìˆ˜ì§‘, ì¶”ê°€ ìˆ˜ì§‘ ì§„í–‰[/yellow]")
            additional_pages = min(10, max_pages - page + 1)
            
            for extra_page in range(page, page + additional_pages):
                if len(all_links) >= self.max_articles * 1.5:
                    break
                    
                page_url = f"{self.politics_url}/{extra_page}"
                html = await self._make_request(page_url)
                
                if html:
                    soup = BeautifulSoup(html, 'html.parser')
                    links = self._extract_links_from_page(soup)
                    new_links = [link for link in links if link not in all_links]
                    all_links.update(new_links)
                    
                    self.console.print(f"ğŸ“„ ì¶”ê°€ í˜ì´ì§€ {extra_page}: {len(new_links)}ê°œ ìƒˆ ë§í¬ (ì´ {len(all_links)}ê°œ)")
                    await asyncio.sleep(self.current_delay)
        
        # 3ë‹¨ê³„: ìµœì¢… í™•ì¸ ë° ì •ë¦¬ (100ê°œ + ì—¬ìœ ë¶„)
        final_links = list(all_links)[:self.max_articles + 20]  # 100ê°œ + 20ê°œ ì—¬ìœ ë¶„
        
        self.console.print(f"âœ… ì´ {len(final_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ (ëª©í‘œ: 100ê°œ)")
        return final_links
    
    def _extract_links_from_page(self, soup: BeautifulSoup) -> List[str]:
        """í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        # ê¸°ë³¸ ì„ íƒìë¡œ ë§í¬ ì¶”ì¶œ
        article_elements = soup.select("li div.news-con strong.tit-news a")
        
        for element in article_elements:
            href = element.get('href')
            if href:
                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                if href.startswith('/'):
                    full_url = urljoin(self.base_url, href)
                else:
                    full_url = href
                
                # ì •ì¹˜ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸
                if '/politics/' in full_url or '/view/' in full_url:
                    links.append(full_url)
        
        # fallback: ë‹¤ë¥¸ ì„ íƒì ì‹œë„
        if not links:
            fallback_elements = soup.select("a[href*='/view/']")
            for element in fallback_elements:
                href = element.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    if '/politics/' in full_url or '/view/' in full_url:
                        links.append(full_url)
        
        return links
    
    async def extract_article_content(self, url: str) -> Optional[Dict]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (3ë‹¨ê³„ fallback ì „ëµ)"""
        try:
            html = await self._make_request(url)
            if not html:
                return None
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„ íƒìë¡œ ì œëª© ì¶”ì¶œ
            title = self._extract_title_fallback(soup)
            if not title:
                if self.debug:
                    self.console.print(f"[yellow]ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {url}[/yellow]")
                return None
            
            # 2ë‹¨ê³„: ê¸°ë³¸ ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content_fallback(soup)
            if not content:
                if self.debug:
                    self.console.print(f"[yellow]ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {url}[/yellow]")
                return None
            
            # 3ë‹¨ê³„: ë°œí–‰ì¼ ì¶”ì¶œ
            published_at = self._extract_published_at_fallback(soup)
            
            return {
                'title': title,
                'content': content,
                'url': url,
                'published_at': published_at
            }
            
        except Exception as e:
            self.parsing_errors += 1
            if self.debug:
                self.console.print(f"[red]ê¸°ì‚¬ ì¶”ì¶œ ì˜¤ë¥˜: {url} - {str(e)}[/red]")
            return None
    
    def _extract_title_fallback(self, soup: BeautifulSoup) -> Optional[str]:
        """ì œëª© ì¶”ì¶œ (3ë‹¨ê³„ fallback)"""
        # 1ì°¨: ê¸°ë³¸ ì„ íƒì
        title_elem = soup.select_one("strong.tit-news a")
        if title_elem:
            title = title_elem.get_text(strip=True)
            if title and len(title) > 5:
                return title
        
        # 2ì°¨: ëŒ€ì•ˆ ì„ íƒìë“¤
        fallback_selectors = [
            "h1",
            ".title",
            ".headline",
            "meta[property='og:title']",
            "title"
        ]
        
        for selector in fallback_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector == "meta[property='og:title']":
                    title = elem.get('content', '').strip()
                else:
                    title = elem.get_text(strip=True)
                
                if title and len(title) > 5 and len(title) < 200:
                    return title
        
        return None
    
    def _extract_content_fallback(self, soup: BeautifulSoup) -> Optional[str]:
        """ë³¸ë¬¸ ì¶”ì¶œ (3ë‹¨ê³„ fallback)"""
        # 1ì°¨: ê¸°ë³¸ ì„ íƒì
        content = self._extract_content_with_selector(soup, "div.story-news.article p")
        if content:
            return content
        
        # 2ì°¨: ëŒ€ì•ˆ ì„ íƒìë“¤
        fallback_selectors = [
            ".article-content p",
            ".news-content p",
            ".content p",
            "article p",
            ".story p"
        ]
        
        for selector in fallback_selectors:
            content = self._extract_content_with_selector(soup, selector)
            if content:
                return content
        
        # 3ì°¨: ëª¨ë“  p íƒœê·¸ì—ì„œ ì¶”ì¶œ (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
        all_paragraphs = soup.find_all('p')
        if all_paragraphs:
            return self._clean_content_from_paragraphs(all_paragraphs)
        
        return None
    
    def _extract_content_with_selector(self, soup: BeautifulSoup, selector: str) -> Optional[str]:
        """íŠ¹ì • ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ"""
        paragraphs = soup.select(selector)
        if paragraphs:
            return self._clean_content_from_paragraphs(paragraphs)
        return None
    
    def _clean_content_from_paragraphs(self, paragraphs: List) -> str:
        """ë¬¸ë‹¨ì—ì„œ ë³¸ë¬¸ ì •ë¦¬"""
        content = []
        
        for p in paragraphs:
            txt = p.get_text(strip=True)
            if not txt:
                continue
            
            # ë¶ˆí•„ìš”í•œ ë‚´ìš© í•„í„°ë§
            if any(keyword in txt for keyword in [
                "ì €ì‘ê¶Œì", "ì¬ë°°í¬ ê¸ˆì§€", "ì—°í•©ë‰´ìŠ¤", "yna.co.kr",
                "ê¸°ì ì´ë©”ì¼", "ê¸°ì ì—°ë½ì²˜", "ê´‘ê³ ", "sponsored"
            ]):
                continue
            
            # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
            if "@" in txt and ("@" in txt.split()[-1] or len(txt) < 50):
                continue
            
            # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œê±°
            if len(txt) < 10:
                continue
            
            content.append(txt)
        
        return "\n\n".join(content) if content else ""
    
    def _extract_published_at_fallback(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ë°œí–‰ì¼ ì¶”ì¶œ (3ë‹¨ê³„ fallback)"""
        # 1ì°¨: ê¸°ë³¸ ì„ íƒì
        time_elem = soup.select_one("span.time")
        if time_elem:
            time_text = time_elem.get_text(strip=True)
            try:
                # "2025-08-21 21:35" í˜•ì‹ íŒŒì‹±
                return datetime.strptime(time_text, "%Y-%m-%d %H:%M")
            except ValueError:
                pass
        
        # 2ì°¨: ëŒ€ì•ˆ ì„ íƒìë“¤
        fallback_selectors = [
            ".date",
            ".publish-date",
            ".article-date",
            "meta[property='article:published_time']",
            "time"
        ]
        
        for selector in fallback_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector == "meta[property='article:published_time']":
                    time_text = elem.get('content', '').strip()
                else:
                    time_text = elem.get_text(strip=True)
                
                if time_text:
                    try:
                        # ISO í˜•ì‹ íŒŒì‹± ì‹œë„
                        return datetime.fromisoformat(time_text.replace('Z', '+00:00'))
                    except ValueError:
                        pass
        
        # 3ì°¨: í˜„ì¬ ì‹œê°„ ì‚¬ìš© (fallback)
        return datetime.now()
    
    async def save_to_database(self, article_data: Dict) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥"""
        try:
            # ì—°í•©ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            media_outlet = self.supabase_manager.get_media_outlet("ì—°í•©ë‰´ìŠ¤")
            if not media_outlet:
                # ì—°í•©ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„±
                media_id = self.supabase_manager.create_media_outlet("ì—°í•©ë‰´ìŠ¤", "center")
            else:
                media_id = media_outlet['id']
            
            # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            processed_data = {
                'title': article_data['title'],
                'content': article_data['content'],
                'url': article_data['url'],
                'published_at': article_data['published_at'].isoformat(),
                'media_id': media_id,
                'bias': 'center',  # ì—°í•©ë‰´ìŠ¤ëŠ” ì¤‘ë„ ì„±í–¥
                'issue_id': 6  # ì„ì‹œ issue_id
            }
            
            # ì¤‘ë³µ í™•ì¸ (URLë¡œ ì§ì ‘ ì¿¼ë¦¬)
            try:
                result = self.supabase_manager.client.table('articles').select('id').eq('url', article_data['url']).execute()
                if result.data:
                    return False  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬
            except Exception as e:
                self.console.print(f"[yellow]ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {str(e)}[/yellow]")
                # ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ì‚¬ ì €ì¥ ì‹œë„
            
            # ìƒˆ ê¸°ì‚¬ ì‚½ì…
            article_id = self.supabase_manager.insert_article(processed_data)
            if article_id:
                self.console.print(f"âœ… ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:50]}...")
                return True
            else:
                self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:50]}...[/red]")
                return False
                
        except Exception as e:
            self.console.print(f"[red]ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì˜¤ë¥˜: {str(e)}[/red]")
            return False
    
    async def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        self.console.print("ğŸš€ ì—°í•©ë‰´ìŠ¤ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘!")
        self.console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.collect_article_links()
            
            if not article_links:
                self.console.print("[red]ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            # 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ í¬ë¡¤ë§ (100ê°œ ë‹¬ì„± ì‹œ ìë™ ì¤‘ë‹¨)
            self.console.print(f"ğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘... (ëª©í‘œ: 100ê°œ)")
            
            successful_articles = 0
            failed_articles = 0
            target_reached = False  # 100ê°œ ë‹¬ì„± í”Œë˜ê·¸
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                
                # 100ê°œ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘... (ëª©í‘œ: 100ê°œ)", total=100)
                
                # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê¸°ì‚¬ í¬ë¡¤ë§
                semaphore = asyncio.Semaphore(20)  # ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
                
                async def process_article(link: str):
                    nonlocal successful_articles, failed_articles, target_reached
                    
                    # 100ê°œ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                    if target_reached or successful_articles >= 100:
                        return
                    
                    async with semaphore:
                        # ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œ 100ê°œ ë‹¬ì„±í–ˆëŠ”ì§€ ì¬í™•ì¸
                        if target_reached or successful_articles >= 100:
                            return
                            
                        article_data = await self.extract_article_content(link)
                        
                        if article_data:
                            if await self.save_to_database(article_data):
                                successful_articles += 1
                                # 100ê°œ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                                if successful_articles >= 100:
                                    target_reached = True
                                    self.console.print(f"[green]ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜(100ê°œ) ë‹¬ì„±! í¬ë¡¤ë§ ì¤‘ë‹¨[/green]")
                                    return
                            else:
                                failed_articles += 1
                        else:
                            failed_articles += 1
                        
                        # ì§„í–‰ë¥ ì„ 100ê°œ ê¸°ì¤€ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                        if successful_articles <= 100:
                            progress.update(task, completed=successful_articles)
                        await asyncio.sleep(self.current_delay)
                
                # ëª¨ë“  ê¸°ì‚¬ ë³‘ë ¬ ì²˜ë¦¬ (100ê°œ ë‹¬ì„± ì‹œ ìë™ ì¤‘ë‹¨)
                tasks = [process_article(link) for link in article_links]
                await asyncio.gather(*tasks)
            
            # 3ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬ ë° ë©”íŠ¸ë¦­ ê³„ì‚°
            end_time = time.time()
            
            # 100ê°œ ë‹¬ì„± ì‹œ ì‹¤ì œ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°
            actual_processed = min(successful_articles, 100)
            
            self.metrics = CrawlingMetrics(
                start_time=start_time,
                end_time=end_time,
                total_articles=actual_processed,  # ì‹¤ì œ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜
                successful_articles=successful_articles,
                failed_articles=failed_articles,
                network_errors=self.network_errors,
                parsing_errors=self.parsing_errors,
                avg_response_time=statistics.mean(self.response_times) if self.response_times else 0,
                response_times=self.response_times
            )
            
            # ê²°ê³¼ ì¶œë ¥
            self._display_results()
            
        except Exception as e:
            self.console.print(f"[red]í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}[/red]")
            logger.error(f"í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)
    
    def _display_results(self):
        """í¬ë¡¤ë§ ê²°ê³¼ ì¶œë ¥"""
        if not self.metrics:
            return
        
        self.console.print("\n" + "=" * 50)
        self.console.print("      ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ê²°ê³¼      ")
        self.console.print("=" * 50)
        
        # ê²°ê³¼ í…Œì´ë¸”
        table = Table(box=box.ROUNDED)
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê°’", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(self.metrics.total_articles))
        table.add_row("ì„±ê³µ", f"{self.metrics.successful_articles}ê°œ")
        table.add_row("ì‹¤íŒ¨", f"{self.metrics.failed_articles}ê°œ")
        table.add_row("ì„±ê³µë¥ ", f"{self.metrics.success_rate:.1f}%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{self.metrics.duration:.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{self.metrics.articles_per_second:.2f} ê¸°ì‚¬/ì´ˆ")
        table.add_row("ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜", str(self.metrics.network_errors))
        table.add_row("íŒŒì‹± ì˜¤ë¥˜", str(self.metrics.parsing_errors))
        
        if self.metrics.response_times:
            table.add_row("í‰ê·  ì‘ë‹µì‹œê°„", f"{self.metrics.avg_response_time:.3f}ì´ˆ")
            table.add_row("ìµœì†Œ ì‘ë‹µì‹œê°„", f"{min(self.metrics.response_times):.3f}ì´ˆ")
            table.add_row("ìµœëŒ€ ì‘ë‹µì‹œê°„", f"{max(self.metrics.response_times):.3f}ì´ˆ")
        
        self.console.print(table)
        
        # ì„±ëŠ¥ ë¶„ì„
        # ì„±ëŠ¥ ë¶„ì„ (100ê°œ ê¸°ì¤€)
        if self.metrics.duration > 25:
            self.console.print(f"[yellow]âš ï¸  ëª©í‘œ ì‹œê°„(25ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {self.metrics.duration:.1f}ì´ˆ[/yellow]")
        else:
            self.console.print(f"[green]âœ… ëª©í‘œ ì‹œê°„ ë‚´ ì™„ë£Œ: {self.metrics.duration:.1f}ì´ˆ[/green]")
        
        if self.metrics.successful_articles >= 100:
            self.console.print(f"[green]âœ… ëª©í‘œ ê¸°ì‚¬ ìˆ˜(100ê°œ) ë‹¬ì„±: {self.metrics.successful_articles}ê°œ[/green]")
            if target_reached:
                self.console.print("[blue]ğŸ’¡ 100ê°œ ë‹¬ì„±ìœ¼ë¡œ í¬ë¡¤ë§ì´ ìë™ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.[/blue]")
        else:
            self.console.print(f"[yellow]âš ï¸  ëª©í‘œ ê¸°ì‚¬ ìˆ˜(100ê°œ) ë¯¸ë‹¬ì„±: {self.metrics.successful_articles}ê°œ[/yellow]")
        
        self.console.print("âœ… ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ! ğŸ‰")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.collect_article_links()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with YnaPoliticsCrawler(debug=False) as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))
