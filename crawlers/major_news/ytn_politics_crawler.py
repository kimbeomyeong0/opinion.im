#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
YTN ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- ëŒ€ìƒ: YTN ì •ì¹˜ ì„¹ì…˜ (mcd=0101)
- ë°©ì‹: HTML íŒŒì‹± + AJAX API (BeautifulSoup)
- ëª©í‘œ: ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- ì„±ëŠ¥: asyncio + httpx ë³‘ë ¬ ì²˜ë¦¬
"""

import asyncio
import json
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from urllib.parse import urljoin

import httpx
from bs4 import BeautifulSoup
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table

# Supabase ì—°ë™
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.legacy.supabase_manager_v2 import SupabaseManagerV2

console = Console()


class YTNPoliticsCrawler:
    """YTN ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://www.ytn.co.kr"
        self.first_page_url = "https://www.ytn.co.kr/news/list.php?mcd=0101"
        self.ajax_url = "https://www.ytn.co.kr/ajax/getMoreNews.php"
        self.target_count = 100
        self.max_pages = 20  # 100ê°œ ìˆ˜ì§‘ì„ ìœ„í•´ í˜ì´ì§€ ìˆ˜ ì¦ê°€
        self.timeout = 10.0
        self.today = datetime.now().strftime("%Y-%m-%d")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì • (ìµœê·¼ 7ì¼)
        from datetime import timedelta
        self.date_range = []
        for i in range(7):  # ì˜¤ëŠ˜ë¶€í„° 7ì¼ ì „ê¹Œì§€
            date = datetime.now() - timedelta(days=i)
            self.date_range.append(date.strftime("%Y-%m-%d"))
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥
        self.articles = []
        self.seen_urls = set()
        
        # Supabase ì—°ë™
        self.supabase_manager = SupabaseManagerV2()
        self.media_name = "YTN"
        self.media_bias = "Center"
        
        # HTTP í—¤ë”
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }

    async def _make_request(self, url: str, params: Optional[Dict] = None) -> Optional[str]:
        """HTTP GET ìš”ì²­ ìˆ˜í–‰"""
        try:
            async with httpx.AsyncClient(
                headers=self.headers,
                timeout=self.timeout,
                follow_redirects=True
            ) as client:
                if params:
                    response = await client.get(url, params=params)
                else:
                    response = await client.get(url)
                
                response.raise_for_status()
                return response.text
                
        except httpx.HTTPStatusError as e:
            console.print(f"âŒ HTTP ì˜¤ë¥˜: {e.response.status_code} - {url}")
            return None
        except httpx.TimeoutException:
            console.print(f"â° íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except Exception as e:
            console.print(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {str(e)} - {url}")
            return None

    async def _extract_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            html = await self._make_request(url)
            if not html:
                return ""
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ - span íƒœê·¸ì—ì„œ ë³¸ë¬¸ ì°¾ê¸°
            content_elem = soup.find('span', style=lambda x: x and 'word-break:keep-all' in x)
            if content_elem:
                # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                content = content_elem.get_text(separator='\n', strip=True)
                # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
                content = re.sub(r'\n\s*\n', '\n\n', content)
                return content.strip()
            
            # ëŒ€ì•ˆ: ë‹¤ë¥¸ ë³¸ë¬¸ ì„ íƒì ì‹œë„
            content_elem = soup.find('div', class_='content') or soup.find('div', class_='article_content')
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                content = re.sub(r'\n\s*\n', '\n\n', content)
                return content.strip()
            
            return ""
            
        except Exception as e:
            console.print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)} - {url}")
            return ""

    async def _make_post_request(self, url: str, data: Dict) -> Optional[Dict]:
        """HTTP POST ìš”ì²­ ìˆ˜í–‰ (AJAXìš©)"""
        try:
            async with httpx.AsyncClient(
                headers=self.headers,
                timeout=self.timeout,
                follow_redirects=True
            ) as client:
                response = await client.post(url, data=data)
                response.raise_for_status()
                
                if response.text:
                    return json.loads(response.text)
                return None
                
        except httpx.HTTPStatusError as e:
            console.print(f"âŒ HTTP ì˜¤ë¥˜: {e.response.status_code} - {url}")
            return None
        except httpx.TimeoutException:
            console.print(f"â° íƒ€ì„ì•„ì›ƒ: {url}")
            return None
        except json.JSONDecodeError as e:
            console.print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)} - {url}")
            return None
        except Exception as e:
            console.print(f"âŒ ìš”ì²­ ì˜¤ë¥˜: {str(e)} - {url}")
            return None

    def _parse_date(self, date_str: str) -> Optional[str]:
        """ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            if not date_str or date_str.strip() == "":
                return None
                
            # YTN ë‚ ì§œ í˜•ì‹: "2025.08.22. 14:21"
            if re.match(r'^\d{4}\.\d{2}\.\d{2}\.\s+\d{2}:\d{2}$', date_str.strip()):
                date_part = date_str.strip().split('.')[0:3]
                year, month, day = date_part
                return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
            
            return None
        except Exception as e:
            console.print(f"âŒ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {str(e)} - {date_str}")
            return None

    def _clean_title(self, title: str) -> str:
        """ì œëª© ì •ë¦¬"""
        if not title:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        title = re.sub(r'<[^>]+>', '', title)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        title = re.sub(r'\s+', ' ', title)
        title = title.strip()
        
        return title

    def _parse_articles_from_html(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ê¸°ì‚¬ ëª©ë¡ íŒŒì‹±"""
        articles = []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # div.news_listì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ
            news_items = soup.find_all('div', class_='news_list')
            # div.news_listì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ
            for item in news_items:
                try:
                    # text_area ì•ˆì—ì„œ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    text_area = item.find('div', class_='text_area')
                    if not text_area:
                        continue
                    
                    title_elem = text_area.find('div', class_='title')
                    if not title_elem:
                        continue
                    
                    title_link = title_elem.find('a')
                    if not title_link:
                        continue
                    
                    title = self._clean_title(title_link.get_text())
                    if not title:
                        continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    href = title_link.get('href')
                    if not href:
                        continue
                    
                    # ì ˆëŒ€ URL ë³€í™˜
                    if href.startswith('/'):
                        url = urljoin(self.base_url, href)
                    else:
                        url = href
                    
                    # ì¤‘ë³µ í™•ì¸
                    if url in self.seen_urls:
                        continue
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_elem = text_area.find('div', class_='info')
                    date = None
                    if date_elem:
                        date_elem = date_elem.find('div', class_='date')
                        if date_elem:
                            date_str = date_elem.get_text().strip()
                            date = self._parse_date(date_str)
                    
                    # ë‚ ì§œ í•„í„°ë§ ì„ì‹œ ì œê±° - 100ê°œ ìˆ˜ì§‘ì„ ìœ„í•´
                    # if date not in self.date_range:
                    #     continue
                    
                    # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                    article = {
                        "title": title,
                        "url": url,
                        "date": date,
                        "content": ""  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— ë³„ë„ë¡œ ìˆ˜ì§‘
                    }
                    
                    articles.append(article)
                    
                except Exception as e:
                    console.print(f"âŒ ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
        
        except Exception as e:
            console.print(f"âŒ HTML íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        
        return articles
    
    def _parse_articles_from_json(self, json_data: Dict) -> List[Dict]:
        """JSON ì‘ë‹µì—ì„œ ê¸°ì‚¬ ëª©ë¡ íŒŒì‹±"""
        articles = []
        
        try:
            if not json_data or 'data' not in json_data:
                return articles
            
            for data in json_data['data']:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title = data.get('title', '').strip()
                    if not title:
                        continue
                    
                    title = self._clean_title(title)
                    if not title:
                        continue
                    
                    # join_keyë¡œ URL ìƒì„±
                    join_key = data.get('join_key', '')
                    if not join_key:
                        continue
                    
                    url = f"https://www.ytn.co.kr/_ln/0101_{join_key}"
                    
                    # ì¤‘ë³µ í™•ì¸
                    if url in self.seen_urls:
                        continue
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date_str = data.get('n_date', '')
                    date = self._parse_date(date_str)
                    
                    # ë‚ ì§œ í•„í„°ë§ ì„ì‹œ ì œê±° - 100ê°œ ìˆ˜ì§‘ì„ ìœ„í•´
                    # if date not in self.date_range:
                    #     continue
                    
                    # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                    article = {
                        "title": title,
                        "url": url,
                        "date": date,
                        "content": ""  # ë³¸ë¬¸ì€ ë‚˜ì¤‘ì— ë³„ë„ë¡œ ìˆ˜ì§‘
                    }
                    
                    articles.append(article)
                    # seen_urlsëŠ” ìƒìœ„ì—ì„œ ê´€ë¦¬
                    
                except Exception as e:
                    console.print(f"âŒ JSON ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
        
        except Exception as e:
            console.print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
        
        return articles

    async def _collect_from_first_page(self) -> int:
        """ì²« í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print("ğŸ“° ì²« í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        
        html = await self._make_request(self.first_page_url)
        if not html:
            console.print("âŒ ì²« í˜ì´ì§€ ìˆ˜ì§‘ ì‹¤íŒ¨")
            return 0
        
        articles = self._parse_articles_from_html(html)
        
        # ì¤‘ë³µ ì œê±° ë° ê¸°ì‚¬ ì¶”ê°€
        new_articles = []
        for article in articles:
            if len(self.articles) + len(new_articles) >= self.target_count:
                break
            if article["url"] not in self.seen_urls:
                new_articles.append(article)
                self.seen_urls.add(article["url"])
        
        # ê¸°ì‚¬ ì¶”ê°€
        self.articles.extend(new_articles)
        
        collected = len(new_articles)
        console.print(f"âœ… ì²« í˜ì´ì§€: {collected}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {len(self.articles)}ê°œ)")
        return collected

    async def _collect_from_ajax_pages(self) -> int:
        """AJAX í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        console.print("ğŸ“° AJAX í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...")
        
        total_collected = 0
        
        for page in range(2, self.max_pages + 1):
            # ëª©í‘œ ë‹¬ì„± ì‹œ ì¤‘ë‹¨
            if len(self.articles) >= self.target_count:
                break
            
            # pivot ê°’ ê³„ì‚° (ë§ˆì§€ë§‰ ê¸°ì‚¬ì˜ join_key ì¶”ì¶œ)
            pivot = ""
            if self.articles:
                # URLì—ì„œ join_key ì¶”ì¶œ: _ln/0101_{join_key}
                last_url = self.articles[-1]["url"]
                match = re.search(r'0101_(\d+)', last_url)
                if match:
                    pivot = match.group(1)
            
            # POST ë°ì´í„° ì¤€ë¹„
            post_data = {
                "mcd": "0101",
                "hcd": "",
                "page": str(page),
                "pivot": pivot
            }
            
            json_data = await self._make_post_request(self.ajax_url, post_data)
            if not json_data:
                console.print(f"âš ï¸ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì‹¤íŒ¨ (JSON ì‘ë‹µ ì—†ìŒ)")
                continue
            
            articles = self._parse_articles_from_json(json_data)
            
            # ì¤‘ë³µ ì œê±° ë° ê¸°ì‚¬ ì¶”ê°€
            page_collected = 0
            for article in articles:
                if len(self.articles) >= self.target_count:
                    break
                if article["url"] not in self.seen_urls:
                    self.articles.append(article)
                    self.seen_urls.add(article["url"])
                    page_collected += 1
            
            total_collected += page_collected
            console.print(f"âœ… í˜ì´ì§€ {page}: {page_collected}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {len(self.articles)}ê°œ)")
            
            # ì—°ì† 3í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
            if page_collected == 0:
                console.print(f"âš ï¸ í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ì—†ìŒ")
                if page > 5:  # 5í˜ì´ì§€ ì´í›„ë¶€í„°ë§Œ ì¤‘ë‹¨ ê³ ë ¤
                    console.print(f"   í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ ì—†ìœ¼ë¯€ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break
            
            # ì§§ì€ ë”œë ˆì´
            await asyncio.sleep(0.1)
        
        return total_collected

    async def _collect_article_contents(self, articles: List[Dict]) -> None:
        """ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ ë‚´ìš© ìˆ˜ì§‘"""
        console.print("ğŸ“ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            task = progress.add_task("ë³¸ë¬¸ ìˆ˜ì§‘", total=len(articles))
            
            for i, article in enumerate(articles):
                try:
                    # ë³¸ë¬¸ ì¶”ì¶œ
                    content = await self._extract_article_content(article["url"])
                    article["content"] = content
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress.update(task, advance=1)
                    
                    # ì§§ì€ ë”œë ˆì´ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    console.print(f"âŒ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {article['title'][:30]}... - {str(e)}")
                    article["content"] = ""
                    progress.update(task, advance=1)
                    continue

    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘"""
        start_time = datetime.now()
        
        console.print("ğŸš€ YTN ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘")
        console.print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ ë²”ìœ„: {self.date_range}")
        console.print(f"ğŸ¯ ëª©í‘œ: {self.target_count}ê°œ ê¸°ì‚¬")
        console.print(f"â±ï¸ ëª©í‘œ ì‹œê°„: 20ì´ˆ ì´ë‚´")
        console.print("=" * 50)
        
        # ì²« í˜ì´ì§€ ìˆ˜ì§‘
        await self._collect_from_first_page()
        
        # AJAX í˜ì´ì§€ ìˆ˜ì§‘
        if len(self.articles) < self.target_count:
            await self._collect_from_ajax_pages()
        
        # ê²°ê³¼ ì œí•œ
        if len(self.articles) > self.target_count:
            self.articles = self.articles[:self.target_count]
        
        # ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
        await self._collect_article_contents(self.articles)
        
        end_time = datetime.now()
        elapsed = (end_time - start_time).total_seconds()
        
        console.print("=" * 50)
        console.print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        console.print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        console.print(f"   - ì´ ìˆ˜ì§‘: {len(self.articles)}ê°œ")
        console.print(f"   - ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
        if elapsed > 0:
            speed = len(self.articles) / elapsed
            console.print(f"   - í‰ê·  ì†ë„: {speed:.1f} ê¸°ì‚¬/ì´ˆ")
        
        if elapsed <= 20:
            console.print("âœ… ëª©í‘œ ì‹œê°„ ë‹¬ì„±! (20ì´ˆ ì´ë‚´)")
        else:
            console.print("â° ëª©í‘œ ì‹œê°„ ì´ˆê³¼")
        
        return self.articles

    def display_results(self, articles: List[Dict]):
        """ìˆ˜ì§‘ ê²°ê³¼ í‘œì‹œ"""
        if not articles:
            console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        console.print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì˜ˆì‹œ:")
        
        # í…Œì´ë¸” ìƒì„±
        table = Table(title="ìˆ˜ì§‘ëœ ê¸°ì‚¬ ëª©ë¡")
        table.add_column("ë²ˆí˜¸", justify="right", style="cyan", no_wrap=True)
        table.add_column("ì œëª©", style="magenta", max_width=50)
        table.add_column("ë‚ ì§œ", justify="center", style="green")
        table.add_column("URL", style="blue", max_width=50)
        
        # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
        for i, article in enumerate(articles[:10], 1):
            title = article['title']
            if len(title) > 50:
                title = title[:47] + "..."
            
            url = article['url']
            if len(url) > 50:
                url = url[:47] + "..."
            
            table.add_row(
                str(i),
                title,
                article['date'],
                url
            )
        
        console.print(table)

    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        console.print(f"\nğŸ’¾ Supabaseì— {len(articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
        
        # media_outlet ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
        if not media_outlet:
            console.print(f"âŒ ë¯¸ë””ì–´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.media_name}")
            return {"success": 0, "failed": len(articles)}
        
        # issue ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
        issue_title = f"YTN ì •ì¹˜ ë‰´ìŠ¤ - {datetime.now().strftime('%Yë…„ %mì›” %dì¼')}"
        issue = self.supabase_manager.get_issue_by_title(issue_title)
        if not issue:
            issue_id = self.supabase_manager.create_issue(issue_title, "ì •ì¹˜", "YTN ì •ì¹˜ ê´€ë ¨ ë‰´ìŠ¤")
            if issue_id:
                issue = {"id": issue_id}
            else:
                console.print("âŒ Issue ìƒì„± ì‹¤íŒ¨")
                return {"success": 0, "failed": len(articles)}
        
        success_count = 0
        failed_count = 0
        
        for i, article in enumerate(articles, 1):
            try:
                # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
                article_data = {
                    "title": article["title"],
                    "url": article["url"],
                    "content": article.get("content", ""),  # ìˆ˜ì§‘ëœ ë³¸ë¬¸ ì‚¬ìš©
                    "published_at": article["date"],
                    "media_id": media_outlet["id"],
                    "issue_id": issue["id"],
                    "bias": self.media_bias
                }
                
                # Supabaseì— ì €ì¥
                result = self.supabase_manager.insert_article(article_data)
                
                if result:
                    success_count += 1
                    if i <= 5:  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸ ì¶œë ¥
                        console.print(f"âœ… [{i}/{len(articles)}] ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                else:
                    failed_count += 1
                    console.print(f"âŒ [{i}/{len(articles)}] ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                
            except Exception as e:
                failed_count += 1
                console.print(f"âŒ [{i}/{len(articles)}] ì €ì¥ ì˜¤ë¥˜: {str(e)}")
        
        console.print(f"\nğŸ“Š ì €ì¥ ê²°ê³¼:")
        console.print(f"   - ì„±ê³µ: {success_count}ê°œ")
        console.print(f"   - ì‹¤íŒ¨: {failed_count}ê°œ")
        console.print(f"   - ì„±ê³µë¥ : {success_count/len(articles)*100:.1f}%")
        
        return {"success": success_count, "failed": failed_count}


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = YTNPoliticsCrawler()
    
    try:
        # ê¸°ì‚¬ ìˆ˜ì§‘
        articles = await crawler.collect_all_articles()
        
        # ê²°ê³¼ í‘œì‹œ
        crawler.display_results(articles)
        
        # Supabaseì— ì €ì¥
        await crawler.save_to_supabase(articles)
        
    except KeyboardInterrupt:
        console.print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        console.print(f"âŒ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")


if __name__ == "__main__":
    asyncio.run(main())
