#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
JTBC ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ê¸°
JTBC ë‰´ìŠ¤ APIì—ì„œ ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Supabaseì— ì €ì¥
"""
import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import asyncio
import httpx
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Set
import logging
from urllib.parse import urljoin
from utils.supabase_manager_unified import UnifiedSupabaseManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class JTBCPoliticsCollector:
    def __init__(self):
        self.articles = []
        self.seen_urls: Set[str] = set()
        self.collected_count = 0
        self.target_count = 50
        self.supabase_manager = UnifiedSupabaseManager()
        
        # JTBC ì„¤ì •
        self.base_url = "https://news-api.jtbc.co.kr"
        self.api_endpoint = "/v1/get/contents/section/list/articles"
        self.media_name = "JTBC"
        self.media_id = 13  # JTBC media_id (media_outlets í…Œì´ë¸” ê¸°ì¤€)
        
        # API ì„¤ì •
        self.max_pages = 5  # ìµœëŒ€ 5í˜ì´ì§€ (ê¸°ì‚¬ 50ê°œ ëª©í‘œ)
        self.page_size = 10  # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜
        self.timeout = 5.0  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_concurrent_requests = 10
        
        # ì—ëŸ¬ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        self.content_errors = 0

    def clean_content(self, text: str) -> str:
        """ê¸°ì‚¬ ë‚´ìš© ì •ì œ"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        text = text.replace('&hellip;', '...').replace('&quot;', '"')
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text

    def parse_date(self, date_text: str) -> str:
        """ë‚ ì§œ íŒŒì‹± (ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜)"""
        if not date_text:
            return datetime.now().isoformat()
        
        try:
            # "2025-08-22T11:33" í˜•ì‹ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            dt = datetime.fromisoformat(date_text)
            return dt.isoformat()
        except Exception as e:
            logger.warning(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_text}, ì˜¤ë¥˜: {e}")
            return datetime.now().isoformat()

    def construct_article_url(self, article_idx: str) -> str:
        """ê¸°ì‚¬ URL ìƒì„±"""
        return f"https://news.jtbc.co.kr/article/{article_idx}"

    async def fetch_page(self, client: httpx.AsyncClient, page_no: int) -> Optional[Dict]:
        """API í˜ì´ì§€ ìš”ì²­"""
        try:
            params = {
                'pageNo': page_no,
                'sectionEngName': 'politics',
                'articleListType': 'ARTICLE',
                'pageSize': self.page_size
            }
            
            url = f"{self.base_url}{self.api_endpoint}"
            response = await client.get(url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            return response.json()
            
        except httpx.TimeoutException:
            logger.warning(f"í˜ì´ì§€ {page_no} ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            self.network_errors += 1
            return None
        except httpx.HTTPStatusError as e:
            logger.warning(f"í˜ì´ì§€ {page_no} HTTP ì˜¤ë¥˜: {e.response.status_code}")
            self.network_errors += 1
            return None
        except Exception as e:
            logger.warning(f"í˜ì´ì§€ {page_no} ìš”ì²­ ì‹¤íŒ¨: {e}")
            self.network_errors += 1
            return None

    def parse_articles(self, api_response: Dict, page_no: int) -> List[Dict]:
        """API ì‘ë‹µì—ì„œ ê¸°ì‚¬ ì •ë³´ íŒŒì‹±"""
        articles = []
        
        try:
            if api_response.get('resultCode') != '00':
                logger.warning(f"í˜ì´ì§€ {page_no} API ì˜¤ë¥˜: {api_response.get('resultMessage', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                return articles
            
            data = api_response.get('data', {})
            article_list = data.get('list', [])
            
            logger.info(f"  - í˜ì´ì§€ {page_no}: {len(article_list)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            for article in article_list:
                try:
                    # í•„ìˆ˜ í•„ë“œ í™•ì¸
                    article_idx = article.get('articleIdx')
                    title = article.get('articleTitle')
                    
                    if not article_idx or not title:
                        continue
                    
                    # ê¸°ì‚¬ URL ìƒì„±
                    article_url = self.construct_article_url(article_idx)
                    
                    # ì¤‘ë³µ ì²´í¬
                    if article_url in self.seen_urls:
                        continue
                    
                    # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                    article_data = {
                        'id': article_idx,
                        'title': title,
                        'summary': self.clean_content(article.get('articleInnerTextContent', '')),
                        'url': article_url,
                        'thumbnail': article.get('articleThumbnailImgUrl', ''),
                        'published_at': self.parse_date(article.get('publicationDate')),
                        'reporter': article.get('journalistName', ''),
                        'page_no': page_no
                    }
                    
                    articles.append(article_data)
                    self.seen_urls.add(article_url)
                    
                except Exception as e:
                    logger.warning(f"ê¸°ì‚¬ íŒŒì‹± ì‹¤íŒ¨ (í˜ì´ì§€ {page_no}): {e}")
                    self.parsing_errors += 1
                    continue
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {page_no} ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.parsing_errors += 1
        
        return articles

    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        start_time = datetime.now()
        
        try:
            # HTTP í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            limits = httpx.Limits(max_connections=self.max_concurrent_requests)
            timeout = httpx.Timeout(self.timeout)
            
            async with httpx.AsyncClient(limits=limits, timeout=timeout) as client:
                
                # 1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
                logger.info("ğŸ“„ 1ë‹¨ê³„: ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
                
                page_tasks = []
                for page_no in range(1, self.max_pages + 1):
                    task = self.fetch_page(client, page_no)
                    page_tasks.append(task)
                
                # ë³‘ë ¬ë¡œ í˜ì´ì§€ ìš”ì²­
                page_results = await asyncio.gather(*page_tasks, return_exceptions=True)
                
                # ê¸°ì‚¬ íŒŒì‹±
                all_articles = []
                for i, result in enumerate(page_results, 1):
                    if isinstance(result, Exception):
                        logger.warning(f"í˜ì´ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                        continue
                    
                    if result:
                        articles = self.parse_articles(result, i)
                        all_articles.extend(articles)
                
                logger.info(f"âœ… ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # ëª©í‘œ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
                target_articles = all_articles[:self.target_count]
                
                # ìˆ˜ì§‘ ì™„ë£Œ ì‹œê°„
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info(f"ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
                logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {len(target_articles)}ê°œ ê¸°ì‚¬")
                logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
                logger.info(f"ğŸš€ í‰ê·  ì†ë„: {len(target_articles) / duration:.1f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "ğŸš€ í‰ê·  ì†ë„: 0.0 ê¸°ì‚¬/ì´ˆ")
                
                return target_articles
                
        except Exception as e:
            logger.error(f"ìˆ˜ì§‘ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self.articles

    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0, "total": 0}
        
        success_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # Supabase í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article['summary'],  # ìš”ì•½ì„ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
                    'published_at': article['published_at'],
                    'media_id': self.media_id,
                    'bias': 'Center',  # JTBCëŠ” ì¤‘ë„
                    'issue_id': 1  # ê¸°ë³¸ê°’
                }
                
                # ê¸°ì‚¬ ì €ì¥
                result = self.supabase_manager.insert_article(article_data)
                
                if result:
                    success_count += 1
                    logger.info(f"ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                else:
                    failed_count += 1
                    logger.warning(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}...")
                
            except Exception as e:
                failed_count += 1
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return {"success": success_count, "failed": failed_count, "total": len(articles)}

    def save_to_json(self, articles: List[Dict], filename: str = "jtbc_articles.json"):
        """ê¸°ì‚¬ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {filename}ì— ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")

    def display_results(self, articles: List[Dict], save_results: Dict[str, int], duration: float):
        """ê²°ê³¼ ì¶œë ¥"""
        print("=" * 60)
        print("      JTBC ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ ì™„ë£Œ!      ")
        print("=" * 60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"  â€¢ í‰ê·  ì†ë„: {len(articles) / duration:.1f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "  â€¢ í‰ê·  ì†ë„: 0.0 ê¸°ì‚¬/ì´ˆ")
        print(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.network_errors}íšŒ")
        print(f"  â€¢ íŒŒì‹± ì˜¤ë¥˜: {self.parsing_errors}íšŒ")
        print(f"  â€¢ ëª©í‘œ ë‹¬ì„±: {'âœ…' if len(articles) >= self.target_count else 'âš ï¸'} {len(articles)}ê°œ (ëª©í‘œ: {self.target_count}ê°œ)")
        print()
        
        if save_results:
            print(f"ğŸ’¾ Supabase ì €ì¥ ê²°ê³¼:")
            print(f"  â€¢ ì„±ê³µ: {save_results['success']}ê°œ")
            print(f"  â€¢ ì‹¤íŒ¨: {save_results['failed']}ê°œ")
            print(f"  â€¢ ì´ ê¸°ì‚¬: {save_results['total']}ê°œ")
            print(f"âœ… {save_results['success']}ê°œ ê¸°ì‚¬ê°€ Supabaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            print()
        
        if articles:
            print(f"ğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© (ì²˜ìŒ 10ê°œ):")
            for i, article in enumerate(articles[:10], 1):
                print(f"  {i}. {article['title'][:50]}...")
            if len(articles) > 10:
                print(f"  ... ì™¸ {len(articles) - 10}ê°œ ê¸°ì‚¬")
            print()
        
        print("ğŸ‰ JTBC ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ ì™„ë£Œ!")
        if save_results and save_results['success'] > 0:
            print("ğŸ’¾ Supabaseì— ì €ì¥ ì™„ë£Œ!")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    collector = JTBCPoliticsCollector()
    
    start_time = datetime.now()
    
    # ê¸°ì‚¬ ìˆ˜ì§‘
    articles = await collector.collect_all_articles()
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    if articles:
        # Supabase ì €ì¥
        save_results = await collector.save_to_supabase(articles)
        
        # JSON íŒŒì¼ ì €ì¥
        collector.save_to_json(articles)
        
        # ê²°ê³¼ ì¶œë ¥
        collector.display_results(articles, save_results, duration)
    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))
