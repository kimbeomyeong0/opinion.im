#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„¸ê³„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
ì„¸ê³„ì¼ë³´ì˜ ì •ì¹˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•˜ì—¬ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
import logging
from datetime import datetime
from bs4 import BeautifulSoup
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin, urlparse
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel
from rich.text import Text
from supabase_manager_v2 import SupabaseManagerV2
from playwright.async_api import async_playwright

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SegyePoliticsCrawler:
    """ì„¸ê³„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        self.base_url = "https://www.segye.com"
        self.politics_url = "https://www.segye.com/news/politics"
        self.console = Console()
        self.supabase_manager = SupabaseManagerV2()
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        # í†µê³„
        self.total_articles = 0
        self.successful_articles = 0
        self.failed_articles = 0
        self.start_time = None
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(headers=self.headers)
        return self
        
    async def __aexit__(self, self_exc_type, self_exc_val, self_exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    def _is_valid_article_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if url.startswith('/'):
            url = urljoin(self.base_url, url)
        
        parsed = urlparse(url)
        return (
            '/newsView/' in url and
            len(url.split('/newsView/')[-1]) > 5  # ê¸°ì‚¬ IDê°€ ìˆëŠ”ì§€ í™•ì¸
        )
    
    async def get_politics_article_links(self, target_count: int = 100) -> List[str]:
        """ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ - 100ê°œ ëª©í‘œ"""
        article_links = []
        
        with Progress() as progress:
            task = progress.add_task("ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...", total=target_count)
            
            # 1. ì²« í˜ì´ì§€ì—ì„œ ê³ ì •ëœ Topë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘
            try:
                logger.info("ì²« í˜ì´ì§€ì—ì„œ ê³ ì •ëœ Topë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
                
                async with self.session.get(self.politics_url) as response:
                    if response.status != 200:
                        logger.error(f"ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                        return article_links
                    
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ê³ ì •ëœ Topë‰´ìŠ¤ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
                    top_news_section = soup.select_one('article.newsSubjTop')
                    if top_news_section:
                        top_news_links = top_news_section.select('ul li a')
                        for link in top_news_links:
                            if len(article_links) >= target_count:
                                break
                                
                            href = link.get('href')
                            if href and self._is_valid_article_url(href):
                                full_url = urljoin(self.base_url, href)
                                if full_url not in article_links:
                                    article_links.append(full_url)
                                    progress.update(task, completed=len(article_links))
                    
                    logger.info(f"ê³ ì •ëœ Topë‰´ìŠ¤ì—ì„œ {len(article_links)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                    
            except Exception as e:
                logger.error(f"Topë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # 2. ë” ë§ì€ í˜ì´ì§€ íƒìƒ‰ (JavaScript ë™ì  ë¡œë”© ê³ ë ¤)
            for page in range(1, 21):  # 1~20í˜ì´ì§€ê¹Œì§€ ì‹œë„
                if len(article_links) >= target_count:
                    break
                    
                try:
                    url = f"{self.politics_url}?page={page}"
                    logger.info(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
                    
                    async with self.session.get(url) as response:
                        if response.status != 200:
                            logger.warning(f"í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                            continue
                        
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        page_article_count = 0
                        
                        # ì „ì²´ í˜ì´ì§€ì—ì„œ newsView ë§í¬ ì°¾ê¸° (newsSubjTop ì œì™¸)
                        all_news_links = soup.select('a[href*="/newsView/"]')
                        
                        for link in all_news_links:
                            if len(article_links) >= target_count:
                                break
                            
                            # newsSubjTop ì„¹ì…˜ ë‚´ì˜ ë§í¬ëŠ” ì œì™¸ (ì¤‘ë³µ ë°©ì§€)
                            if link.find_parent('article', class_='newsSubjTop'):
                                continue
                            
                            href = link.get('href')
                            if href and self._is_valid_article_url(href):
                                full_url = urljoin(self.base_url, href)
                                if full_url not in article_links:
                                    article_links.append(full_url)
                                    page_article_count += 1
                                    progress.update(task, completed=len(article_links))
                        
                        logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {page_article_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                        
                        # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
                        if page_article_count == 0:
                            logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            break
                            
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # 3. ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘ì„ ìœ„í•´ ë‹¤ì–‘í•œ URL íŒ¨í„´ ì‹œë„
            if len(article_links) < target_count:
                additional_urls = [
                    "https://www.segye.com/newsList/0101010000000",
                    "https://www.segye.com/newsList/0101010100000", 
                    "https://www.segye.com/newsList/0101010200000",
                    "https://www.segye.com/newsList/0101010300000",
                    "https://www.segye.com/newsList/0101010400000",
                    "https://www.segye.com/newsList/0101010500000",
                    "https://www.segye.com/newsList/0101010600000",
                    "https://www.segye.com/newsList/0101010700000",
                    "https://www.segye.com/newsList/0101010800000",
                    "https://www.segye.com/newsList/0101010900000"
                ]
                
                for url in additional_urls:
                    if len(article_links) >= target_count:
                        break
                        
                    try:
                        logger.info(f"ì¶”ê°€ URL ì²˜ë¦¬ ì¤‘: {url}")
                        
                        async with self.session.get(url) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'html.parser')
                                
                                # newsView ë§í¬ ìˆ˜ì§‘
                                news_links = soup.select('a[href*="/newsView/"]')
                                additional_count = 0
                                
                                for link in news_links:
                                    if len(article_links) >= target_count:
                                        break
                                    
                                    href = link.get('href')
                                    if href and self._is_valid_article_url(href):
                                        full_url = urljoin(self.base_url, href)
                                        if full_url not in article_links:
                                            article_links.append(full_url)
                                            additional_count += 1
                                            progress.update(task, completed=len(article_links))
                                
                                logger.info(f"ì¶”ê°€ URLì—ì„œ {additional_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                                
                    except Exception as e:
                        logger.error(f"ì¶”ê°€ URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        continue
            
            # 4. ë§ˆì§€ë§‰ ì‹œë„: ë‹¤ë¥¸ ì •ì¹˜ ê´€ë ¨ URLë“¤
            if len(article_links) < target_count:
                final_urls = [
                    "https://www.segye.com/news/politics/list",
                    "https://www.segye.com/news/politics/breaking",
                    "https://www.segye.com/news/politics/analysis",
                    "https://www.segye.com/news/politics/column"
                ]
                
                for url in final_urls:
                    if len(article_links) >= target_count:
                        break
                        
                    try:
                        logger.info(f"ìµœì¢… URL ì²˜ë¦¬ ì¤‘: {url}")
                        
                        async with self.session.get(url) as response:
                            if response.status == 200:
                                html = await response.text()
                                soup = BeautifulSoup(html, 'html.parser')
                                
                                # newsView ë§í¬ ìˆ˜ì§‘
                                news_links = soup.select('a[href*="/newsView/"]')
                                final_count = 0
                                
                                for link in news_links:
                                    if len(article_links) >= target_count:
                                        break
                                    
                                    href = link.get('href')
                                    if href and self._is_valid_article_url(href):
                                        full_url = urljoin(self.base_url, href)
                                        if full_url not in article_links:
                                            article_links.append(full_url)
                                            final_count += 1
                                            progress.update(task, completed=len(article_links))
                                
                                logger.info(f"ìµœì¢… URLì—ì„œ {final_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                                
                    except Exception as e:
                        logger.error(f"ìµœì¢… URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        continue
        
        logger.info(f"ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return article_links
    
    async def get_politics_article_links_with_playwright(self, target_count: int = 100) -> List[str]:
        """Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ JavaScript ë™ì  ë¡œë”© ì²˜ë¦¬ - 100ê°œ ëª©í‘œ"""
        article_links = []
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                with Progress() as progress:
                    task = progress.add_task("Playwrightë¡œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...", total=target_count)
                    
                    # 1. ì²« í˜ì´ì§€ì—ì„œ ê³ ì •ëœ Topë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘
                    logger.info("ì²« í˜ì´ì§€ì—ì„œ ê³ ì •ëœ Topë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
                    
                    await page.goto(self.politics_url, wait_until='domcontentloaded', timeout=60000)
                    
                    # ê³ ì •ëœ Topë‰´ìŠ¤ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
                    top_news_links = await page.query_selector_all('article.newsSubjTop ul li a')
                    
                    for link in top_news_links:
                        if len(article_links) >= target_count:
                            break
                            
                        href = await link.get_attribute('href')
                        if href and self._is_valid_article_url(href):
                            full_url = urljoin(self.base_url, href)
                            if full_url not in article_links:
                                article_links.append(full_url)
                                progress.update(task, completed=len(article_links))
                    
                    logger.info(f"ê³ ì •ëœ Topë‰´ìŠ¤ì—ì„œ {len(article_links)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                    
                    # 2. í˜ì´ì§€ë³„ë¡œ ë” ë§ì€ ê¸°ì‚¬ ìˆ˜ì§‘ (JavaScript ë™ì  ë¡œë”© ëŒ€ê¸°)
                    for page_num in range(1, 21):  # 1~20í˜ì´ì§€ê¹Œì§€ ì‹œë„
                        if len(article_links) >= target_count:
                            break
                            
                        try:
                            url = f"{self.politics_url}?page={page_num}"
                            logger.info(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘: {url}")
                            
                            await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                            
                            # JavaScript ë™ì  ë¡œë”© ëŒ€ê¸° (ê¸°ì‚¬ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€)
                            await page.wait_for_timeout(3000)  # 3ì´ˆ ëŒ€ê¸°
                            
                            # ì „ì²´ í˜ì´ì§€ì—ì„œ newsView ë§í¬ ì°¾ê¸° (newsSubjTop ì œì™¸)
                            all_news_links = await page.query_selector_all('a[href*="/newsView/"]')
                            
                            page_article_count = 0
                            for link in all_news_links:
                                if len(article_links) >= target_count:
                                    break
                                
                                # newsSubjTop ì„¹ì…˜ ë‚´ì˜ ë§í¬ëŠ” ì œì™¸ (ì¤‘ë³µ ë°©ì§€)
                                parent_article = await link.query_selector('xpath=ancestor::article[@class="newsSubjTop"]')
                                if parent_article:
                                    continue
                                
                                href = await link.get_attribute('href')
                                if href and self._is_valid_article_url(href):
                                    full_url = urljoin(self.base_url, href)
                                    if full_url not in article_links:
                                        article_links.append(full_url)
                                        page_article_count += 1
                                        progress.update(task, completed=len(article_links))
                            
                            logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ {page_article_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                            
                            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
                            if page_article_count == 0:
                                logger.info(f"í˜ì´ì§€ {page_num}ì—ì„œ ë” ì´ìƒ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                                break
                                
                        except Exception as e:
                            logger.error(f"í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                            continue
                    
                    # 3. ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘ì„ ìœ„í•´ ë‹¤ë¥¸ URLë“¤ë„ ì‹œë„
                    if len(article_links) < target_count:
                        additional_urls = [
                            "https://www.segye.com/newsList/0101010000000",
                            "https://www.segye.com/newsList/0101010100000", 
                            "https://www.segye.com/newsList/0101010200000",
                            "https://www.segye.com/newsList/0101010300000",
                            "https://www.segye.com/newsList/0101010400000",
                            "https://www.segye.com/newsList/0101010500000"
                        ]
                        
                        for url in additional_urls:
                            if len(article_links) >= target_count:
                                break
                                
                            try:
                                logger.info(f"ì¶”ê°€ URL ì²˜ë¦¬ ì¤‘: {url}")
                                
                                await page.goto(url, wait_until='domcontentloaded', timeout=60000)
                                await page.wait_for_timeout(2000)  # 2ì´ˆ ëŒ€ê¸°
                                
                                # newsView ë§í¬ ìˆ˜ì§‘
                                news_links = await page.query_selector_all('a[href*="/newsView/"]')
                                additional_count = 0
                                
                                for link in news_links:
                                    if len(article_links) >= target_count:
                                        break
                                    
                                    href = await link.get_attribute('href')
                                    if href and self._is_valid_article_url(href):
                                        full_url = urljoin(self.base_url, href)
                                        if full_url not in article_links:
                                            article_links.append(full_url)
                                            additional_count += 1
                                            progress.update(task, completed=len(article_links))
                                
                                logger.info(f"ì¶”ê°€ URLì—ì„œ {additional_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                                
                            except Exception as e:
                                logger.error(f"ì¶”ê°€ URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                continue
                    
            finally:
                await browser.close()
        
        logger.info(f"Playwrightë¡œ ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return article_links
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„: h3#title_sns > title > og:title
            title_selectors = [
                'h3#title_sns',
                'title',
                'meta[property="og:title"]'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    if selector == 'meta[property="og:title"]':
                        title = title_elem.get('content', '')
                    else:
                        title = title_elem.get_text(strip=True)
                    
                    if title and len(title) > 5:
                        return title
            
            return None
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„: #article_txt > article.viewBox2 > og:description
            content_selectors = [
                '#article_txt',
                'article.viewBox2',
                'meta[property="og:description"]',
                'meta[name="description"]'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    if selector.startswith('meta'):
                        content = content_elem.get('content', '')
                    else:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (ê´‘ê³ , ì´ë¯¸ì§€, ê¸°ìì •ë³´ ë“±)
                        for unwanted in content_elem.select('.image, figure, img, figcaption, .viewInfo, .viewIssue, .precis'):
                            unwanted.decompose()
                        
                        content = content_elem.get_text(separator='\n', strip=True)
                        
                        # ì¤„ë°”ê¿ˆ ì •ë¦¬ ë° ë¹ˆ ì¤„ ì œê±°
                        lines = content.split('\n')
                        clean_lines = []
                        for line in lines:
                            line = line.strip()
                            # ê¸°ì ì„œëª…, ì €ì‘ê¶Œ, ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                            if any(pattern in line for pattern in ['ê¸°ì', 'jm100@segye.com', 'ì„¸ê³„ì¼ë³´', 'ë¬´ë‹¨ì „ì¬', 'ì¬ë°°í¬ ê¸ˆì§€', 'â“’']):
                                continue
                            # ë¹ˆ ì¤„ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±°
                            if line and len(line) > 5:
                                clean_lines.append(line)
                        
                        content = '\n'.join(clean_lines)
                    
                    if content and len(content) > 50:
                        return content
            
            return None
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_published_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ê¸°ì‚¬ ì‘ì„± ì‹œê°„ ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„: p.viewInfo > og:article:published_time > meta article:published_time
            time_selectors = [
                'p.viewInfo',
                'meta[property="og:article:published_time"]',
                'meta[property="article:published_time"]'
            ]
            
            for selector in time_selectors:
                time_elem = soup.select_one(selector)
                if time_elem:
                    if selector == 'p.viewInfo':
                        # "ì…ë ¥ : 2025-08-20 17:58:14" í˜•ì‹ì—ì„œ ì¶”ì¶œ
                        text = time_elem.get_text()
                        if 'ì…ë ¥ :' in text:
                            time_str = text.split('ì…ë ¥ :')[1].split()[0] + ' ' + text.split('ì…ë ¥ :')[1].split()[1]
                            try:
                                return datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                            except ValueError:
                                continue
                    else:
                        # ISO 8601 í˜•ì‹
                        time_str = time_elem.get('content', '')
                        if time_str:
                            try:
                                return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                            except ValueError:
                                continue
            
            return None
        except Exception as e:
            logger.error(f"ì‘ì„±ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def _fetch_article_details(self, url: str) -> Optional[Dict[str, Any]]:
        """ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title = self._extract_title(soup)
                if not title:
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = self._extract_content(soup)
                if not content:
                    return None
                
                # ì‘ì„±ì‹œê°„ ì¶”ì¶œ
                published_at = self._extract_published_time(soup)
                if not published_at:
                    published_at = datetime.now()
                
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': published_at
                }
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {str(e)}")
            return None
    
    async def crawl_articles(self):
        """ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        self.start_time = datetime.now()
        
        try:
            # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            self.console.print("\nğŸ“‹ 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘")
            
            # Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ JavaScript ë™ì  ë¡œë”© ì²˜ë¦¬
            article_links = await self.get_politics_article_links_with_playwright(target_count=100)
            
            if not article_links:
                self.console.print("âŒ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            self.console.print(f"âœ“ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            
            # 2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            self.console.print("\nğŸ“° 2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘")
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                for i, article_url in enumerate(article_links):
                    try:
                        article_data = await self._fetch_article_details(article_url)
                        if article_data:
                            # í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œëŠ” issue_idë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ (í´ëŸ¬ìŠ¤í„°ë§ í›„ ì„¤ì •)
                            # ì„ì‹œ ì´ìŠˆ ID 6 ì‚¬ìš© (ë°ì´í„°ë² ì´ìŠ¤ ì œì•½ì¡°ê±´ ì¤€ìˆ˜)
                            issue = {'id': 6}
                            
                            # ì–¸ë¡ ì‚¬ ì¡°íšŒ
                            media_outlet = self.supabase_manager.get_media_outlet("ì„¸ê³„ì¼ë³´")
                            if not media_outlet:
                                media_outlet = self.supabase_manager.create_media_outlet("ì„¸ê³„ì¼ë³´", "center")
                            
                            # ê¸°ì‚¬ ì €ì¥
                            article_insert_data = {
                                'title': article_data['title'],
                                'url': article_data['url'],
                                'content': article_data['content'],
                                'published_at': article_data['published_at'],
                                'issue_id': issue['id'] if isinstance(issue, dict) else issue,
                                'media_id': media_outlet['id'] if isinstance(media_outlet, dict) else media_outlet,
                                'bias': media_outlet.get('bias', 'center') if isinstance(media_outlet, dict) else 'center'
                            }
                            
                            self.supabase_manager.insert_article(article_insert_data)
                            logger.info(f"ìƒˆ ê¸°ì‚¬ ì‚½ì…: {article_data['title']}")
                            
                            self.successful_articles += 1
                        else:
                            self.failed_articles += 1
                            
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({article_url}): {str(e)}")
                        self.failed_articles += 1
                    
                    progress.advance(task)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if (i + 1) % 10 == 0:
                        progress.update(task, description=f"ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i + 1}/{len(article_links)})")
            
            # 3ë‹¨ê³„: ê²°ê³¼ í‘œì‹œ
            self._display_results()
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        finally:
            if self.session:
                await self.session.close()
    
    def _display_results(self) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        if not self.start_time:
            return
        
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="ğŸŒ ì„¸ê³„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ê²°ê³¼")
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê²°ê³¼", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(self.total_articles))
        table.add_row("ì„±ê³µ", f"{self.successful_articles}ê°œ")
        table.add_row("ì‹¤íŒ¨", f"{self.failed_articles}ê°œ")
        table.add_row("ì„±ê³µë¥ ", f"{(self.successful_articles/self.total_articles*100):.1f}%" if self.total_articles > 0 else "0%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration:.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{self.successful_articles/duration:.2f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "0 ê¸°ì‚¬/ì´ˆ")
        
        self.console.print(table)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ìš”ì•½
        if self.successful_articles > 0:
            self.console.print(f"\n[green]âœ… ì„±ê³µì ìœ¼ë¡œ {self.successful_articles}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤![/green]")
        
        if self.failed_articles > 0:
            self.console.print(f"\n[yellow]âš ï¸ {self.failed_articles}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/yellow]")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with SegyePoliticsCrawler() as crawler:
        await crawler.crawl_articles()

if __name__ == "__main__":
    asyncio.run(main())
