#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
êµ­ë¯¼ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- ëª©í‘œ: 100ê°œ ì •ì¹˜ ê¸°ì‚¬ ìˆ˜ì§‘
- ì–¸ë¡ ì‚¬: êµ­ë¯¼ì¼ë³´ (Center)
- í¬ë¡¤ë§ ì‹œê°„: 20ì´ˆ ë‚´
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Optional, Dict, Any
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich.panel import Panel
from utils.supabase_manager_unified import UnifiedSupabaseManager
from urllib.parse import urljoin

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rich ì½˜ì†” ì„¤ì •
console = Console()

class KMIBPoliticsCrawler:
    def __init__(self):
        self.base_url = "https://www.kmib.co.kr"
        self.politics_url = "https://www.kmib.co.kr/article/listing.asp?sid1=pol"
        self.media_name = "êµ­ë¯¼ì¼ë³´"
        self.media_bias = "Center"
        self.supabase_manager = UnifiedSupabaseManager()
        self.session = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=10),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _is_valid_article_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
        return (
            'kmib.co.kr' in url and
            '/article/view.asp' in url and
            'arcid=' in url and
            not url.endswith('.jpg') and
            not url.endswith('.png')
        )
    
    async def get_politics_article_links(self, target_count: int = 100) -> List[str]:
        """ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ - 100ê°œ ëª©í‘œ"""
        article_links = []
        
        with Progress() as progress:
            task = progress.add_task("ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...", total=target_count)
            
            # 1. ì²« í˜ì´ì§€ì—ì„œ ìµœìƒë‹¨ ê³ ì • ê¸°ì‚¬ 5ê°œ ìˆ˜ì§‘
            try:
                logger.info("ì²« í˜ì´ì§€ì—ì„œ ìµœìƒë‹¨ ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
                
                async with self.session.get(self.politics_url) as response:
                    if response.status != 200:
                        logger.error(f"ì²« í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                        return article_links
                    
                    html = await response.read()
                    try:
                        html = html.decode('euc-kr')
                    except UnicodeDecodeError:
                        try:
                            html = html.decode('cp949')
                        except UnicodeDecodeError:
                            html = html.decode('utf-8', errors='ignore')
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ìµœìƒë‹¨ ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘ (section.list_main_top)
                    top_section = soup.select_one('section.list_main_top')
                    if top_section:
                        # ì™¼ìª½ ë©”ì¸ ê¸°ì‚¬ 1ê°œ
                        main_article = top_section.select_one('.col_lg8 .card a')
                        if main_article:
                            href = main_article.get('href')
                            if href and self._is_valid_article_url(href):
                                full_url = urljoin(self.base_url, href)
                                article_links.append(full_url)
                                progress.update(task, completed=len(article_links))
                        
                        # ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œ ê¸°ì‚¬ 4ê°œ
                        side_articles = top_section.select('.col_lg4 .card a')
                        for article in side_articles:
                            if len(article_links) >= 5:  # ìµœìƒë‹¨ ê³ ì • ê¸°ì‚¬ëŠ” 5ê°œë§Œ
                                break
                            href = article.get('href')
                            if href and self._is_valid_article_url(href):
                                full_url = urljoin(self.base_url, href)
                                if full_url not in article_links:
                                    article_links.append(full_url)
                                    progress.update(task, completed=len(article_links))
                    
                    logger.info(f"ìµœìƒë‹¨ ê³ ì • ê¸°ì‚¬ {len(article_links)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                    
            except Exception as e:
                logger.error(f"ìµœìƒë‹¨ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
            # 2. í˜ì´ì§€ë³„ ë³€ê²½ ê¸°ì‚¬ ìˆ˜ì§‘ (í˜ì´ì§€ 1ë¶€í„° ì‹œì‘)
            page = 1
            while len(article_links) < target_count:
                try:
                    # ì˜¬ë°”ë¥¸ í˜ì´ì§€ë„¤ì´ì…˜ URL íŒ¨í„´ ì‚¬ìš©
                    if page == 1:
                        url = f"{self.politics_url}&page={page}"
                    else:
                        url = f"{self.politics_url}&sid2=&page={page}"
                    
                    logger.info(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘: {url}")
                    
                    async with self.session.get(url) as response:
                        if response.status != 200:
                            logger.warning(f"í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {response.status}")
                            break
                        
                        html = await response.read()
                        try:
                            html = html.decode('euc-kr')
                        except UnicodeDecodeError:
                            try:
                                html = html.decode('cp949')
                            except UnicodeDecodeError:
                                html = html.decode('utf-8', errors='ignore')
                        
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        page_article_count = 0
                        
                        # í˜ì´ì§€ë³„ ë³€ê²½ ê¸°ì‚¬ ìˆ˜ì§‘ - ëª¨ë“  ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                        all_links = soup.select('a[href*="article/view.asp"]')
                        for link in all_links:
                            if len(article_links) >= target_count:
                                break
                            
                            href = link.get('href')
                            if href and self._is_valid_article_url(href):
                                full_url = urljoin(self.base_url, href)
                                if full_url not in article_links:
                                    article_links.append(full_url)
                                    page_article_count += 1
                                    progress.update(task, completed=len(article_links))
                        
                        logger.info(f"í˜ì´ì§€ {page}ì—ì„œ {page_article_count}ê°œ ê¸°ì‚¬ ë°œê²¬")
                        
                        # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ
                        if page_article_count == 0:
                            logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            break
                        
                        page += 1
                        
                        # ë„ˆë¬´ ë§ì€ í˜ì´ì§€ë¥¼ íƒìƒ‰í•˜ì§€ ì•Šë„ë¡ ì œí•œ (100ê°œ ëª©í‘œë¥¼ ìœ„í•´ 15í˜ì´ì§€ê¹Œì§€)
                        if page > 15:
                            logger.warning("í˜ì´ì§€ ì œí•œ(15)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                            break
                            
                except Exception as e:
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        
        logger.info(f"ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        return article_links
    
    async def crawl_article(self, url: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                # EUC-KR ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸°
                html = await response.read()
                try:
                    html = html.decode('euc-kr')
                except UnicodeDecodeError:
                    try:
                        html = html.decode('cp949')
                    except UnicodeDecodeError:
                        html = html.decode('utf-8', errors='ignore')
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title = self._extract_title(soup)
                if not title:
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = self._extract_content(soup)
                if not content:
                    return None
                
                # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                published_at = self._extract_published_time(soup)
                
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': published_at
                }
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„: og:title > meta title > title íƒœê·¸
            title_selectors = [
                'meta[property="og:title"]',
                'meta[name="title"]',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    if selector.startswith('meta'):
                        title = title_elem.get('content', '')
                    else:
                        title = title_elem.get_text(strip=True)
                    
                    if title and len(title) > 5:
                        # " - êµ­ë¯¼ì¼ë³´" ì œê±°
                        title = title.replace(' - êµ­ë¯¼ì¼ë³´', '').replace(' - åœ‹æ°‘æ—¥å ±', '')
                        return title
            
            return None
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ìš°ì„ ìˆœìœ„: .article_content > .article_body > ë©”íƒ€ íƒœê·¸
            content_selectors = [
                '.article_content',
                '.article_body',
                'meta[property="og:description"]',
                'meta[name="description"]'
            ]
            
            for selector in content_selectors:
                if selector.startswith('meta'):
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        content = content_elem.get('content', '')
                        if content and len(content) > 50:
                            return content
                else:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±° (ê´‘ê³ , ì¶”ì²œê¸°ì‚¬, ê¸°ìì •ë³´, ì´ë¯¸ì§€ ë“±)
                        for unwanted in content_elem.select('.advertisement, .ad, .banner, .popup, .share, .layer_popup, .article_recommend, .view_reporter, .view_m_adK, .view_ad06, .view_m_adA, .view_m_adB, .a1, .article_body_img, figure, img, figcaption, .card, .card_body, .card_img, .primary, .tit'):
                            unwanted.decompose()
                        
                        # ëª¨ë“  script íƒœê·¸ ì œê±°
                        for script in content_elem.find_all('script'):
                            script.decompose()
                        
                        # ëª¨ë“  style íƒœê·¸ ì œê±°
                        for style in content_elem.find_all('style'):
                            style.decompose()
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬ (HTML íƒœê·¸ ì™„ì „ ì œê±°)
                        content = content_elem.get_text(separator='\n', strip=True)
                        
                        # ì¤„ë°”ê¿ˆ ì •ë¦¬ ë° ë¹ˆ ì¤„ ì œê±°
                        lines = content.split('\n')
                        clean_lines = []
                        for line in lines:
                            line = line.strip()
                            # ê¸°ì ì„œëª… íŒ¨í„´ ì œê±°
                            if any(pattern in line for pattern in ['ê¸°ì,', 'ê¸°ì', 'pan@kmib.co.kr', 'GoodNews paper', 'ë¬´ë‹¨ì „ì¬', 'AIí•™ìŠµ ì´ìš© ê¸ˆì§€', 'êµ­ë¯¼ì¼ë³´(www.kmib.co.kr)', 'kmib.co.kr', 'êµ­ë¯¼ì¼ë³´', '== $0', 'ì¶”ì²œê¸°ì‚¬', 'ê¸°ì‚¬ëŠ” ì–´ë– ì…¨ë‚˜ìš”', 'í›„ì†ê¸°ì‚¬ ì›í•´ìš”', 'ë§ì´ ë³¸ ê¸°ì‚¬', 'í•´ë‹¹ë¶„ì•¼ë³„ ê¸°ì‚¬ ë”ë³´ê¸°']):
                                continue
                            # ë¹ˆ ì¤„ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ì¤„ ì œê±°
                            if line and len(line) > 5:
                                clean_lines.append(line)
                        
                        content = '\n'.join(clean_lines)
                        
                        # ë§ˆì§€ë§‰ ì¤„ì— ë‚¨ì€ ë¶ˆí•„ìš”í•œ ì •ë³´ ì œê±°
                        if content:
                            lines = content.split('\n')
                            while lines and any(pattern in lines[-1] for pattern in ['êµ­ë¯¼ì¼ë³´', 'kmib.co.kr', 'www.kmib.co.kr', '== $0']):
                                lines.pop()
                            content = '\n'.join(lines)
                        
                        if content and len(content) > 100:  # ë©”íƒ€ íƒœê·¸ë³´ë‹¤ ê¸´ ë³¸ë¬¸ ìš”êµ¬
                            return content
            
            return None
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_published_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ë°œí–‰ì‹œê°„ ì¶”ì¶œ"""
        try:
            # article:published_time ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
            time_elem = soup.find('meta', property='article:published_time')
            if time_elem:
                time_str = time_elem.get('content', '')
                if time_str:
                    # ISO 8601 í˜•ì‹ íŒŒì‹± (ì˜ˆ: 2025-08-21T00:04:00+09:00)
                    try:
                        # +09:00 ì œê±° í›„ íŒŒì‹±
                        if '+' in time_str:
                            time_str = time_str.split('+')[0]
                        return datetime.fromisoformat(time_str)
                    except ValueError:
                        pass
            
            return None
        except Exception as e:
            logger.error(f"ë°œí–‰ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def start_crawling(self, target_count: int = 100):
        """í¬ë¡¤ë§ ì‹œì‘"""
        start_time = datetime.now()
        
        # ì‹œì‘ ë©”ì‹œì§€
        console.print(Panel(
            f"[bold blue]ğŸš€ í¬ë¡¤ëŸ¬ ì‹œì‘[/bold blue]\n"
            f"êµ­ë¯¼ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬\n"
            f"ëª©í‘œ: {target_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘\n"
            f"ì–¸ë¡ ì‚¬: {self.media_name} ({self.media_bias})",
            title="êµ­ë¯¼ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬",
            border_style="blue"
        ))
        
        # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        console.print("\nğŸ” 1ë‹¨ê³„: êµ­ë¯¼ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        article_links = await self.get_politics_article_links(target_count)
        
        if not article_links:
            console.print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # 2ë‹¨ê³„: ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§
        console.print(f"\nğŸ“° 2ë‹¨ê³„: {len(article_links)}ê°œ ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì¤‘...")
        
        articles = []
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            task = progress.add_task("êµ­ë¯¼ì¼ë³´ ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì¤‘...", total=len(article_links))
            
            for link in article_links:
                article = await self.crawl_article(link)
                if article:
                    articles.append(article)
                progress.advance(task)
        
        # ê²°ê³¼ í†µê³„
        elapsed_time = (datetime.now() - start_time).total_seconds()
        success_rate = len(articles) / len(article_links) * 100
        
        console.print("\n" + "="*80)
        console.print("ğŸ¯ êµ­ë¯¼ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        console.print("="*80)
        
        # í†µê³„ í…Œì´ë¸”
        stats_table = Table(title="ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        stats_table.add_column("í•­ëª©", style="cyan")
        stats_table.add_column("ê°’", style="magenta")
        
        stats_table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(article_links)))
        stats_table.add_row("ì„±ê³µí•œ ê¸°ì‚¬ ìˆ˜", str(len(articles)))
        stats_table.add_row("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        stats_table.add_row("í¬ë¡¤ë§ ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
        stats_table.add_row("í‰ê·  ì†ë„", f"{len(articles)/elapsed_time:.2f} ê¸°ì‚¬/ì´ˆ")
        
        console.print(stats_table)
        
        # ìƒ˜í”Œ ê¸°ì‚¬ í…Œì´ë¸”
        if articles:
            sample_table = Table(title="ğŸ“° ìƒ˜í”Œ ê¸°ì‚¬ (ì²˜ìŒ 10ê°œ)")
            sample_table.add_column("ë²ˆí˜¸", style="cyan")
            sample_table.add_column("ì œëª©", style="green")
            sample_table.add_column("URL", style="blue")
            sample_table.add_column("ë°œí–‰ì‹œê°„", style="yellow")
            
            for i, article in enumerate(articles[:10], 1):
                title = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
                url = article['url'][:50] + "..." if len(article['url']) > 50 else article['url']
                published_at = article['published_at'].strftime("%Y-%m-%d %H:%M") if article['published_at'] else "N/A"
                
                sample_table.add_row(str(i), title, url, published_at)
            
            console.print(sample_table)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if articles:
            console.print(f"\nğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
            await self.save_articles_to_db(articles)
        
        console.print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬")
    
    async def save_articles_to_db(self, articles: List[Dict[str, Any]]):
        """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        saved_count = 0
        
        for article in articles:
            try:
                # í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œëŠ” issue_idë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ (í´ëŸ¬ìŠ¤í„°ë§ í›„ ì„¤ì •)
                # ì„ì‹œ ì´ìŠˆ ID 6 ì‚¬ìš© (ë°ì´í„°ë² ì´ìŠ¤ ì œì•½ì¡°ê±´ ì¤€ìˆ˜)
                issue = {'id': 6}
                
                # ì–¸ë¡ ì‚¬ ìƒì„± ë˜ëŠ” ì¡°íšŒ
                media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
                if not media_outlet:
                    media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
                    media_outlet = {'id': media_id, 'bias': self.media_bias}
                
                # ì¤‘ë³µ ê¸°ì‚¬ í™•ì¸ (get_article_by_url ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì œê±°)
                # existing_article = self.supabase_manager.get_article_by_url(article['url'])
                # if existing_article:
                #     continue
                
                # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
                article_data = {
                    'issue_id': issue['id'],
                    'media_id': media_outlet['id'],
                    'title': article['title'],
                    'url': article['url'],
                    'content': article['content'],
                    'bias': media_outlet['bias'],
                    'published_at': article['published_at']
                }
                
                # ê¸°ì‚¬ ì €ì¥
                self.supabase_manager.insert_article(article_data)
                saved_count += 1
                
                console.print(f"ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article['title'][:50]}...")
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                continue
        
        console.print(f"âœ… {saved_count}ê°œ ê¸°ì‚¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_article()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with KMIBPoliticsCrawler() as crawler:
        await crawler.start_crawling(100)

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))
