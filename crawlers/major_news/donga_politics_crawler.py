import asyncio
import aiohttp
import time
import sys
import os
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.layout import Layout
from rich.columns import Columns
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager_unified import UnifiedSupabaseManager
from utils.common.html_parser import HTMLParserUtils

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DongaPoliticsCrawler:
    def __init__(self, max_articles: int = 100):
        self.base_url = "https://www.donga.com"
        self.politics_url = "https://www.donga.com/news/Politics"
        self.max_articles = max_articles
        self.console = Console()
        self.supabase_manager = UnifiedSupabaseManager()
        
        # ë™ì•„ì¼ë³´ëŠ” ìš°íŒŒ ì–¸ë¡ ì‚¬
        self.media_name = "ë™ì•„ì¼ë³´"
        self.media_bias = "Right"
        
    async def get_politics_article_links(self) -> List[str]:
        """ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹)"""
        all_links = []
        
        # í˜ì´ì§€ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
        page = 1
        page_offset = 0
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            task = progress.add_task("ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...", total=None)
            
            while len(all_links) < self.max_articles:
                if page == 1:
                    url = self.politics_url
                else:
                    # ë™ì•„ì¼ë³´ í˜ì´ì§€ë„¤ì´ì…˜: p=11, p=21, p=31...
                    page_offset = (page - 1) * 10
                    url = f"{self.politics_url}?p={page_offset + 1}&prod=news&ymd=&m="
                
                try:
                    page_links = await self._get_links_from_page(url)
                    if not page_links:
                        self.console.print(f"[yellow]í˜ì´ì§€ {page}ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¤‘ë‹¨í•©ë‹ˆë‹¤.[/yellow]")
                        break
                    
                    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
                    new_links = [link for link in page_links if link not in all_links]
                    all_links.extend(new_links)
                    
                    self.console.print(f"[cyan]í˜ì´ì§€ {page}: {len(page_links)}ê°œ ê¸°ì‚¬ ë°œê²¬ (ì´ {len(all_links)}ê°œ)[/cyan]")
                    
                    if len(all_links) >= self.max_articles:
                        all_links = all_links[:self.max_articles]
                        break
                    
                    page += 1
                    await asyncio.sleep(0.5)  # í˜ì´ì§€ ê°„ ë”œë ˆì´
                    
                except Exception as e:
                    self.console.print(f"[red]í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}[/red]")
                    logger.error(f"í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    break
        
        self.console.print(f"[green]ì´ {len(all_links)}ê°œì˜ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤![/green]")
        return all_links
    
    async def _get_links_from_page(self, url: str) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ê¸°ì‚¬ ì¹´ë“œ ì°¾ê¸° (ëª¨ë“  ì„¹ì…˜)
                        articles = soup.select('.news_card')
                        
                        for article in articles:
                            try:
                                # ì œëª© ë§í¬ ì°¾ê¸°
                                title_link = article.select_one('.tit a')
                                if title_link and title_link.get('href'):
                                    href = title_link.get('href')
                                    
                                    # ë™ì•„ì¼ë³´ ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
                                    if '/article/' in href:
                                        full_url = urljoin(self.base_url, href) if href.startswith('/') else href
                                        if self._is_valid_article_url(full_url):
                                            links.append(full_url)
                                            
                            except Exception as e:
                                continue
                                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {url} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            
        return links
    
    def _is_valid_article_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
        is_valid = (
            'donga.com' in url and 
            '/article/' in url and
            not url.endswith('.jpg') and
            not url.endswith('.png')
        )
        return is_valid
    
    async def crawl_articles(self, urls: List[str]) -> List[Dict]:
        """ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§"""
        articles = []
        semaphore = asyncio.Semaphore(10)  # ë™ì‹œ ìš”ì²­ ì œí•œ
        
        async def crawl_single_article(url: str) -> Optional[Dict]:
            async with semaphore:
                try:
                    return await self._crawl_single_article(url)
                except Exception as e:
                    logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {str(e)}")
                    return None
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("({task.completed}/{task.total})"),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            task = progress.add_task("ë™ì•„ì¼ë³´ ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì¤‘...", total=len(urls))
            
            # ë™ì‹œ ì‹¤í–‰
            tasks = [crawl_single_article(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and result:
                    articles.append(result)
                progress.advance(task)
        
        return articles
    
    async def _crawl_single_article(self, url: str) -> Optional[Dict]:
        """ë‹¨ì¼ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=15) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ì œëª© ì¶”ì¶œ
                        title = self._extract_title(soup)
                        if not title:
                            return None
                        
                        # ë³¸ë¬¸ ì¶”ì¶œ
                        content = self._extract_content(soup)
                        if not content:
                            return None
                        
                        # ë°œí–‰ ì‹œê°„ ì¶”ì¶œ
                        published_at = self._extract_published_time(soup)
                        
                        return {
                            'title': title,
                            'url': url,
                            'content': content,
                            'published_at': published_at
                        }
                        
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {url}: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        try:
            # ë™ì•„ì¼ë³´ ì œëª© ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
            title_selectors = [
                'h1:not(:has(a))',  # ë§í¬ê°€ ì—†ëŠ” h1 (ë¡œê³  ì œì™¸)
                'title',
                'meta[property="og:title"]'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    if selector == 'title':
                        title = title_elem.get_text(strip=True)
                        # "ï½œë™ì•„ì¼ë³´" ë¶€ë¶„ ì œê±°
                        if 'ï½œ' in title:
                            title = title.split('ï½œ')[0]
                    elif selector == 'meta[property="og:title"]':
                        title = title_elem.get('content', '')
                    else:
                        title = title_elem.get_text(strip=True)
                    
                    if title and len(title) > 5 and title != 'ë™ì•„ì¼ë³´':
                        return title
            
            return None
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ë™ì•„ì¼ë³´ ë³¸ë¬¸ ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
            content_selectors = [
                'section.news_view',
                'meta[property="og:description"]',
                'meta[name="description"]',
                '.article_body',
                '.article_content',
                '.content',
                '.article_txt'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    if selector == 'section.news_view':
                        # ì‹¤ì œ ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ê´‘ê³ ì™€ ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_elem.select('.view_ad06, .view_m_adA, .view_m_adB, .view_m_adK, .a1, script, .ad'):
                            unwanted.decompose()
                        
                        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                        content = content_elem.get_text(separator='\n', strip=True)
                        
                        # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬
                        content = '\n'.join(line.strip() for line in content.split('\n') if line.strip())
                        
                    elif selector.startswith('meta'):
                        content = content_elem.get('content', '')
                    else:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_elem.select('.advertisement, .related_news, .social_share'):
                            unwanted.decompose()
                        content = content_elem.get_text(strip=True)
                    
                    if content and len(content) > 50:
                        return content
            
            return None
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_published_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ë°œí–‰ ì‹œê°„ ì¶”ì¶œ"""
        try:
            # ë™ì•„ì¼ë³´ ì‹œê°„ ì„ íƒì (ìš°ì„ ìˆœìœ„ ìˆœ)
            time_selectors = [
                'meta[property="og:pubdate"]',
                'meta[property="article:published_time"]',
                'meta[property="dd:published_time"]',
                '.article_date',
                '.date',
                '.publish_date'
            ]
            
            for selector in time_selectors:
                time_elem = soup.select_one(selector)
                if time_elem:
                    if selector.startswith('meta'):
                        time_str = time_elem.get('content')
                    else:
                        time_str = time_elem.get_text(strip=True)
                    
                    if time_str:
                        # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹±
                        parsed_time = self._parse_time_string(time_str)
                        if parsed_time:
                            return parsed_time
            
            return None
        except Exception as e:
            logger.error(f"ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _parse_time_string(self, time_str: str) -> Optional[datetime]:
        """ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ íŒŒì‹±"""
        try:
            # "1ì‹œê°„ ì „", "2ì‹œê°„ ì „" ë“±ì˜ ìƒëŒ€ì  ì‹œê°„ ì²˜ë¦¬
            if 'ì‹œê°„ ì „' in time_str:
                hours = int(re.search(r'(\d+)', time_str).group(1))
                from datetime import timedelta
                return datetime.now() - timedelta(hours=hours)
            
            # "2025.08.20 22:48" í˜•ì‹ ì²˜ë¦¬
            if re.match(r'\d{4}\.\d{2}\.\d{2}', time_str):
                time_str = time_str.replace('.', '-')
                return datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            
            # ISO í˜•ì‹ ì²˜ë¦¬
            if 'T' in time_str and 'Z' in time_str:
                return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
            
            return None
        except Exception as e:
            logger.error(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_str}, {str(e)}")
            return None
    
    def display_results(self, articles: List[Dict], total_time: float):
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        self.console.print("\n" + "="*80)
        self.console.print(f"ğŸ¯ ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ì™„ë£Œ!")
        self.console.print("="*80)
        
        # í†µê³„ ì •ë³´
        stats_table = Table(title="ğŸ“Š í¬ë¡¤ë§ í†µê³„")
        stats_table.add_column("í•­ëª©", style="cyan")
        stats_table.add_column("ê°’", style="green")
        
        stats_table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(articles)))
        stats_table.add_row("í¬ë¡¤ë§ ì‹œê°„", f"{total_time:.2f}ì´ˆ")
        stats_table.add_row("í‰ê·  ì†ë„", f"{len(articles)/total_time:.2f} ê¸°ì‚¬/ì´ˆ")
        
        self.console.print(stats_table)
        
        # ìƒ˜í”Œ ê¸°ì‚¬ í‘œì‹œ
        if articles:
            sample_table = Table(title="ğŸ“° ìƒ˜í”Œ ê¸°ì‚¬ (ì²˜ìŒ 10ê°œ)")
            sample_table.add_column("ë²ˆí˜¸", style="cyan")
            sample_table.add_column("ì œëª©", style="white")
            sample_table.add_column("URL", style="blue")
            sample_table.add_column("ë°œí–‰ì‹œê°„", style="yellow")
            
            for i, article in enumerate(articles[:10], 1):
                title = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
                url = article['url'][:60] + "..." if len(article['url']) > 60 else article['url']
                published = article['published_at'].strftime('%Y-%m-%d %H:%M') if article['published_at'] else "N/A"
                
                sample_table.add_row(str(i), title, url, published)
            
            self.console.print(sample_table)
    
    async def save_to_database(self, articles: List[Dict]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            self.console.print("[yellow]ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return
        
        self.console.print(f"\nğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        
        saved_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # ìƒˆë¡œ ë§Œë“  ì €ì¥ ë©”ì„œë“œ ì‚¬ìš©
                if await self.save_article_to_supabase(article):
                    saved_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}... - {str(e)}[/red]")
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                failed_count += 1
        
        self.console.print(f"\nâœ… ì´ {saved_count}ê°œ ê¸°ì‚¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if failed_count > 0:
            self.console.print(f"[red]ì‹¤íŒ¨: {failed_count}ê°œ[/red]")
    
    async def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        self.console.print(Panel(
            f"[bold blue]ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬[/bold blue]\n"
            f"ëª©í‘œ: [bold green]{self.max_articles}ê°œ[/bold green] ê¸°ì‚¬ ìˆ˜ì§‘\n"
            f"ì–¸ë¡ ì‚¬: [bold yellow]{self.media_name}[/bold yellow] ({self.media_bias})",
            title="ğŸš€ í¬ë¡¤ëŸ¬ ì‹œì‘",
            border_style="blue"
        ))
        
        try:
            # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            self.console.print("\nğŸ” 1ë‹¨ê³„: ë™ì•„ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
            links = await self.get_politics_article_links()
            
            if not links:
                self.console.print("[red]ìˆ˜ì§‘ëœ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            # 2. ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§
            self.console.print(f"\nğŸ“° 2ë‹¨ê³„: {len(links)}ê°œ ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§ ì¤‘...")
            articles = await self.crawl_articles(links)
            
            if not articles:
                self.console.print("[red]í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            # 3. ê²°ê³¼ í‘œì‹œ
            total_time = time.time() - start_time
            self.display_results(articles, total_time)
            
            # 4. ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            await self.save_to_database(articles)
            
        except Exception as e:
            self.console.print(f"[red]í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}[/red]")
            logger.error(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_articles()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

    async def create_default_issue(self):
        """ê¸°ë³¸ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì´ìŠˆ í™•ì¸
            existing = self.supabase_manager.client.table('issues').select('id').eq('id', 1).execute()
            
            if not existing.data:
                # ê¸°ë³¸ ì´ìŠˆ ìƒì„±
                issue_data = {
                    'id': 1,
                    'title': 'ê¸°ë³¸ ì´ìŠˆ',
                    'subtitle': 'í¬ë¡¤ëŸ¬ë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ì´ìŠˆ',
                    'summary': 'ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ëœ ì •ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ì´ìŠˆì…ë‹ˆë‹¤.',
                    'bias_left_pct': 0,
                    'bias_center_pct': 0,
                    'bias_right_pct': 0,
                    'dominant_bias': 'center',
                    'source_count': 0
                }
                
                result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                self.console.print("âœ… ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì™„ë£Œ")
            else:
                self.console.print("â„¹ï¸ ê¸°ë³¸ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                
        except Exception as e:
            self.console.print(f"âŒ ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    async def save_article_to_supabase(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
            await self.create_default_issue()
            
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            published_at = article_data.get('published_at')
            if isinstance(published_at, datetime):
                published_at = published_at.isoformat()
            
            # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
            insert_data = {
                'issue_id': 1,  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                'media_id': 2,  # ë™ì•„ì¼ë³´ media_id
                'title': article_data['title'],
                'url': article_data['url'],
                'content': article_data['content'],
                'bias': self.media_bias.lower(),
                'published_at': published_at
            }
            
            # ê¸°ì‚¬ ì €ì¥
            result = self.supabase_manager.client.table('articles').insert(insert_data).execute()
            
            if result.data:
                self.console.print(f"âœ… ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:30]}...")
                return True
            else:
                self.console.print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:30]}...")
                return False
                
        except Exception as e:
            self.console.print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = DongaPoliticsCrawler(max_articles=100)
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
