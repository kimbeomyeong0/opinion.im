#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ìµœì í™” ë²„ì „)
ê²½í–¥ì‹ ë¬¸ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì •ì¹˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich import box
import re
from utils.supabase_manager_unified import UnifiedSupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rich ì½˜ì†” ì„¤ì •
console = Console()

class KhanPoliticsCrawler:
    def __init__(self):
        self.base_url = "https://www.khan.co.kr"
        self.politics_url = "https://www.khan.co.kr/politics"
        self.supabase_manager = UnifiedSupabaseManager()
        self.media_outlet = "ê²½í–¥ì‹ ë¬¸"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    async def get_page_content(self, session, url):
        """í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
            return None

    async def collect_article_links(self, session, max_articles=100):
        """ê¸°ì‚¬ ë§í¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        console.print("ğŸ” ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = []
        page = 1
        
        while len(all_links) < max_articles and page <= 10:  # ìµœëŒ€ 10í˜ì´ì§€
            if page == 1:
                url = self.politics_url
            else:
                url = f"{self.politics_url}?page={page}"
            
            console.print(f"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ({url})")
            
            html_content = await self.get_page_content(session, url)
            if not html_content:
                break
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°: div.list ul#recentList li article div a
            article_elements = soup.select('div.list ul#recentList li article div a')
            
            page_links = []
            for element in article_elements:
                href = element.get('href')
                if href and '/article/' in href:
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    else:
                        full_url = href
                    page_links.append(full_url)
            
            # ìƒë‹¨ ê¸°ì‚¬ ì„¹ì…˜ì—ì„œë„ ë§í¬ ìˆ˜ì§‘
            top_articles = soup.select('dl dt article div a, dl dd ul li article div a')
            for element in top_articles:
                href = element.get('href')
                if href and '/article/' in href:
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    else:
                        full_url = href
                    if full_url not in page_links:
                        page_links.append(full_url)
            
            # contents ì„¹ì…˜ì—ì„œë„ ë§í¬ ìˆ˜ì§‘
            contents_articles = soup.select('section.contents article div a')
            for element in contents_articles:
                href = element.get('href')
                if href and '/article/' in href:
                    if href.startswith('/'):
                        full_url = f"{self.base_url}{href}"
                    else:
                        full_url = href
                    if full_url not in page_links:
                        page_links.append(full_url)
            
            if not page_links:
                console.print(f"âŒ {page}í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            all_links.extend(page_links)
            console.print(f"âœ… {page}í˜ì´ì§€ì—ì„œ {len(page_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ì´ {len(all_links)}ê°œ)")
            
            page += 1
        
        # ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°
        if len(all_links) > max_articles:
            all_links = all_links[:max_articles]
        
        console.print(f"ğŸ¯ ì´ {len(all_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_links

    def extract_article_content(self, html_content, url):
        """ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ - ê²½í–¥ì‹ ë¬¸ì˜ ê²½ìš° title íƒœê·¸ë‚˜ og:title ë©”íƒ€íƒœê·¸ì—ì„œ ì¶”ì¶œ
        title = None
        
        # 1. og:title ë©”íƒ€íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ (ê°€ì¥ ì •í™•)
        meta_title = soup.find('meta', property='og:title')
        if meta_title and meta_title.get('content'):
            title = meta_title.get('content').strip()
        
        # 2. title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        if not title:
            page_title = soup.find('title')
            if page_title:
                title = page_title.get_text(strip=True)
                # ì‚¬ì´íŠ¸ëª… ì œê±° (ì˜ˆ: " - ê²½í–¥ì‹ ë¬¸")
                if ' - ' in title:
                    title = title.split(' - ')[0].strip()
        
        # 3. ë‹¤ë¥¸ ì„ íƒìë“¤
        if not title:
            title_selectors = [
                'h1',
                '.article_title',
                'h1.title',
                'h2.title',
                'h3.title',
                'p.title',
                '.title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    candidate_title = title_elem.get_text(strip=True)
                    if candidate_title and len(candidate_title) > 10:
                        title = candidate_title
                        break
        
        if not title:
            title = f"ê²½í–¥ì‹ ë¬¸ ê¸°ì‚¬ - {url.split('/')[-1]}"

        # ë³¸ë¬¸ ì¶”ì¶œ - ê²½í–¥ì‹ ë¬¸ì˜ ê²½ìš° art_body í´ë˜ìŠ¤ì—ì„œ ì¶”ì¶œ
        content = None
        content_selectors = [
            'div.art_body',  # ê²½í–¥ì‹ ë¬¸ ì‹¤ì œ ë³¸ë¬¸
            'section.art_cont',  # ê²½í–¥ì‹ ë¬¸ ë³¸ë¬¸ ì„¹ì…˜
            'div.article_body',
            'div.article-content',
            'div.content',
            'div.body',
            'div.article-body',
            'div.text',
            'article',
            '.desc'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related, .comment, .ui-control, .font-control, .share-control, .sidebar, .side-news, .related-news, .reading-mode, .dark-mode, .font-size, .bookmark, .print, .share, .reaction, .recommend, .like, .subscribe, .donation, .footer, .navigation, .menu, .header, .banner, .art_photo, .art_photo_wrap, .caption, .contbox-wrap, .footerFirst, .editor-wrap, .swiper-container, .area-replay-wrap, .replay-cont, .bottom-wrap, .m-pop, .wrap.category'):
                    unwanted.decompose()
                
                content = content_elem.get_text(strip=True, separator='\n')
                if content and len(content) > 100:
                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                    unwanted_patterns = [
                        'ì½ê¸°ëª¨ë“œ', 'ë‹¤í¬ëª¨ë“œ', 'í°íŠ¸í¬ê¸°', 'ê°€ê°€ê°€ê°€ê°€ê°€', 'ë¶ë§ˆí¬', 'ê³µìœ í•˜ê¸°', 'í”„ë¦°íŠ¸',
                        'ê¸°ì‚¬ë°˜ì‘', 'ì¶”ì²œí•´ìš”', 'ì¢‹ì•„ìš”', 'ê°ë™ì´ì—ìš”', 'í™”ë‚˜ìš”', 'ìŠ¬í¼ìš”',
                        'My ì¶”ì²œ ê¸°ì‚¬', 'ê°€ì¥ ë§ì´ ì½ì€ ê¸°ì‚¬', 'ëŒ“ê¸€ ë§ì€ ê¸°ì‚¬', 'ì‹¤ì‹œê°„ ìµœì‹  ë‰´ìŠ¤',
                        'ì£¼ìš”ë‰´ìŠ¤', 'ì´ìŠˆNOW', 'ê´€ë ¨ê¸°ì‚¬', 'ë”ë³´ê¸°', 'ëª©ë¡', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€',
                        'êµ¬ë…', 'ê¸°ì‚¬ í›„ì›í•˜ê¸°', 'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ë¼ì¸', 'ë§í¬ë³µì‚¬',
                        'í¼ì¹˜ê¸°/ì ‘ê¸°', 'ê¸°ì‚¬ ì½ê¸°', 'ìš”ì•½', 'ë‹«ê¸°', 'Please activate JavaScript',
                        'AD', 'ëŒ“ê¸€', 'ìƒˆë¡œê³ ì¹¨', 'ì£¼ìš” ê¸°ì‚¬', 'ë‰´ìŠ¤ë£¸ PICK', 'ì§€ê¸ˆ ë§ì´ ë³´ëŠ” ê¸°ì‚¬'
                    ]
                    
                    for pattern in unwanted_patterns:
                        content = content.replace(pattern, '')
                    
                    # ê¸°ì ì •ë³´ ì´í›„ì˜ ëª¨ë“  ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
                    lines = content.split('\n')
                    cleaned_lines = []
                    for line in lines:
                        # ê¸°ì ì •ë³´ë¥¼ ì°¾ìœ¼ë©´ ê·¸ ì´í›„ëŠ” ëª¨ë‘ ì œê±°
                        if 'ê¸°ì' in line and len(line.strip()) < 30:
                            cleaned_lines.append(line.strip())
                            break
                        cleaned_lines.append(line)
                    
                    content = '\n'.join(cleaned_lines)
                    
                    # ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€ ì •ë¦¬
                    import re
                    
                    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
                    content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', content)
                    
                    # í•´ì‹œíƒœê·¸ ì œê±°
                    content = re.sub(r'#\s*\w+', '', content)
                    
                    # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸ ì œê±°
                    content = re.sub(r'^\d+$', '', content, flags=re.MULTILINE)
                    
                    # ì‹œê°„ í˜•ì‹ ì œê±° (12:10 ê°™ì€)
                    content = re.sub(r'^\d{1,2}:\d{2}$', '', content, flags=re.MULTILINE)
                    
                    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                    content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
                    
                    if content and len(content) > 100:
                        break
        
        return title, content

    def extract_publish_date(self, html_content):
        """ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë‚ ì§œ ì„ íƒìë“¤
        date_selectors = [
            '.date',
            'p.date',
            '.publish-date',
            '.article-date',
            'time',
            '[datetime]'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    # ê²½í–¥ì‹ ë¬¸ ë‚ ì§œ í˜•ì‹: "30ë¶„ ì „", "1ì‹œê°„ ì „" ë“±
                    try:
                        # ìƒëŒ€ì  ì‹œê°„ì„ ì ˆëŒ€ ì‹œê°„ìœ¼ë¡œ ë³€í™˜
                        now = datetime.now()
                        
                        if 'ë¶„ ì „' in date_text:
                            minutes = int(re.search(r'(\d+)ë¶„', date_text).group(1))
                            date_obj = now - timedelta(minutes=minutes)
                        elif 'ì‹œê°„ ì „' in date_text:
                            hours = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
                            date_obj = now - timedelta(hours=hours)
                        elif 'ì¼ ì „' in date_text:
                            days = int(re.search(r'(\d+)ì¼', date_text).group(1))
                            date_obj = now - timedelta(days=days)
                        else:
                            # ì ˆëŒ€ ë‚ ì§œ í˜•ì‹ì´ ìˆëŠ” ê²½ìš°
                            date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})', date_text)
                            if date_match:
                                year, month, day = date_match.groups()
                                date_obj = datetime(int(year), int(month), int(day))
                            else:
                                date_obj = now
                        
                        return date_obj.isoformat()
                    except Exception as e:
                        logger.error(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_text} - {e}")
                        continue
        
        # ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        return datetime.now().isoformat()

    async def process_single_article(self, session, url, semaphore):
        """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        async with semaphore:
            try:
                # ê¸°ì‚¬ê°€ ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸
                existing_article = self.supabase_manager.client.table('articles').select('id').eq('url', url).execute()
                if existing_article.data:
                    return True  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬
                
                # ê¸°ì‚¬ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                html_content = await self.get_page_content(session, url)
                if not html_content:
                    return False
                
                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                title, content = self.extract_article_content(html_content, url)
                if not title or not content:
                    return False
                
                # ë°œí–‰ì¼ ì¶”ì¶œ
                publish_date = self.extract_publish_date(html_content)
                
                # ìƒˆ ê¸°ì‚¬ ì‚½ì…
                article_data = {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': publish_date,
                    'media_id': self.supabase_manager.get_media_outlet(self.media_outlet)['id'],
                    'issue_id': 6  # ì„ì‹œ issue_id
                }
                
                result = self.supabase_manager.insert_article(article_data)
                return result is not None
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
                return False

    async def crawl_articles(self, article_links):
        """ê¸°ì‚¬ë“¤ì„ ë™ì‹œì— í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        console.print(f"\nğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì„¸ì…˜ ì„¤ì • - ì„±ëŠ¥ ìµœì í™”
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=10, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=console
            ) as progress:
                
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                # ë™ì‹œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ìµœëŒ€ 30ê°œ ë™ì‹œ ìš”ì²­)
                semaphore = asyncio.Semaphore(30)
                
                async def process_with_progress(url):
                    result = await self.process_single_article(session, url, semaphore)
                    progress.update(task, advance=1)
                    return result
                
                # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
                results = await asyncio.gather(*[process_with_progress(url) for url in article_links], return_exceptions=True)
                
                success_count = sum(1 for result in results if result is True)
                failed_count = len(results) - success_count
        
        return success_count, failed_count

    async def run(self):
        """í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        start_time = datetime.now()
        
        console.print("ğŸš€ ê²½í–¥ì‹ ë¬¸ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘!")
        console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ì„¸ì…˜ ì„¤ì •
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=10, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.collect_article_links(session)
            
            if not article_links:
                console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2. ê¸°ì‚¬ í¬ë¡¤ë§
            success_count, failed_count = await self.crawl_articles(article_links)
        
        # ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        duration = end_time - start_time
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="ê²½í–¥ì‹ ë¬¸ í¬ë¡¤ë§ ê²°ê³¼", box=box.ROUNDED)
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê°’", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(article_links)))
        table.add_row("ì„±ê³µ", f"{success_count}ê°œ", style="green")
        table.add_row("ì‹¤íŒ¨", f"{failed_count}ê°œ", style="red")
        table.add_row("ì„±ê³µë¥ ", f"{(success_count/len(article_links)*100):.1f}%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration.total_seconds():.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{len(article_links)/duration.total_seconds():.2f} ê¸°ì‚¬/ì´ˆ")
        
        console.print(table)
        console.print(f"âœ… ê²½í–¥ì‹ ë¬¸ í¬ë¡¤ë§ ì™„ë£Œ! ğŸ‰")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = KhanPoliticsCrawler()
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
