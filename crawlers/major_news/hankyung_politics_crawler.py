#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ëŸ¬
- ëª©í‘œ: í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- URL: https://www.hankyung.com/all-news-politics
- ë°©ì‹: í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ (page=1~5)
- ì¤‘ë³µ ì œê±°: URL ê¸°ì¤€
- ë°ì´í„° í’ˆì§ˆ: ì œëª©, ë§í¬, ë‚ ì§œ, ë³¸ë¬¸
- ì•ˆì •ì„±: 3ë‹¨ê³„ fallback ì „ëµ
- ì €ì¥: Supabase articles í…Œì´ë¸”
- ì†ë„: 20ì´ˆ ë‚´ì™¸
"""
import sys
import os
import requests
from bs4 import BeautifulSoup
import time
import re
from tqdm import tqdm
from typing import List, Dict, Optional, Set
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager_unified import UnifiedSupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
BASE_URL = "https://www.hankyung.com/all-news-politics"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

class HankyungPoliticsCrawler:
    """í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100):
        self.max_articles = max_articles
        self.articles = []
        self.seen_urls: Set[str] = set()
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.supabase_manager = UnifiedSupabaseManager()
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.adaptive_delay = 0.3
        self.min_delay = 0.1
        self.max_delay = 1.0
        
        # ì—ëŸ¬ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        
        # í•œêµ­ê²½ì œëŠ” ì¤‘ë„ ì„±í–¥
        self.media_name = "í•œêµ­ê²½ì œ"
        self.media_bias = "center"
    
    def clean_text(self, text: str) -> str:
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""
        if not text:
            return ""
            
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        text = re.sub(r"â“’\s*í•œêµ­ê²½ì œ.*", "", text)  # ì €ì‘ê¶Œ ë¬¸êµ¬
        text = re.sub(r"[\w\.-]+@[\w\.-]+\.\w+", "", text)  # ì´ë©”ì¼ ì œê±°
        text = re.sub(r"ê¸°ì\s*:.*?(?:\n|$)", "", text, flags=re.MULTILINE)  # ê¸°ì ì •ë³´
        text = re.sub(r"í¸ì§‘\s*:.*?(?:\n|$)", "", text, flags=re.MULTILINE)  # í¸ì§‘ ì •ë³´
        
        # ê´‘ê³  ê´€ë ¨ ë¬¸êµ¬ ì œê±°
        text = re.sub(r"(ê´‘ê³ |sponsored|advertisement).*?(?:\n|$)", "", text, flags=re.MULTILINE | re.IGNORECASE)
        
        # ë¹ˆ ì¤„ ì •ë¦¬
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def parse_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (3ë‹¨ê³„ fallback)"""
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ê¸°ë³¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            container = soup.select_one("div.article-body#articletxt")
            if container:
                # br ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
                content = container.decode_contents()
                paragraphs = content.split("<br>")
                
                clean_paragraphs = []
                for p in paragraphs:
                    if p.strip():
                        # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        clean_p = BeautifulSoup(p, "html.parser").get_text(" ", strip=True)
                        if clean_p and len(clean_p) > 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                            clean_paragraphs.append(clean_p)
                
                if clean_paragraphs:
                    result = "\n".join(clean_paragraphs)
                    return self.clean_text(result)
            
            # 2ë‹¨ê³„: ëŒ€ì•ˆ ì„ íƒìë“¤ ì‹œë„
            fallback_selectors = [
                ".article-body",
                ".article-content", 
                ".content",
                "article",
                ".article_body"
            ]
            
            for selector in fallback_selectors:
                container = soup.select_one(selector)
                if container:
                    text = container.get_text("\n", strip=True)
                    if text and len(text) > 100:  # ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ì¸ì§€ í™•ì¸
                        return self.clean_text(text)
            
            # 3ë‹¨ê³„: ëª¨ë“  p íƒœê·¸ì—ì„œ ì¶”ì¶œ
            all_paragraphs = soup.find_all('p')
            if all_paragraphs:
                paragraphs_text = []
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                        paragraphs_text.append(text)
                
                if paragraphs_text:
                    result = "\n".join(paragraphs_text)
                    return self.clean_text(result)
            
            return "[ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤]"
            
        except requests.RequestException as e:
            self.network_errors += 1
            return f"[ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜] {str(e)}"
        except Exception as e:
            self.parsing_errors += 1
            return f"[ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨] {str(e)}"
    
    def crawl_page(self, page: int) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘"""
        articles = []
        url = f"{BASE_URL}?page={page}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
            news_items = soup.select("div.allnews-wrap div.allnews-panel ul.allnews-list li")
            if not news_items:
                return articles
            
            for item in news_items:
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_elem = item.select_one("h2.news-tit a")
                    date_elem = item.select_one("p.txt-date")
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get("href", "")
                    
                    # ë§í¬ê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if link and not link.startswith("http"):
                        link = "https://www.hankyung.com" + link
                    
                    # ì¤‘ë³µ í™•ì¸
                    if link in self.seen_urls:
                        continue
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date = date_elem.get_text(strip=True) if date_elem else ""
                    
                    # ê³ ìœ  ID ì¶”ì¶œ (data-aid ì†ì„±)
                    join_key = item.get("data-aid", "")
                    if not join_key:
                        # URLì—ì„œ ê¸°ì‚¬ ID ì¶”ì¶œ
                        join_key_match = re.search(r"/article/(\d+)", link)
                        if join_key_match:
                            join_key = join_key_match.group(1)
                        else:
                            join_key = str(hash(link))  # ìµœí›„ ìˆ˜ë‹¨
                    
                    # ë³¸ë¬¸ ìˆ˜ì§‘
                    body = self.parse_article_content(link)
                    
                    # ê¸°ì‚¬ ì •ë³´ êµ¬ì„±
                    article = {
                        "title": title,
                        "url": link,
                        "published_at": date,
                        "content": body,
                        "join_key": join_key,
                        "crawled_at": datetime.now().isoformat()
                    }
                    
                    articles.append(article)
                    self.seen_urls.add(link)
                    
                except Exception as e:
                    self.parsing_errors += 1
                    logger.warning(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
            
            return articles
            
        except requests.RequestException as e:
            self.network_errors += 1
            logger.error(f"í˜ì´ì§€ {page} ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
            return articles
        except Exception as e:
            self.parsing_errors += 1
            logger.error(f"í˜ì´ì§€ {page} íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return articles
    
    def crawl_hankyung(self) -> List[Dict]:
        """í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        page = 1
        max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ì‹œë„
        start_time = time.time()
        
        print(f"ğŸš€ í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ëª©í‘œ: {self.max_articles}ê°œ ê¸°ì‚¬")
        print(f"ğŸ“° ëŒ€ìƒ URL: {BASE_URL}")
        
        with tqdm(total=self.max_articles, desc="í¬ë¡¤ë§ ì§„í–‰", unit="ê¸°ì‚¬") as pbar:
            while len(self.articles) < self.max_articles and page <= max_pages:
                print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘... (í˜„ì¬: {len(self.articles)}ê°œ)")
                
                # í˜ì´ì§€ í¬ë¡¤ë§
                page_articles = self.crawl_page(page)
                
                if not page_articles:
                    print(f"âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # 3ë‹¨ê³„ fallback: ë‚¨ì€ ê°œìˆ˜ ì±„ìš°ê¸° ì‹œë„
                    if len(self.articles) < self.max_articles:
                        remaining = self.max_articles - len(self.articles)
                        print(f"ğŸ”„ ë‚¨ì€ {remaining}ê°œ ê¸°ì‚¬ë¥¼ ìœ„í•œ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„...")
                        
                        # ì¶”ê°€ í˜ì´ì§€ë“¤ ì‹œë„
                        for extra_page in range(page + 1, page + 5):
                            extra_articles = self.crawl_page(extra_page)
                            if extra_articles:
                                for article in extra_articles:
                                    if len(self.articles) >= self.max_articles:
                                        break
                                    if article["url"] not in self.seen_urls:
                                        self.articles.append(article)
                                        self.seen_urls.add(article["url"])
                                        pbar.update(1)
                            time.sleep(self.adaptive_delay)
                    
                    break
                
                # ìƒˆ ê¸°ì‚¬ ì¶”ê°€
                new_count = 0
                for article in page_articles:
                    if len(self.articles) >= self.max_articles:
                        break
                    
                    self.articles.append(article)
                    new_count += 1
                    pbar.update(1)
                
                print(f"âœ… í˜ì´ì§€ {page}: {new_count}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {len(self.articles)}ê°œ)")
                
                # 100ê°œ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                if len(self.articles) >= self.max_articles:
                    print(f"ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë‹¬ì„±! í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                
                # ì„±ê³µì ì¸ í¬ë¡¤ë§ìœ¼ë¡œ ë”œë ˆì´ ì¡°ì •
                self.adjust_delay(True)
                
                page += 1
                time.sleep(self.adaptive_delay)
        
        # ê²°ê³¼ ì €ì¥ (ë©”ì¸ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°)
        # self.save_to_supabase()
        
        # ì„±ëŠ¥ ë¶„ì„
        end_time = time.time()
        duration = end_time - start_time
        
        # display_resultsëŠ” ë©”ì¸ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì œê±°
        # self.display_results(duration)
        return self.articles
    
    def adjust_delay(self, success: bool):
        """ì ì‘í˜• ë”œë ˆì´ ì¡°ì •"""
        if success:
            # ì„±ê³µ ì‹œ ë”œë ˆì´ ê°ì†Œ
            self.adaptive_delay = max(self.adaptive_delay * 0.9, self.min_delay)
        else:
            # ì‹¤íŒ¨ ì‹œ ë”œë ˆì´ ì¦ê°€
            self.adaptive_delay = min(self.adaptive_delay * 1.2, self.max_delay)
    
    async def save_to_database(self, articles: List[Dict]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            print("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"\nğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        
        successful_saves = 0
        failed_saves = 0
        
        for article in articles:
            try:
                # ìƒˆë¡œ ë§Œë“  ì €ì¥ ë©”ì„œë“œ ì‚¬ìš©
                if await self.save_article_to_supabase(article):
                    successful_saves += 1
                else:
                    failed_saves += 1
                    
            except Exception as e:
                failed_saves += 1
                print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article['title']} - {str(e)}")
        
        print(f"\nğŸ“Š Supabase ì €ì¥ ê²°ê³¼:")
        print(f"  â€¢ ì„±ê³µ: {successful_saves}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {failed_saves}ê°œ")
        print(f"  â€¢ ì´ ê¸°ì‚¬: {len(articles)}ê°œ")
        
        if successful_saves > 0:
            print(f"âœ… {successful_saves}ê°œ ê¸°ì‚¬ê°€ Supabaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    def display_results(self, duration: float):
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        print(f"\n{'='*60}")
        print(f"      í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!      ")
        print(f"{'='*60}")
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ë¬¸ì œ í•´ê²°
        if duration > 0:
            print(f"  â€¢ í‰ê·  ì†ë„: {len(self.articles)/duration:.1f} ê¸°ì‚¬/ì´ˆ")
        else:
            print(f"  â€¢ í‰ê·  ì†ë„: ê³„ì‚° ë¶ˆê°€")
            
        print(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.network_errors}íšŒ")
        print(f"  â€¢ íŒŒì‹± ì˜¤ë¥˜: {self.parsing_errors}íšŒ")
        
        if duration > 25:
            print(f"âš ï¸  ëª©í‘œ ì‹œê°„(25ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {duration:.1f}ì´ˆ")
        else:
            print(f"âœ… ëª©í‘œ ì‹œê°„ ë‚´ ì™„ë£Œ: {duration:.1f}ì´ˆ")
        
        if len(self.articles) >= self.max_articles:
            print(f"âœ… ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë‹¬ì„±!")
        else:
            print(f"âš ï¸  ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë¯¸ë‹¬ì„±: {len(self.articles)}ê°œ")
    
    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (ë¹„ë™ê¸° ì¸í„°í˜ì´ìŠ¤)"""
        try:
            return self.crawl_hankyung()
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return self.articles
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return self.articles

    def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            return self.crawl_hankyung()
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return self.articles
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return self.articles

    async def create_default_issue(self):
        """ê¸°ë³¸ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì´ìŠˆ í™•ì¸
            existing = self.supabase_manager.client.table('issues').select('id').eq('id', 1).execute()
            
            if not existing.data:
                # ê¸°ë³¸ ì´ìŠˆ ìƒì„±
                issue_data = {
                    'id': 1,
                    'title': 'ê¸°ë³¸ ì´ìŠˆ',
                    'subtitle': 'í¬ë¡¤ëŸ¬ë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ì´ìŠˆ',
                    'summary': 'ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ëœ ì •ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ì´ìŠˆì…ë‹ˆë‹¤.',
                    'bias_left_pct': 0,
                    'bias_center_pct': 0,
                    'bias_right_pct': 0,
                    'dominant_bias': 'center',
                    'source_count': 0
                }
                
                result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                logger.info("ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì„±ê³µ")
                return True
            else:
                logger.info("ê¸°ë³¸ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                return True
                
        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    async def save_article_to_supabase(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
            await self.create_default_issue()
            
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            published_at = article_data.get('published_at')
            if isinstance(published_at, datetime):
                published_at = published_at.isoformat()
            
            # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
            insert_data = {
                'issue_id': 1,  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                'media_id': 4,  # í•œê²½ media_id
                'title': article_data['title'],
                'url': article_data['url'],
                'content': article_data['content'],
                'bias': self.media_bias.lower(),
                'published_at': published_at
            }
            
            # Supabaseì— ì €ì¥
            result = self.supabase_manager.client.table('articles').insert(insert_data).execute()
            
            if result.data:
                logger.info(f"ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:50]}...")
                return True
            else:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:50]}...")
                return False
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = HankyungPoliticsCrawler(max_articles=100)
    
    # ê¸°ì‚¬ ìˆ˜ì§‘
    articles = await crawler.collect_all_articles()
    
    # ê²°ê³¼ í‘œì‹œ
    crawler.display_results(0)  # ì‹œê°„ì€ ì„ì‹œë¡œ 0ìœ¼ë¡œ ì„¤ì •
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    await crawler.save_to_database(articles)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
