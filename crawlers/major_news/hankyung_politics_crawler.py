#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ëŸ¬
- ëª©í‘œ: í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ì—ì„œ ìµœì‹  ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- URL: https://www.hankyung.com/all-news-politics
- ë°©ì‹: í˜ì´ì§€ë„¤ì´ì…˜ ê¸°ë°˜ (page=1~5)
- ì¤‘ë³µ ì œê±°: URL ê¸°ì¤€
- ë°ì´í„° í’ˆì§ˆ: ì œëª©, ë§í¬, ë‚ ì§œ, ë³¸ë¬¸
- ì•ˆì •ì„±: 3ë‹¨ê³„ fallback ì „ëµ
- ì €ì¥: Supabase articles í…Œì´ë¸”
- ì†ë„: 20ì´ˆ ë‚´ì™¸
"""
import requests
from bs4 import BeautifulSoup
import time
import re
from tqdm import tqdm
from typing import List, Dict, Optional, Set
from datetime import datetime
import logging
from utils.legacy.supabase_manager_v2 import SupabaseManagerV2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ìƒìˆ˜ ì •ì˜
BASE_URL = "https://www.hankyung.com/all-news-politics"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

class HankyungPoliticsCrawler:
    """í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100):
        self.max_articles = max_articles
        self.articles = []
        self.seen_urls: Set[str] = set()
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.supabase_manager = SupabaseManagerV2()
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.adaptive_delay = 0.3
        self.min_delay = 0.1
        self.max_delay = 1.0
        
        # ì—ëŸ¬ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        
        # í•œêµ­ê²½ì œëŠ” ì¤‘ë„ ì„±í–¥
        self.media_name = "í•œêµ­ê²½ì œ"
        self.media_bias = "center"
    
    def clean_text(self, text: str) -> str:
        """ë³¸ë¬¸ í…ìŠ¤íŠ¸ í›„ì²˜ë¦¬"""
        if not text:
            return ""
            
        # ë¶ˆí•„ìš”í•œ ë¬¸êµ¬ ì œê±°
        text = re.sub(r"â“’\s*í•œêµ­ê²½ì œ.*", "", text)  # ì €ì‘ê¶Œ ë¬¸êµ¬
        text = re.sub(r"[\w\.-]+@[\w\.-]+\.\w+", "", text)  # ì´ë©”ì¼ ì œê±°
        text = re.sub(r"ê¸°ì\s*:.*?(?:\n|$)", "", text, flags=re.MULTILINE)  # ê¸°ì ì •ë³´
        text = re.sub(r"í¸ì§‘\s*:.*?(?:\n|$)", "", text, flags=re.MULTILINE)  # í¸ì§‘ ì •ë³´
        
        # ê´‘ê³  ê´€ë ¨ ë¬¸êµ¬ ì œê±°
        text = re.sub(r"(ê´‘ê³ |sponsored|advertisement).*?(?:\n|$)", "", text, flags=re.MULTILINE | re.IGNORECASE)
        
        # ë¹ˆ ì¤„ ì •ë¦¬
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def parse_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (3ë‹¨ê³„ fallback)"""
        try:
            # 1ë‹¨ê³„: ê¸°ë³¸ ì„ íƒìë¡œ ë³¸ë¬¸ ì¶”ì¶œ
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ê¸°ë³¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            container = soup.select_one("div.article-body#articletxt")
            if container:
                # br ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë¶„ë¦¬
                content = container.decode_contents()
                paragraphs = content.split("<br>")
                
                clean_paragraphs = []
                for p in paragraphs:
                    if p.strip():
                        # HTML íƒœê·¸ ì œê±°í•˜ê³  í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        clean_p = BeautifulSoup(p, "html.parser").get_text(" ", strip=True)
                        if clean_p and len(clean_p) > 10:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                            clean_paragraphs.append(clean_p)
                
                if clean_paragraphs:
                    result = "\n".join(clean_paragraphs)
                    return self.clean_text(result)
            
            # 2ë‹¨ê³„: ëŒ€ì•ˆ ì„ íƒìë“¤ ì‹œë„
            fallback_selectors = [
                ".article-body",
                ".article-content", 
                ".content",
                "article",
                ".article_body"
            ]
            
            for selector in fallback_selectors:
                container = soup.select_one(selector)
                if container:
                    text = container.get_text("\n", strip=True)
                    if text and len(text) > 100:  # ì˜ë¯¸ìˆëŠ” ë³¸ë¬¸ì¸ì§€ í™•ì¸
                        return self.clean_text(text)
            
            # 3ë‹¨ê³„: ëª¨ë“  p íƒœê·¸ì—ì„œ ì¶”ì¶œ
            all_paragraphs = soup.find_all('p')
            if all_paragraphs:
                paragraphs_text = []
                for p in all_paragraphs:
                    text = p.get_text(strip=True)
                    if text and len(text) > 20:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                        paragraphs_text.append(text)
                
                if paragraphs_text:
                    result = "\n".join(paragraphs_text)
                    return self.clean_text(result)
            
            return "[ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤]"
            
        except requests.RequestException as e:
            self.network_errors += 1
            return f"[ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜] {str(e)}"
        except Exception as e:
            self.parsing_errors += 1
            return f"[ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨] {str(e)}"
    
    def crawl_page(self, page: int) -> List[Dict]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘"""
        articles = []
        url = f"{BASE_URL}?page={page}"
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
            news_items = soup.select("div.allnews-wrap div.allnews-panel ul.allnews-list li")
            if not news_items:
                return articles
            
            for item in news_items:
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_elem = item.select_one("h2.news-tit a")
                    date_elem = item.select_one("p.txt-date")
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get("href", "")
                    
                    # ë§í¬ê°€ ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if link and not link.startswith("http"):
                        link = "https://www.hankyung.com" + link
                    
                    # ì¤‘ë³µ í™•ì¸
                    if link in self.seen_urls:
                        continue
                    
                    # ë‚ ì§œ ì¶”ì¶œ
                    date = date_elem.get_text(strip=True) if date_elem else ""
                    
                    # ê³ ìœ  ID ì¶”ì¶œ (data-aid ì†ì„±)
                    join_key = item.get("data-aid", "")
                    if not join_key:
                        # URLì—ì„œ ê¸°ì‚¬ ID ì¶”ì¶œ
                        join_key_match = re.search(r"/article/(\d+)", link)
                        if join_key_match:
                            join_key = join_key_match.group(1)
                        else:
                            join_key = str(hash(link))  # ìµœí›„ ìˆ˜ë‹¨
                    
                    # ë³¸ë¬¸ ìˆ˜ì§‘
                    body = self.parse_article_content(link)
                    
                    # ê¸°ì‚¬ ì •ë³´ êµ¬ì„±
                    article = {
                        "title": title,
                        "url": link,
                        "published_at": date,
                        "content": body,
                        "join_key": join_key,
                        "crawled_at": datetime.now().isoformat()
                    }
                    
                    articles.append(article)
                    self.seen_urls.add(link)
                    
                except Exception as e:
                    self.parsing_errors += 1
                    logger.warning(f"ê¸°ì‚¬ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
                    continue
            
            return articles
            
        except requests.RequestException as e:
            self.network_errors += 1
            logger.error(f"í˜ì´ì§€ {page} ìš”ì²­ ì˜¤ë¥˜: {str(e)}")
            return articles
        except Exception as e:
            self.parsing_errors += 1
            logger.error(f"í˜ì´ì§€ {page} íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return articles
    
    def crawl_hankyung(self) -> List[Dict]:
        """í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
        page = 1
        max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ì‹œë„
        start_time = time.time()
        
        print(f"ğŸš€ í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ¯ ëª©í‘œ: {self.max_articles}ê°œ ê¸°ì‚¬")
        print(f"ğŸ“° ëŒ€ìƒ URL: {BASE_URL}")
        
        with tqdm(total=self.max_articles, desc="í¬ë¡¤ë§ ì§„í–‰", unit="ê¸°ì‚¬") as pbar:
            while len(self.articles) < self.max_articles and page <= max_pages:
                print(f"\nğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘... (í˜„ì¬: {len(self.articles)}ê°œ)")
                
                # í˜ì´ì§€ í¬ë¡¤ë§
                page_articles = self.crawl_page(page)
                
                if not page_articles:
                    print(f"âš ï¸  í˜ì´ì§€ {page}ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # 3ë‹¨ê³„ fallback: ë‚¨ì€ ê°œìˆ˜ ì±„ìš°ê¸° ì‹œë„
                    if len(self.articles) < self.max_articles:
                        remaining = self.max_articles - len(self.articles)
                        print(f"ğŸ”„ ë‚¨ì€ {remaining}ê°œ ê¸°ì‚¬ë¥¼ ìœ„í•œ ì¶”ê°€ ìˆ˜ì§‘ ì‹œë„...")
                        
                        # ì¶”ê°€ í˜ì´ì§€ë“¤ ì‹œë„
                        for extra_page in range(page + 1, page + 5):
                            extra_articles = self.crawl_page(extra_page)
                            if extra_articles:
                                for article in extra_articles:
                                    if len(self.articles) >= self.max_articles:
                                        break
                                    if article["url"] not in self.seen_urls:
                                        self.articles.append(article)
                                        self.seen_urls.add(article["url"])
                                        pbar.update(1)
                            time.sleep(self.adaptive_delay)
                    
                    break
                
                # ìƒˆ ê¸°ì‚¬ ì¶”ê°€
                new_count = 0
                for article in page_articles:
                    if len(self.articles) >= self.max_articles:
                        break
                    
                    self.articles.append(article)
                    new_count += 1
                    pbar.update(1)
                
                print(f"âœ… í˜ì´ì§€ {page}: {new_count}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {len(self.articles)}ê°œ)")
                
                # 100ê°œ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                if len(self.articles) >= self.max_articles:
                    print(f"ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë‹¬ì„±! í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                
                # ì„±ê³µì ì¸ í¬ë¡¤ë§ìœ¼ë¡œ ë”œë ˆì´ ì¡°ì •
                self.adjust_delay(True)
                
                page += 1
                time.sleep(self.adaptive_delay)
        
        # ê²°ê³¼ ì €ì¥
        self.save_to_supabase()
        
        # ì„±ëŠ¥ ë¶„ì„
        end_time = time.time()
        duration = end_time - start_time
        
        self.display_results(duration)
        return self.articles
    
    def adjust_delay(self, success: bool):
        """ì ì‘í˜• ë”œë ˆì´ ì¡°ì •"""
        if success:
            # ì„±ê³µ ì‹œ ë”œë ˆì´ ê°ì†Œ
            self.adaptive_delay = max(self.adaptive_delay * 0.9, self.min_delay)
        else:
            # ì‹¤íŒ¨ ì‹œ ë”œë ˆì´ ì¦ê°€
            self.adaptive_delay = min(self.adaptive_delay * 1.2, self.max_delay)
    
    def save_to_supabase(self):
        """ê²°ê³¼ë¥¼ Supabaseì— ì €ì¥"""
        try:
            if not self.supabase_manager.is_connected():
                print("âŒ Supabaseì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            print(f"ğŸ’¾ Supabaseì— {len(self.articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
            
            # media_outlet ìƒì„± ë˜ëŠ” ì¡°íšŒ
            media_outlet = self.supabase_manager.get_media_outlet(self.media_name)
            if media_outlet:
                media_id = media_outlet['id']
                print(f"âœ… ê¸°ì¡´ ì–¸ë¡ ì‚¬ ì‚¬ìš©: {self.media_name} (ID: {media_id})")
            else:
                media_id = self.supabase_manager.create_media_outlet(self.media_name, self.media_bias)
                if media_id:
                    print(f"âœ… ìƒˆ ì–¸ë¡ ì‚¬ ìƒì„±: {self.media_name} (ID: {media_id})")
                else:
                    print(f"âŒ ì–¸ë¡ ì‚¬ ìƒì„± ì‹¤íŒ¨: {self.media_name}")
                    return
            
            # ê¸°ì‚¬ë“¤ì„ Supabaseì— ì €ì¥
            successful_saves = 0
            failed_saves = 0
            
            for article in self.articles:
                try:
                    # Supabase articles í…Œì´ë¸” í˜•ì‹ì— ë§ê²Œ ë°ì´í„° êµ¬ì„±
                    article_data = {
                        'title': article['title'],
                        'url': article['url'],
                        'content': article['content'],
                        'published_at': article['published_at'],
                        'media_id': media_id,
                        'bias': self.media_bias,  # media_outlets í…Œì´ë¸”ì˜ bias ì°¸ì¡°
                        'issue_id': 1  # ì„ì‹œë¡œ ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                    }
                    
                    # Supabaseì— ì €ì¥
                    article_id = self.supabase_manager.insert_article(article_data)
                    if article_id:
                        successful_saves += 1
                    else:
                        failed_saves += 1
                        
                except Exception as e:
                    failed_saves += 1
                    print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article['title']} - {str(e)}")
            
            print(f"\nğŸ“Š Supabase ì €ì¥ ê²°ê³¼:")
            print(f"  â€¢ ì„±ê³µ: {successful_saves}ê°œ")
            print(f"  â€¢ ì‹¤íŒ¨: {failed_saves}ê°œ")
            print(f"  â€¢ ì´ ê¸°ì‚¬: {len(self.articles)}ê°œ")
            
            if successful_saves > 0:
                print(f"âœ… {successful_saves}ê°œ ê¸°ì‚¬ê°€ Supabaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}", exc_info=True)
    
    def display_results(self, duration: float):
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        print(f"\n{'='*60}")
        print(f"      í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!      ")
        print(f"{'='*60}")
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(self.articles)}ê°œ")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"  â€¢ í‰ê·  ì†ë„: {len(self.articles)/duration:.1f} ê¸°ì‚¬/ì´ˆ")
        print(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.network_errors}íšŒ")
        print(f"  â€¢ íŒŒì‹± ì˜¤ë¥˜: {self.parsing_errors}íšŒ")
        
        if duration > 25:
            print(f"âš ï¸  ëª©í‘œ ì‹œê°„(25ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤: {duration:.1f}ì´ˆ")
        else:
            print(f"âœ… ëª©í‘œ ì‹œê°„ ë‚´ ì™„ë£Œ: {duration:.1f}ì´ˆ")
        
        if len(self.articles) >= self.max_articles:
            print(f"âœ… ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë‹¬ì„±!")
        else:
            print(f"âš ï¸  ëª©í‘œ ê¸°ì‚¬ ìˆ˜({self.max_articles}ê°œ) ë¯¸ë‹¬ì„±: {len(self.articles)}ê°œ")
    
    def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        try:
            return self.crawl_hankyung()
        except KeyboardInterrupt:
            print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return self.articles
        except Exception as e:
            print(f"\nâŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            logger.error(f"í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)
            return self.articles

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        crawler = HankyungPoliticsCrawler(max_articles=100)
        articles = crawler.run()
        
        print(f"\nğŸ‰ í•œêµ­ê²½ì œ ì •ì¹˜ ì „ì²´ í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ’¾ Supabaseì— ì €ì¥ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"í¬ë¡¤ëŸ¬ ì˜¤ë¥˜: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()
