#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SBS ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬ (ë¹„ë™ê¸° ìµœì í™” ë²„ì „)

### ì¡°ê±´:
1. **ë¹„ë™ê¸° ë°©ì‹ (asyncio + httpx + BeautifulSoup) ì‚¬ìš©**
   - ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ë¹ ë¥´ê²Œ ìˆ˜ì§‘í•´ì•¼ í•¨
   - ì „ì²´ ì‹¤í–‰ ì‹œê°„ì€ 20ì´ˆ ì´ë‚´ ëª©í‘œ

2. **HTML êµ¬ì¡°**
   - ê¸°ì‚¬ ëª©ë¡ í˜ì´ì§€ URL: https://news.sbs.co.kr/news/newsSection.do?pageIdx=1&sectionType=01&pageDate=20250822
   - ê¸°ì‚¬ ì œëª©: `strong.tit_line`
   - ê¸°ì‚¬ ë§í¬: `a[href*='news/endPage.do']`
   - ê¸°ì‚¬ ë‚ ì§œ: `span.date`
   - ê¸°ì‚¬ ë³¸ë¬¸: `div.text_area`

3. **íƒ€ì„ì•„ì›ƒ**
   - ìš”ì²­ timeoutì€ 5ì´ˆ ì´ë‚´ë¡œ ì„¤ì •
   - ì‹¤íŒ¨í•œ ìš”ì²­ì€ ê±´ë„ˆë›°ê³  ë¡œê·¸ ì¶œë ¥

4. **ì¤‘ë³µ ì œê±°**
   - ê°™ì€ ê¸°ì‚¬ ë§í¬ëŠ” í•œ ë²ˆë§Œ ì €ì¥

5. **ì¶œë ¥ í˜•íƒœ**
   - JSON ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥: `[{"title": "...", "url": "...", "date": "...", "content": "..."}]`

6. **ì„±ëŠ¥ ìµœì í™”**
   - ê¸°ì‚¬ 50ê°œ ìˆ˜ì§‘ ì‹œ 20ì´ˆ ì´ë‚´ ì™„ë£Œ
   - í•„ìš”ì‹œ `asyncio.gather` í™œìš©
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import asyncio
import httpx
import json
import re
from datetime import datetime
from typing import List, Dict, Optional, Set
import logging
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from utils.legacy.supabase_manager_v2 import SupabaseManagerV2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SBSPoliticsCrawler:
    def __init__(self):
        self.articles = []
        self.seen_urls: Set[str] = set()
        self.collected_count = 0
        self.target_count = 50
        
        # Supabase ì—°ê²°
        self.supabase_manager = SupabaseManagerV2()
        
        # SBS ì„¤ì •
        self.base_url = "https://news.sbs.co.kr"
        self.section_url = "https://news.sbs.co.kr/news/newsSection.do"
        self.section_type = "01"  # ì •ì¹˜ ì„¹ì…˜
        self.media_name = "SBS"
        self.media_id = 14  # SBS media_id ê³ ì •ê°’
        
        # í¬ë¡¤ë§ ì„¤ì •
        self.max_pages = 5  # ìµœëŒ€ 5í˜ì´ì§€ê¹Œì§€ë§Œ íƒìƒ‰
        self.articles_per_page = 10  # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜
        self.timeout = 5.0  # 5ì´ˆ íƒ€ì„ì•„ì›ƒ
        self.max_concurrent_requests = 10  # ë™ì‹œ ìš”ì²­ ìˆ˜
        
        # ì˜¤ë¥˜ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        self.content_errors = 0
        
    def clean_content(self, text: str) -> str:
        """ë³¸ë¬¸ ë‚´ìš© ì •ì œ"""
        if not text:
            return ""
        
        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # íŠ¹ì • ë¬¸êµ¬ ì œê±°
        remove_patterns = [
            r'Copyright\s*â’¸\s*SBS',
            r'Copyright\s*Â©\s*SBS',
            r'â“’\s*SBS',
            r'ì €ì‘ê¶Œì\s*Â©\s*SBS',
            r'ë¬´ë‹¨\s*ì „ì¬\s*ë°\s*ì¬ë°°í¬\s*ê¸ˆì§€',
            r'ê¸°ì\s*ì´ë©”ì¼',
            r'ê¸°ì\s*ì—°ë½ì²˜',
            r'ê¸°ì\s*ì¹´ë“œ',
            r'ëŒ“ê¸€',
            r'ê´‘ê³ ',
            r'ë°°ë„ˆ',
            r'SNS',
            r'ê³µìœ í•˜ê¸°',
            r'ì¶”ì²œí•˜ê¸°'
        ]
        
        for pattern in remove_patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)
        
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        text = re.sub(r'\n+', '\n', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def parse_date(self, date_text: str) -> str:
        """ë‚ ì§œ í…ìŠ¤íŠ¸ë¥¼ ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        try:
            # SBS ë‚ ì§œ í˜•ì‹: "2025.08.22" ë˜ëŠ” "08.22"
            if date_text:
                # í˜„ì¬ ì—°ë„ ì¶”ê°€
                if len(date_text.split('.')) == 2:
                    current_year = datetime.now().year
                    date_text = f"{current_year}.{date_text}"
                
                # ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜
                dt = datetime.strptime(date_text, "%Y.%m.%d")
                return dt.isoformat()
        except:
            pass
        
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        return datetime.now().isoformat()
    
    async def fetch_page(self, client: httpx.AsyncClient, page_idx: int) -> Optional[str]:
        """íŠ¹ì • í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ì˜¤ëŠ˜ ë‚ ì§œ
            today = datetime.now().strftime("%Y%m%d")
            
            # í˜ì´ì§€ URL êµ¬ì„±
            url = f"{self.section_url}?pageIdx={page_idx}&sectionType={self.section_type}&pageDate={today}"
            
            # í˜ì´ì§€ ìš”ì²­
            response = await client.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            return response.text
            
        except Exception as e:
            logger.warning(f"í˜ì´ì§€ {page_idx} ìš”ì²­ ì‹¤íŒ¨: {e}")
            self.network_errors += 1
            return None
    
    def parse_article_links(self, html: str, page_idx: int) -> List[Dict]:
        """HTMLì—ì„œ ê¸°ì‚¬ ë§í¬ íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # ê¸°ì‚¬ ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
            articles = []
            
            articles = []
            
            # 1. w_r í´ë˜ìŠ¤ì˜ ì£¼ìš”ë‰´ìŠ¤ 4ê°œ ìˆ˜ì§‘
            main_news = soup.select('div.w_r ul.news li')
            logger.info(f"  - ì£¼ìš”ë‰´ìŠ¤ (w_r): {len(main_news)}ê°œ")
            
            for news_item in main_news:
                # ë§í¬ ì¶”ì¶œ: a íƒœê·¸ì˜ href
                link_elem = news_item.select_one('a[href*="news/endPage.do"]')
                if not link_elem:
                    continue
                    
                url = link_elem.get('href', '')
                if not url:
                    continue
                
                # ì œëª© ì¶”ì¶œ: a íƒœê·¸ì˜ í…ìŠ¤íŠ¸
                title = link_elem.get_text(strip=True)
                if not title:
                    continue
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url.startswith('/'):
                    url = f"https://news.sbs.co.kr{url}"
                elif not url.startswith('http'):
                    url = f"https://news.sbs.co.kr/{url}"
                
                # ì¤‘ë³µ ì²´í¬
                if url in self.seen_urls:
                    continue
                
                articles.append({
                    'title': title,
                    'url': url,
                    'date': '',  # ì£¼ìš”ë‰´ìŠ¤ëŠ” ë‚ ì§œ ì •ë³´ê°€ ì—†ìŒ
                    'page_idx': page_idx
                })
                logger.info(f"    ì£¼ìš”ë‰´ìŠ¤ ì¶”ê°€: {title[:30]}...")
            
            # 2. w_inner í´ë˜ìŠ¤ì˜ ì¼ë°˜ ê¸°ì‚¬ ìˆ˜ì§‘
            list_news = soup.select('div.w_inner a.news[href*="news/endPage.do"]')
            logger.info(f"  - ì¼ë°˜ ê¸°ì‚¬ (w_inner): {len(list_news)}ê°œ")
            
            for news_item in list_news:
                url = news_item.get('href', '')
                if not url:
                    continue
                
                # ì œëª© ì¶”ì¶œ: ê°™ì€ ë¶€ëª¨ ìš”ì†Œì—ì„œ strong.sub ì°¾ê¸°
                title_elem = news_item.select_one('strong.sub')
                if not title_elem:
                    continue
                    
                title = title_elem.get_text(strip=True)
                if not title:
                    continue
                
                # ë‚ ì§œ ì¶”ì¶œ: ê°™ì€ ë¶€ëª¨ ìš”ì†Œì—ì„œ span.date ì°¾ê¸°
                date_elem = news_item.select_one('span.date')
                date_text = date_elem.get_text(strip=True) if date_elem else ''
                
                # ì ˆëŒ€ URLë¡œ ë³€í™˜
                if url.startswith('/'):
                    url = f"https://news.sbs.co.kr{url}"
                elif not url.startswith('http'):
                    url = f"https://news.sbs.co.kr/{url}"
                
                # ì¤‘ë³µ ì²´í¬
                if url in self.seen_urls:
                    continue
                
                articles.append({
                    'title': title,
                    'url': url,
                    'date': date_text,
                    'page_idx': page_idx
                })
            
            return articles
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì‹¤íŒ¨ (í˜ì´ì§€ {page_idx}): {e}")
            self.parsing_errors += 1
            return []
    
    async def fetch_article_content(self, client: httpx.AsyncClient, article_info: Dict) -> Optional[Dict]:
        """ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­
            response = await client.get(article_info['url'], timeout=self.timeout)
            response.raise_for_status()
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°: div.text_area
            content_area = soup.select_one('div.text_area')
            
            if not content_area:
                logger.warning(f"ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {article_info['title'][:30]}...")
                self.content_errors += 1
                return None
            
            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ
            content_html = str(content_area)
            clean_content = self.clean_content(content_html)
            
            if not clean_content or len(clean_content.strip()) < 50:
                logger.warning(f"ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {article_info['title'][:30]}...")
                self.content_errors += 1
                return None
            
            # ë‚ ì§œ íŒŒì‹±
            published_at = self.parse_date(article_info['date'])
            
            # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            article_data = {
                'title': article_info['title'],
                'url': article_info['url'],
                'date': article_info['date'],
                'content': clean_content,
                'published_at': published_at,
                'page_idx': article_info['page_idx']
            }
            
            return article_data
            
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {article_info['title'][:30]}... - {e}")
            self.content_errors += 1
            return None
    
    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)"""
        logger.info(f"ğŸš€ SBS ì •ì¹˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ğŸ¯ ëª©í‘œ: {self.target_count}ê°œ ê¸°ì‚¬")
        logger.info(f"ğŸ“° ìµœëŒ€ í˜ì´ì§€: {self.max_pages}í˜ì´ì§€")
        logger.info(f"âš¡ ë™ì‹œ ìš”ì²­ ìˆ˜: {self.max_concurrent_requests}")
        
        start_time = datetime.now()
        
        try:
            # httpx í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
            limits = httpx.Limits(max_connections=self.max_concurrent_requests)
            timeout = httpx.Timeout(self.timeout)
            
            async with httpx.AsyncClient(limits=limits, timeout=timeout) as client:
                
                # 1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
                logger.info("ğŸ“„ 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
                
                # seen_urls ì´ˆê¸°í™” (ìƒˆë¡œìš´ ì‹¤í–‰ë§ˆë‹¤)
                self.seen_urls.clear()
                
                page_tasks = []
                for page_idx in range(1, self.max_pages + 1):
                    task = self.fetch_page(client, page_idx)
                    page_tasks.append(task)
                
                # ë³‘ë ¬ë¡œ í˜ì´ì§€ ìš”ì²­
                page_results = await asyncio.gather(*page_tasks, return_exceptions=True)
                
                # ê¸°ì‚¬ ë§í¬ íŒŒì‹±
                all_article_links = []
                for i, result in enumerate(page_results, 1):
                    if isinstance(result, Exception):
                        logger.warning(f"í˜ì´ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                        continue
                    
                    if result:
                        article_links = self.parse_article_links(result, i)
                        all_article_links.extend(article_links)
                        logger.info(f"í˜ì´ì§€ {i}: {len(article_links)}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
                
                logger.info(f"âœ… ì´ {len(all_article_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ ë³‘ë ¬ ìˆ˜ì§‘
                logger.info("ğŸ“° 2ë‹¨ê³„: ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ ì¤‘...")
                
                # ì¤‘ë³µ ì œê±°
                unique_articles = []
                for article in all_article_links:
                    if article['url'] not in self.seen_urls:
                        unique_articles.append(article)
                        self.seen_urls.add(article['url'])
                
                logger.info(f"ì¤‘ë³µ ì œê±° í›„: {len(unique_articles)}ê°œ ê¸°ì‚¬")
                
                # ëª©í‘œ ê°œìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
                target_articles = unique_articles[:self.target_count]
                
                # ê¸°ì‚¬ ë³¸ë¬¸ ë³‘ë ¬ ìˆ˜ì§‘
                content_tasks = []
                for article in target_articles:
                    task = self.fetch_article_content(client, article)
                    content_tasks.append(task)
                
                # ë³‘ë ¬ë¡œ ë³¸ë¬¸ ìš”ì²­
                content_results = await asyncio.gather(*content_tasks, return_exceptions=True)
                
                # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
                for i, result in enumerate(content_results):
                    if isinstance(result, Exception):
                        logger.warning(f"ê¸°ì‚¬ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                        continue
                    
                    if result:
                        self.articles.append(result)
                        self.collected_count += 1
                        
                        if self.collected_count % 10 == 0:
                            logger.info(f"âœ… {self.collected_count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
                # ìˆ˜ì§‘ ì™„ë£Œ ì‹œê°„
                end_time = datetime.now()
                duration = (end_time - start_time).total_seconds()
                
                logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                logger.info(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼: {self.collected_count}ê°œ ê¸°ì‚¬")
                logger.info(f"â±ï¸ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
                logger.info(f"ğŸš€ í‰ê·  ì†ë„: {self.collected_count / duration:.1f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "ğŸš€ í‰ê·  ì†ë„: 0.0 ê¸°ì‚¬/ì´ˆ")
                
                return self.articles
                
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self.articles
    
    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0, "total": 0}
        
        success_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # Supabase í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article['content'],
                    'published_at': article['published_at'],
                    'media_id': self.media_id,
                    'bias': 'Center',  # SBSëŠ” ì¤‘ë„
                    'issue_id': 1  # ê¸°ë³¸ê°’
                }
                
                # ê¸°ì‚¬ ì €ì¥
                result = self.supabase_manager.insert_article(article_data)
                if result:
                    success_count += 1
                    logger.info(f"ìƒˆ ê¸°ì‚¬ ì €ì¥: {article['title'][:50]}...")
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {e}")
                failed_count += 1
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(articles)
        }
    
    def save_to_json(self, articles: List[Dict], filename: str = "sbs_articles.json"):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(articles, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {filename}ì— ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def display_results(self, articles: List[Dict], save_results: Dict[str, int], duration: float):
        """ìˆ˜ì§‘ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("      SBS ì •ì¹˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!      ")
        print("="*60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"  â€¢ í‰ê·  ì†ë„: {len(articles) / duration:.1f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "  â€¢ í‰ê·  ì†ë„: 0.0 ê¸°ì‚¬/ì´ˆ")
        print(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.network_errors}íšŒ")
        print(f"  â€¢ íŒŒì‹± ì˜¤ë¥˜: {self.parsing_errors}íšŒ")
        print(f"  â€¢ ë³¸ë¬¸ ì˜¤ë¥˜: {self.content_errors}íšŒ")
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        if len(articles) >= self.target_count:
            print(f"  â€¢ ğŸ¯ ëª©í‘œ ë‹¬ì„±: {self.target_count}ê°œ ì´ìƒ")
        else:
            print(f"  â€¢ âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„±: {len(articles)}ê°œ (ëª©í‘œ: {self.target_count}ê°œ)")
        
        # ì €ì¥ ê²°ê³¼
        print(f"\nğŸ’¾ Supabase ì €ì¥ ê²°ê³¼:")
        print(f"  â€¢ ì„±ê³µ: {save_results['success']}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {save_results['failed']}ê°œ")
        print(f"  â€¢ ì´ ê¸°ì‚¬: {save_results['total']}ê°œ")
        
        if save_results['success'] > 0:
            print(f"âœ… {save_results['success']}ê°œ ê¸°ì‚¬ê°€ Supabaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© ì¼ë¶€ ì¶œë ¥
        if articles:
            print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© (ì²˜ìŒ 10ê°œ):")
            for i, article in enumerate(articles[:10], 1):
                title = article['title']
                title_preview = title[:60] + "..." if len(title) > 60 else title
                print(f"  {i}. {title_preview}")
            
            if len(articles) > 10:
                print(f"  ... ì™¸ {len(articles) - 10}ê°œ ê¸°ì‚¬")
        
        print("\nğŸ‰ SBS ì •ì¹˜ ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")
        print("ğŸ’¾ Supabaseì— ì €ì¥ ì™„ë£Œ!")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        crawler = SBSPoliticsCrawler()
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = datetime.now()
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        articles = await crawler.collect_all_articles()
        
        # ìˆ˜ì§‘ ì™„ë£Œ ì‹œê°„
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Supabase ì €ì¥
        save_results = await crawler.save_to_supabase(articles)
        
        # JSON íŒŒì¼ë¡œë„ ì €ì¥
        crawler.save_to_json(articles)
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.display_results(articles, save_results, duration)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())
