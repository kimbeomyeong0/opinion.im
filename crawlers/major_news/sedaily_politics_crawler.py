#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì„œìš¸ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- 20ì´ˆ ë‚´ì™¸ ë¹ ë¥¸ í¬ë¡¤ë§
- 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ëª©í‘œ
- ì¤‘ë³µ ì œê±°
- ì •ì¹˜ ì„¹ì…˜ ì „ìš© ìˆ˜ì§‘
- aiohttp + BeautifulSoup ê¸°ë°˜ ë¹ ë¥¸ í¬ë¡¤ë§
"""
import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich import box
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))
from legacy.supabase_manager_v2 import SupabaseManagerV2

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SedailyPoliticsCrawler:
    def __init__(self, max_articles: int = 100, debug: bool = False):
        self.base_url = "https://www.sedaily.com"
        self.politics_url = "https://www.sedaily.com/v/NewsMain/GE"
        self.max_articles = max_articles
        self.console = Console()
        self.delay = 0.02  # ë§¤ìš° ë¹ ë¥¸ í¬ë¡¤ë§ì„ ìœ„í•´ ë”œë ˆì´ ìµœì†Œí™”
        self.debug = debug
        
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.supabase_manager = SupabaseManagerV2()
            self.console.print("[green]Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ[/green]")
        except Exception as e:
            self.console.print(f"[red]Supabase ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}[/red]")
            raise
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            },
            timeout=aiohttp.ClientTimeout(total=10)  # ë¹ ë¥¸ íƒ€ì„ì•„ì›ƒ
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
    
    async def collect_article_links(self) -> List[str]:
        """ì„œìš¸ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        self.console.print("ğŸ” ì„œìš¸ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = []
        page = 1
        max_pages = 50  # ì¶©ë¶„í•œ í˜ì´ì§€ ìˆ˜
        
        while len(all_links) < self.max_articles and page <= max_pages:
            try:
                # í˜ì´ì§€ë³„ URL êµ¬ì„±
                if page == 1:
                    url = self.politics_url
                else:
                    url = f"https://www.sedaily.com/NewsMain/GE/{page}"
                
                async with self.session.get(url) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # 1. ê³ ì • ê¸°ì‚¬ë“¤ ìˆ˜ì§‘ (sub_lv1, sub_news)
                        fixed_articles = soup.select('.sub_lv1 .article_tit a, .sub_news .article_tit a')
                        for article in fixed_articles:
                            href = article.get('href')
                            if href:
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                else:
                                    full_url = href
                                
                                if full_url not in all_links:
                                    all_links.append(full_url)
                        
                        # 2. ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (sub_news_list)
                        article_list = soup.select('.sub_news_list li .article_tit a')
                        page_count = 0
                        
                        for article in article_list:
                            href = article.get('href')
                            if href:
                                if href.startswith('/'):
                                    full_url = urljoin(self.base_url, href)
                                else:
                                    full_url = href
                                
                                if full_url not in all_links:
                                    all_links.append(full_url)
                                    page_count += 1
                        
                        self.console.print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(fixed_articles)}ê°œ ê³ ì • + {page_count}ê°œ ë¦¬ìŠ¤íŠ¸ (ì´ {len(all_links)}ê°œ)")
                        
                        if len(all_links) >= self.max_articles:
                            break
                        
                        # ë” ì´ìƒ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                        if page_count == 0 and page > 1:
                            break
                        
                        page += 1
                        await asyncio.sleep(self.delay)
                        
                    else:
                        self.console.print(f"[red]í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {response.status}[/red]")
                        break
                        
            except Exception as e:
                self.console.print(f"[red]í˜ì´ì§€ {page} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}[/red]")
                break
        
        # ì¤‘ë³µ ì œê±°
        unique_links = list(set(all_links))
        self.console.print(f"âœ… ì´ {len(unique_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return unique_links[:self.max_articles]
    
    async def extract_article_content(self, url: str) -> Optional[Dict]:
        """ê¸°ì‚¬ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ì œëª© ì¶”ì¶œ: ìƒì„¸ í˜ì´ì§€ì—ì„œëŠ” title íƒœê·¸ë‚˜ meta íƒœê·¸ ì‚¬ìš©
                    title = None
                    
                    # 1ì°¨: meta property="og:title" ì—ì„œ ì œëª© ì¶”ì¶œ
                    meta_title = soup.select_one('meta[property="og:title"]')
                    if meta_title:
                        title = meta_title.get('content', '').strip()
                        # '| ì„œìš¸ê²½ì œ' ì œê±°
                        if title.endswith(' | ì„œìš¸ê²½ì œ'):
                            title = title[:-6].strip()
                    
                    # 2ì°¨: title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
                    if not title:
                        title_elem = soup.select_one('title')
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            # '| ì„œìš¸ê²½ì œ' ì œê±°
                            if title.endswith(' | ì„œìš¸ê²½ì œ'):
                                title = title[:-6].strip()
                    
                    if not title:
                        if self.debug:
                            self.console.print(f"[yellow]ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ - HTML êµ¬ì¡° í™•ì¸: {url}[/yellow]")
                            # HTML êµ¬ì¡° ì¼ë¶€ ì¶œë ¥
                            html_preview = html[:1000] if len(html) > 1000 else html
                            self.console.print(f"[dim]HTML ë¯¸ë¦¬ë³´ê¸°: {html_preview}...[/dim]")
                        return None
                    
                    # ë°œí–‰ì¼ ì¶”ì¶œ: url_txt í´ë˜ìŠ¤ em íƒœê·¸ ë’¤ì˜ span íƒœê·¸
                    published_at = None
                    url_txt_elem = soup.select_one('.url_txt')
                    if url_txt_elem:
                        # em íƒœê·¸ ë’¤ì˜ span íƒœê·¸ ì°¾ê¸°
                        em_elem = url_txt_elem.find('em')
                        if em_elem:
                            span_elem = em_elem.find_next_sibling('span')
                            if span_elem:
                                date_text = span_elem.get_text(strip=True)
                                # ë‚ ì§œ íŒŒì‹± ì‹œë„
                                try:
                                    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                                    if re.match(r'\d{4}-\d{2}-\d{2}', date_text):
                                        published_at = datetime.strptime(date_text, '%Y-%m-%d')
                                    elif re.match(r'\d{2}-\d{2}', date_text):
                                        # í˜„ì¬ ì—°ë„ ì¶”ê°€
                                        current_year = datetime.now().year
                                        date_text = f"{current_year}-{date_text}"
                                        published_at = datetime.strptime(date_text, '%Y-%m-%d')
                                    else:
                                        published_at = datetime.now()
                                except:
                                    published_at = datetime.now()
                    
                    if not published_at:
                        published_at = datetime.now()
                    
                    # ë³¸ë¬¸ ì¶”ì¶œ: ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
                    content_elems = None
                    
                    # 1ì°¨: article_view í´ë˜ìŠ¤ (ê¸°ë³¸)
                    content_elems = soup.select('.article_view')
                    
                    # 2ì°¨: ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤
                    if not content_elems:
                        content_elems = soup.select('.article_content') or soup.select('.content') or soup.select('.news_content')
                    
                    # 3ì°¨: ë” ê´‘ë²”ìœ„í•œ ë³¸ë¬¸ ì„ íƒìë“¤
                    if not content_elems:
                        content_elems = soup.select('.article_body') or soup.select('.news_body') or soup.select('.text')
                    
                    # 4ì°¨: ëª¨ë“  p íƒœê·¸ ì¤‘ ë³¸ë¬¸ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤
                    if not content_elems:
                        all_p_tags = soup.select('p')
                        content_elems = []
                        for p in all_p_tags:
                            text = p.get_text(strip=True)
                            # ë³¸ë¬¸ìœ¼ë¡œ ë³´ì´ëŠ” p íƒœê·¸ í•„í„°ë§ (ê¸¸ì´, ë‚´ìš© ë“±)
                            if len(text) > 20 and not text.startswith('[') and not text.startswith('Â©'):
                                content_elems.append(p)
                    
                    if not content_elems:
                        if self.debug:
                            self.console.print(f"[yellow]ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - HTML êµ¬ì¡° í™•ì¸: {url}[/yellow]")
                            # HTML êµ¬ì¡° ì¼ë¶€ ì¶œë ¥
                            html_preview = html[:1000] if len(html) > 1000 else html
                            self.console.print(f"[dim]HTML ë¯¸ë¦¬ë³´ê¸°: {html_preview}...[/dim]")
                        return None
                    
                    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ë¦¬
                    content_parts = []
                    for elem in content_elems:
                        # <br> íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¦¬
                        text = elem.get_text(separator='\n', strip=True)
                        lines = [line.strip() for line in text.split('\n') if line.strip()]
                        content_parts.extend(lines)
                    
                    content = '\n\n'.join(content_parts)
                    
                    if not content.strip():
                        if self.debug:
                            self.console.print(f"[yellow]ë³¸ë¬¸ ë‚´ìš©ì´ ë¹„ì–´ìˆìŒ: {url}[/yellow]")
                        return None
                    
                    return {
                        'title': title,
                        'content': content,
                        'url': url,
                        'published_at': published_at
                    }
                    
        except Exception as e:
            if self.debug:
                self.console.print(f"[red]ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {str(e)}[/red]")
            return None
    
    async def save_to_database(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            # ì„œìš¸ê²½ì œ ì–¸ë¡ ì‚¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            media_outlet = self.supabase_manager.get_media_outlet("ì„œìš¸ê²½ì œ")
            if not media_outlet:
                # ì„œìš¸ê²½ì œê°€ ì—†ìœ¼ë©´ ìƒì„± (ë³´ìˆ˜ ì„±í–¥)
                media_id = self.supabase_manager.create_media_outlet("ì„œìš¸ê²½ì œ", "right")
            else:
                media_id = media_outlet['id']
            
            # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            processed_data = {
                'title': article_data['title'],
                'content': article_data['content'],
                'url': article_data['url'],
                'published_at': article_data['published_at'].isoformat(),
                'media_id': media_id,
                'bias': 'right',  # ì„œìš¸ê²½ì œëŠ” ë³´ìˆ˜ ì„±í–¥
                'issue_id': 6  # ì„ì‹œ issue_id
            }
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            result = self.supabase_manager.insert_article(processed_data)
            if result:
                self.console.print(f"âœ… ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:50]}...")
                return True
            else:
                self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:50]}...[/red]")
                return False
                
        except Exception as e:
            self.console.print(f"[red]ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}[/red]")
            return False
    
    async def run(self):
        """í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        start_time = time.time()
        self.console.print("ğŸš€ ì„œìš¸ê²½ì œ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘!")
        self.console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.collect_article_links()
            
            if not article_links:
                self.console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2. ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ë° ì €ì¥
            self.console.print(f"\nğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
            
            successful = 0
            failed = 0
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                for i, url in enumerate(article_links):
                    try:
                        article_data = await self.extract_article_content(url)
                        if article_data:
                            if await self.save_to_database(article_data):
                                successful += 1
                            else:
                                failed += 1
                        else:
                            failed += 1
                        
                        progress.update(task, advance=1)
                        await asyncio.sleep(self.delay)
                        
                    except Exception as e:
                        self.console.print(f"[red]ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {str(e)}[/red]")
                        failed += 1
                        progress.update(task, advance=1)
            
            # 3. ê²°ê³¼ ì¶œë ¥
            end_time = time.time()
            elapsed_time = end_time - start_time
            
            self.console.print("\n" + "="*50)
            self.console.print("      ì„œìš¸ê²½ì œ í¬ë¡¤ë§ ê²°ê³¼      ")
            self.console.print("="*50)
            
            table = Table(box=box.ROUNDED)
            table.add_column("í•­ëª©", style="cyan")
            table.add_column("ê°’", style="magenta")
            
            table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(article_links)))
            table.add_row("ì„±ê³µ", f"{successful}ê°œ")
            table.add_row("ì‹¤íŒ¨", f"{failed}ê°œ")
            table.add_row("ì„±ê³µë¥ ", f"{successful/len(article_links)*100:.1f}%")
            table.add_row("ì†Œìš” ì‹œê°„", f"{elapsed_time:.2f}ì´ˆ")
            table.add_row("í‰ê·  ì†ë„", f"{len(article_links)/elapsed_time:.2f} ê¸°ì‚¬/ì´ˆ")
            
            self.console.print(table)
            self.console.print("âœ… ì„œìš¸ê²½ì œ í¬ë¡¤ë§ ì™„ë£Œ! ğŸ‰")
            
        except Exception as e:
            self.console.print(f"[red]í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}[/red]")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë””ë²„ê¹… ëª¨ë“œë¡œ ì‹¤í–‰ (ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ì›ì¸ í™•ì¸)
    async with SedailyPoliticsCrawler(debug=True) as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
