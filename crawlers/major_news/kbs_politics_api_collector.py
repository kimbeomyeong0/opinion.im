#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KBS ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ê¸°

### ì¡°ê±´
- ì˜¤ëŠ˜ì ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ (datetimeBegin=ì˜¤ëŠ˜ 00ì‹œ, datetimeEnd=ì˜¤ëŠ˜ 23:59:59)
- ìƒë‹¨ ê³ ì • ê¸°ì‚¬(rowsPerPage=5)ì™€ í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸(rowsPerPage=12, í˜ì´ì§€ë„¤ì´ì…˜ í¬í•¨)ë¥¼ ëª¨ë‘ ìˆ˜ì§‘
- ì •ì¹˜ ì„¹ì…˜ ì½”ë“œ(contentsCode=0003)ë§Œ ê°€ì ¸ì™€
- ì¤‘ë³µì€ ì œê±° (articles.url ê¸°ì¤€, Supabase ì¡°íšŒ í›„ insert)
- ê¸°ì‚¬ ë³¸ë¬¸ì€ newsContents í•„ë“œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ originNewsContentsë¥¼ ì¨
- ë³¸ë¬¸ ì•ˆì˜ <br>ì€ ì¤„ë°”ê¿ˆ(\n)ìœ¼ë¡œ ë°”ê¾¸ê³ , ë‹¤ë¥¸ HTML íƒœê·¸ëŠ” ì œê±°
- Supabase articles í…Œì´ë¸”ì— ì €ì¥

### Supabase ì €ì¥ ê·œì¹™
- title â†’ newsTitle
- url â†’ https://news.kbs.co.kr/news/pc/view/view.do?ncd={newsCode}
- content â†’ newsContents ë˜ëŠ” originNewsContents (ì •ì œ í›„)
- published_at â†’ serviceTime
- media_id â†’ 15
- bias â†’ center

### API ì—”ë“œí¬ì¸íŠ¸
- ìƒë‹¨ ê³ ì • ê¸°ì‚¬: https://news.kbs.co.kr/api/getNewsList?currentPageNo=1&rowsPerPage=5&exceptPhotoYn=Y&contentsExpYn=Y&datetimeBegin=YYYYMMDD000000&datetimeEnd=YYYYMMDD235959&contentsCode=0003
- í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸: https://news.kbs.co.kr/api/getNewsList?currentPageNo={page}&rowsPerPage=12&exceptPhotoYn=Y&datetimeBegin=YYYYMMDD000000&datetimeEnd=YYYYMMDD235959&contentsCode=0003&localCode=00
- ê¸°ì‚¬ ìƒì„¸: https://news.kbs.co.kr/api/getNews?id={newsCode}
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import asyncio
import httpx
import re
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
import logging
from tqdm import tqdm
from utils.supabase_manager_unified import UnifiedSupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KBSPoliticsAPICollector:
    def __init__(self):
        self.articles = []
        self.seen_urls: Set[str] = set()
        
        # Supabase ì—°ê²°
        self.supabase_manager = UnifiedSupabaseManager()
        
        # ì˜¤ëŠ˜ ë‚ ì§œ ì„¤ì •
        self.today = datetime.now()
        self.date_str = self.today.strftime("%Y%m%d")
        self.datetime_begin = f"{self.date_str}000000"
        self.datetime_end = f"{self.date_str}235959"
        
        # API ì„¤ì •
        self.contents_code = "0003"  # ì •ì¹˜ ì„¹ì…˜
        self.media_id = 15  # KBS media_id
        self.media_bias = "center"
        
        # API ì—”ë“œí¬ì¸íŠ¸
        self.fixed_news_url = "https://news.kbs.co.kr/api/getNewsList"
        self.list_news_url = "https://news.kbs.co.kr/api/getNewsList"
        self.detail_news_url = "https://news.kbs.co.kr/api/getNews"
        
        # ì„±ëŠ¥ ìµœì í™”
        self.max_concurrent_requests = 5
        self.timeout = httpx.Timeout(30.0)
        
        # ì˜¤ë¥˜ ì¹´ìš´í„°
        self.network_errors = 0
        self.parsing_errors = 0
        
    def clean_html_content(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° <br>ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜"""
        if not text:
            return ""
        
        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
        text = re.sub(r'<br\s*/?>', '\n', text, flags=re.IGNORECASE)
        
        # ë‹¤ë¥¸ HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        text = re.sub(r'\n+', '\n', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        return text
    
    def build_article_url(self, news_id: str) -> str:
        """ê¸°ì‚¬ URL ìƒì„±"""
        return f"https://news.kbs.co.kr/news/pc/view/view.do?ncd={news_id}"
    
    def convert_service_time_to_iso(self, service_time: str) -> str:
        """KBS serviceTimeì„ ISO í¬ë§·ìœ¼ë¡œ ë³€í™˜"""
        try:
            # serviceTime í˜•ì‹: "2025-08-22 14:30:00"
            dt = datetime.strptime(service_time, "%Y-%m-%d %H:%M:%S")
            return dt.isoformat()
        except Exception as e:
            logger.warning(f"ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨: {service_time}, ì˜¤ë¥˜: {e}")
            # í˜„ì¬ ì‹œê°„ìœ¼ë¡œ ëŒ€ì²´
            return datetime.now().isoformat()
    
    async def fetch_fixed_news(self, client: httpx.AsyncClient) -> List[Dict]:
        """ìƒë‹¨ ê³ ì • ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
        params = {
            "currentPageNo": 1,
            "rowsPerPage": 5,
            "exceptPhotoYn": "Y",
            "contentsExpYn": "Y",
            "datetimeBegin": self.datetime_begin,
            "datetimeEnd": self.datetime_end,
            "contentsCode": self.contents_code
        }
        
        try:
            response = await client.get(self.fixed_news_url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            logger.info(f"ìƒë‹¨ ê³ ì • ê¸°ì‚¬ API ì‘ë‹µ: {data.get('success', False)}")
            
            if data.get('success') and 'data' in data:
                return data['data']
            else:
                logger.warning("ìƒë‹¨ ê³ ì • ê¸°ì‚¬ API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            logger.error(f"ìƒë‹¨ ê³ ì • ê¸°ì‚¬ API ìš”ì²­ ì‹¤íŒ¨: {e}")
            self.network_errors += 1
            return []
    
    async def fetch_list_news_page(self, client: httpx.AsyncClient, page: int) -> List[Dict]:
        """í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë³„)"""
        params = {
            "currentPageNo": page,
            "rowsPerPage": 12,
            "exceptPhotoYn": "Y",
            "datetimeBegin": self.datetime_begin,
            "datetimeEnd": self.datetime_end,
            "contentsCode": self.contents_code,
            "localCode": "00"
        }
        
        try:
            response = await client.get(self.list_news_url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            logger.info(f"í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} API ì‘ë‹µ: {data.get('success', False)}")
            
            if data.get('success') and 'data' in data:
                return data['data']
            else:
                logger.warning(f"í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
                
        except Exception as e:
            logger.error(f"í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ {page} API ìš”ì²­ ì‹¤íŒ¨: {e}")
            self.network_errors += 1
            return []
    
    async def fetch_news_detail(self, client: httpx.AsyncClient, news_id: str) -> Optional[Dict]:
        """ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        params = {"id": news_id}
        
        try:
            response = await client.get(self.detail_news_url, params=params, timeout=self.timeout)
            response.raise_for_status()
            
            data = response.json()
            if data.get('success') and 'data' in data:
                return data['data']
            else:
                logger.warning(f"ê¸°ì‚¬ ìƒì„¸ API ì‘ë‹µì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {news_id}")
                return None
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìƒì„¸ API ìš”ì²­ ì‹¤íŒ¨ {news_id}: {e}")
            self.network_errors += 1
            return None
    
    def parse_news_item(self, news_item: Dict, detail_data: Optional[Dict] = None) -> Optional[Dict]:
        """ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹±"""
        try:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸
            if not all(key in news_item for key in ["newsTitle", "newsCode", "serviceTime"]):
                logger.warning(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {news_item.get('newsTitle', 'ì œëª© ì—†ìŒ')}")
                return None
            
            news_id = news_item["newsCode"]
            title = news_item["newsTitle"]
            service_time = news_item["serviceTime"]
            
            # ê¸°ì‚¬ URL ìƒì„±
            article_url = self.build_article_url(news_id)
            
            # ì¤‘ë³µ ì²´í¬
            if article_url in self.seen_urls:
                logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ ê±´ë„ˆë›°ê¸°: {title[:30]}...")
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ (ìƒì„¸ API ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ í•„ë“œ)
            content = ""
            if detail_data and "newsContents" in detail_data and detail_data["newsContents"]:
                content = detail_data["newsContents"]
            elif detail_data and "originNewsContents" in detail_data and detail_data["originNewsContents"]:
                content = detail_data["originNewsContents"]
            elif "newsContents" in news_item and news_item["newsContents"]:
                content = news_item["newsContents"]
            elif "originNewsContents" in news_item and news_item["originNewsContents"]:
                content = news_item["originNewsContents"]
            
            # HTML íƒœê·¸ ì •ì œ
            clean_content = self.clean_html_content(content)
            
            # ë‚ ì§œ ë³€í™˜
            published_at = self.convert_service_time_to_iso(service_time)
            
            # ê¸°ì‚¬ ë°ì´í„° êµ¬ì„±
            article_data = {
                "title": title,
                "url": article_url,
                "content": clean_content,
                "published_at": published_at,
                "news_id": news_id,
                "raw_data": news_item
            }
            
            return article_data
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            self.parsing_errors += 1
            return None
    
    async def collect_all_news(self) -> List[Dict]:
        """ëª¨ë“  ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘"""
        logger.info(f"ğŸš€ KBS ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {self.date_str}")
        logger.info(f"â° ì‹œê°„ ë²”ìœ„: {self.datetime_begin} ~ {self.datetime_end}")
        
        all_articles = []
        
        async with httpx.AsyncClient(
            timeout=self.timeout,
            limits=httpx.Limits(max_connections=self.max_concurrent_requests)
        ) as client:
            
            # 1. ìƒë‹¨ ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘
            logger.info("ğŸ“° ìƒë‹¨ ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
            fixed_news = await self.fetch_fixed_news(client)
            
            for news_item in fixed_news:
                # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                detail_data = await self.fetch_news_detail(client, news_item["newsCode"])
                
                # ê¸°ì‚¬ íŒŒì‹±
                article = self.parse_news_item(news_item, detail_data)
                if article:
                    all_articles.append(article)
                    self.seen_urls.add(article["url"])
                    logger.info(f"âœ… ê³ ì • ê¸°ì‚¬ ìˆ˜ì§‘: {article['title'][:50]}...")
                
                # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                await asyncio.sleep(0.2)
            
            logger.info(f"âœ… ìƒë‹¨ ê³ ì • ê¸°ì‚¬: {len(fixed_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2. í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ ê¸°ì‚¬ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜)
            logger.info("ğŸ“° í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...")
            page = 1
            max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ë§Œ ì‹œë„
            
            with tqdm(desc="í•˜ë‹¨ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘", unit="í˜ì´ì§€") as pbar:
                while page <= max_pages:
                    pbar.set_description(f"í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘")
                    
                    # í˜ì´ì§€ë³„ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
                    page_news = await self.fetch_list_news_page(client, page)
                    
                    if not page_news:
                        logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    new_articles_count = 0
                    for news_item in page_news:
                        # ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                        detail_data = await self.fetch_news_detail(client, news_item["newsCode"])
                        
                        # ê¸°ì‚¬ íŒŒì‹±
                        article = self.parse_news_item(news_item, detail_data)
                        if article:
                            all_articles.append(article)
                            self.seen_urls.add(article["url"])
                            new_articles_count += 1
                            logger.info(f"âœ… ë¦¬ìŠ¤íŠ¸ ê¸°ì‚¬ ìˆ˜ì§‘: {article['title'][:50]}...")
                        
                        # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                        await asyncio.sleep(0.2)
                    
                    logger.info(f"âœ… í˜ì´ì§€ {page}: {new_articles_count}ê°œ ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘ (ì´ {len(all_articles)}ê°œ)")
                    
                    if new_articles_count == 0:
                        logger.info("ë” ì´ìƒ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        break
                    
                    page += 1
                    pbar.update(1)
                    
                    # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                    await asyncio.sleep(0.5)
        
        return all_articles
    
    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0, "total": 0}
        
        success_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # Supabase í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
                article_data = {
                    'title': article['title'],
                    'url': article['url'],
                    'content': article['content'],
                    'published_at': article['published_at'],
                    'media_id': self.media_id,
                    'bias': self.media_bias,
                    'issue_id': 1  # ê¸°ë³¸ê°’
                }
                
                # ê¸°ì‚¬ ì €ì¥
                result = self.supabase_manager.insert_article(article_data)
                if result:
                    success_count += 1
                    logger.info(f"ìƒˆ ê¸°ì‚¬ ì €ì¥: {article['title'][:50]}...")
                else:
                    failed_count += 1
                    
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {e}")
                failed_count += 1
        
        return {
            "success": success_count,
            "failed": failed_count,
            "total": len(articles)
        }
    
    def display_results(self, articles: List[Dict], save_results: Dict[str, int], duration: float):
        """ìˆ˜ì§‘ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*60)
        print("      KBS ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ ì™„ë£Œ!      ")
        print("="*60)
        print(f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:")
        print(f"  â€¢ ì´ ê¸°ì‚¬ ìˆ˜: {len(articles)}ê°œ")
        print(f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
        print(f"  â€¢ í‰ê·  ì†ë„: {len(articles) / duration:.1f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "  â€¢ í‰ê·  ì†ë„: 0.0 ê¸°ì‚¬/ì´ˆ")
        print(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {self.network_errors}íšŒ")
        print(f"  â€¢ íŒŒì‹± ì˜¤ë¥˜: {self.parsing_errors}íšŒ")
        
        # ì €ì¥ ê²°ê³¼
        print(f"\nğŸ’¾ Supabase ì €ì¥ ê²°ê³¼:")
        print(f"  â€¢ ì„±ê³µ: {save_results['success']}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {save_results['failed']}ê°œ")
        print(f"  â€¢ ì´ ê¸°ì‚¬: {save_results['total']}ê°œ")
        
        if save_results['success'] > 0:
            print(f"âœ… {save_results['success']}ê°œ ê¸°ì‚¬ê°€ Supabaseì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        
        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© ì¼ë¶€ ì¶œë ¥
        if articles:
            print(f"\nğŸ“° ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì œëª© (ì²˜ìŒ 10ê°œ):")
            for i, article in enumerate(articles[:10], 1):
                title = article['title']
                title_preview = title[:60] + "..." if len(title) > 60 else title
                print(f"  {i}. {title_preview}")
            
            if len(articles) > 10:
                print(f"  ... ì™¸ {len(articles) - 10}ê°œ ê¸°ì‚¬")
        
        print("\nğŸ‰ KBS ì •ì¹˜ ë‰´ìŠ¤ API ìˆ˜ì§‘ ì™„ë£Œ!")
        print("ğŸ’¾ Supabaseì— ì €ì¥ ì™„ë£Œ!")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        collector = KBSPoliticsAPICollector()
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = datetime.now()
        
        # ë‰´ìŠ¤ ìˆ˜ì§‘
        articles = await collector.collect_all_news()
        
        # ìˆ˜ì§‘ ì™„ë£Œ ì‹œê°„
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # Supabase ì €ì¥
        save_results = await collector.save_to_supabase(articles)
        
        # ê²°ê³¼ ì¶œë ¥
        collector.display_results(articles, save_results, duration)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ìˆ˜ì§‘ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    asyncio.run(main())
