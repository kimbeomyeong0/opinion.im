#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
- ìµœì‹  ì •ì¹˜ ê¸°ì‚¬ 100ê°œ ìˆ˜ì§‘
- í˜ì´ì§€ë„¤ì´ì…˜ í™œìš© (?page={page})
- 20ì´ˆ ë‚´ í¬ë¡¤ë§ ì™„ë£Œ ëª©í‘œ
"""

import asyncio
import aiohttp
import time
import sys
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from utils.supabase_manager_unified import UnifiedSupabaseManager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HaniPoliticsCrawler:
    """í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, max_articles: int = 100):
        self.max_articles = max_articles
        self.console = Console()
        self.session: Optional[aiohttp.ClientSession] = None
        self.supabase_manager = UnifiedSupabaseManager()
        
        # í•œê²¨ë ˆ ì„¤ì •
        self.base_url = "https://www.hani.co.kr"
        self.politics_url = "https://www.hani.co.kr/arti/politics"
        
        # í•œê²¨ë ˆëŠ” ì¢ŒíŒŒ ì–¸ë¡ ì‚¬
        self.media_name = "í•œê²¨ë ˆ"
        self.media_bias = "Left"
        
        # í†µê³„
        self.total_articles = 0
        self.successful_articles = 0
        self.failed_articles = 0
        self.start_time: Optional[datetime] = None
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì„¤ì •
        self.page_size = 20  # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜ (ì¶”ì •)
        self.max_pages = 10  # ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def get_politics_article_links(self) -> List[str]:
        """ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘"""
        self.console.print("ğŸ” í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = set()
        
        try:
            # 1. ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
            async with self.session.get(self.politics_url) as response:
                if response.status == 200:
                    html = await response.text()
                    soup = BeautifulSoup(html, 'html.parser')
                    
                    # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                    article_links = soup.select('li.ArticleList_item___OGQO a.BaseArticleCard_link__Q3YFK')
                    for link in article_links:
                        href = link.get('href')
                        if href and href.startswith('/arti/'):
                            full_url = self.base_url + href
                            all_links.add(full_url)
                    
                    self.console.print(f"âœ… ë©”ì¸ í˜ì´ì§€: {len(article_links)}ê°œ ê¸°ì‚¬ ë°œê²¬")
            
            # 2. í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì¶”ê°€ ê¸°ì‚¬ ìˆ˜ì§‘
            for page in range(2, self.max_pages + 1):
                if len(all_links) >= self.max_articles:
                    break
                
                page_url = f"{self.politics_url}?page={page}"
                
                try:
                    async with self.session.get(page_url) as response:
                        if response.status == 200:
                            html = await response.text()
                            soup = BeautifulSoup(html, 'html.parser')
                            
                            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                            article_links = soup.select('li.ArticleList_item___OGQO a.BaseArticleCard_link__Q3YFK')
                            
                            if not article_links:
                                self.console.print(f"âš ï¸ í˜ì´ì§€ {page}: ë” ì´ìƒ ê¸°ì‚¬ê°€ ì—†ìŒ")
                                break
                            
                            for link in article_links:
                                href = link.get('href')
                                if href and href.startswith('/arti/'):
                                    full_url = self.base_url + href
                                    all_links.add(full_url)
                            
                            self.console.print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(article_links)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                            
                            # í˜ì´ì§€ë‹¹ ê¸°ì‚¬ ìˆ˜ê°€ ì ìœ¼ë©´ ë” ë§ì€ í˜ì´ì§€ í™•ì¸
                            if len(article_links) < self.page_size:
                                break
                                
                        else:
                            self.console.print(f"âš ï¸ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {response.status}")
                            break
                            
                except Exception as e:
                    self.console.print(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ ê°œìˆ˜ ì œí•œ
            unique_links = list(all_links)[:self.max_articles]
            
            self.console.print(f"ğŸ¯ ì´ {len(unique_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ!")
            return unique_links
            
        except Exception as e:
            self.console.print(f"âŒ ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    async def _fetch_article_details(self, article_url: str) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            async with self.session.get(article_url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì œëª© ì¶”ì¶œ
                title = None
                title_selectors = [
                    'h1.article-head-headline',
                    'h1.headline',
                    'h1.title',
                    'h2.headline',
                    'h2.title',
                    'title'
                ]
                
                for selector in title_selectors:
                    title_elem = soup.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        if title:
                            break
                
                if not title:
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = None
                content_selectors = [
                    'div.article-text',
                    'div.article-body',
                    'div.content',
                    'div.body',
                    'article',
                    'div.text'
                ]
                
                for selector in content_selectors:
                    content_elem = soup.select_one(selector)
                    if content_elem:
                        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                        for unwanted in content_elem.select('script, style, .ad, .advertisement, .related, .comment, .article-info, .audio-player, .tts-player, .read-aloud'):
                            unwanted.decompose()
                        
                        content = content_elem.get_text(strip=True, separator='\n')
                        if content and len(content) > 100:  # ìµœì†Œ 100ì ì´ìƒ
                            # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                            unwanted_patterns = [
                                'ê¸°ì‚¬ë¥¼ ì½ì–´ë“œë¦½ë‹ˆë‹¤',
                                'Your browser does not support the',
                                'audio',
                                'element',
                                '0:00'
                            ]
                            
                            for pattern in unwanted_patterns:
                                content = content.replace(pattern, '')
                            
                            # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                            content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
                            
                            if content and len(content) > 100:
                                break
                
                if not content:
                    return None
                
                # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
                published_at = None
                time_selectors = [
                    'div.article-date',
                    'div.date',
                    'time',
                    'span.date',
                    'meta[property="article:published_time"]'
                ]
                
                for selector in time_selectors:
                    time_elem = soup.select_one(selector)
                    if time_elem:
                        if selector == 'meta[property="article:published_time"]':
                            time_str = time_elem.get('content', '')
                        else:
                            time_str = time_elem.get_text(strip=True)
                        
                        if time_str:
                            try:
                                # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ íŒŒì‹±
                                if 'T' in time_str:  # ISO í˜•ì‹
                                    published_at = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                                elif len(time_str) == 19:  # YYYY-MM-DD HH:MM:SS
                                    published_at = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')
                                elif len(time_str) == 16:  # YYYY-MM-DD HH:MM
                                    published_at = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
                                else:
                                    # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
                                    published_at = datetime.now()
                                break
                            except ValueError:
                                continue
                
                if not published_at:
                    published_at = datetime.now()
                
                return {
                    'title': title,
                    'url': article_url,
                    'content': content,
                    'published_at': published_at
                }
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨ ({article_url}): {str(e)}")
            return None
    
    async def crawl_articles(self) -> None:
        """ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤í–‰"""
        self.start_time = datetime.now()
        
        try:
            # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.get_politics_article_links()
            
            if not article_links:
                self.console.print("[red]ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            self.total_articles = len(article_links)
            self.console.print(f"\nğŸ“° {self.total_articles}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
            
            # 2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TextColumn("({task.completed}/{task.total})"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                for i, article_url in enumerate(article_links):
                    try:
                        article_data = await self._fetch_article_details(article_url)
                        if article_data:
                            # í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œëŠ” issue_idë¥¼ ì„¤ì •í•˜ì§€ ì•ŠìŒ (í´ëŸ¬ìŠ¤í„°ë§ í›„ ì„¤ì •)
                            # ê¸°ë³¸ ì´ìŠˆ ID 1 ì‚¬ìš©
                            issue = {'id': 1}
                            
                            # ì–¸ë¡ ì‚¬ ì¡°íšŒ
                            media_outlet = self.supabase_manager.get_media_outlet("í•œê²¨ë ˆ")
                            if not media_outlet:
                                media_outlet = self.supabase_manager.create_media_outlet("í•œê²¨ë ˆ", "left")
                            
                            # ê¸°ì‚¬ ì €ì¥
                            article_insert_data = {
                                'title': article_data['title'],
                                'url': article_data['url'],
                                'content': article_data['content'],
                                'published_at': article_data['published_at'],
                                'issue_id': issue['id'] if isinstance(issue, dict) else issue,
                                'media_id': media_outlet['id'] if isinstance(media_outlet, dict) else media_outlet,
                                'bias': media_outlet.get('bias', 'left') if isinstance(media_outlet, dict) else 'left'
                            }
                            
                            self.supabase_manager.insert_article(article_insert_data)
                            logger.info(f"ìƒˆ ê¸°ì‚¬ ì‚½ì…: {article_data['title']}")
                            
                            self.successful_articles += 1
                        else:
                            self.failed_articles += 1
                            
                    except Exception as e:
                        logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({article_url}): {str(e)}")
                        self.failed_articles += 1
                    
                    progress.advance(task)
                    
                    # ì§„í–‰ ìƒí™© í‘œì‹œ
                    if (i + 1) % 10 == 0:
                        progress.update(task, description=f"ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘... ({i + 1}/{len(article_links)})")
            
            # 3ë‹¨ê³„: ê²°ê³¼ í‘œì‹œ
            self._display_results()
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.console.print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        finally:
            if self.session:
                await self.session.close()
    
    def _display_results(self) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        if not self.start_time:
            return
        
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="ğŸ“° í•œê²¨ë ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ë§ ê²°ê³¼")
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê²°ê³¼", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(self.total_articles))
        table.add_row("ì„±ê³µ", f"{self.successful_articles}ê°œ")
        table.add_row("ì‹¤íŒ¨", f"{self.failed_articles}ê°œ")
        table.add_row("ì„±ê³µë¥ ", f"{(self.successful_articles/self.total_articles*100):.1f}%" if self.total_articles > 0 else "0%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration:.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{self.successful_articles/duration:.2f} ê¸°ì‚¬/ì´ˆ" if duration > 0 else "0 ê¸°ì‚¬/ì´ˆ")
        
        self.console.print(table)
        
        # ì„±ê³µ/ì‹¤íŒ¨ ìš”ì•½
        if self.successful_articles > 0:
            self.console.print(f"\n[green]âœ… ì„±ê³µì ìœ¼ë¡œ {self.successful_articles}ê°œì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤![/green]")
        
        if self.failed_articles > 0:
            self.console.print(f"\n[yellow]âš ï¸ {self.failed_articles}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.[/yellow]")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_articles()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_database(self, articles: List[Dict]):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            self.console.print("[yellow]ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return
        
        self.console.print(f"\nğŸ’¾ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        
        saved_count = 0
        failed_count = 0
        
        for article in articles:
            try:
                # ìƒˆë¡œ ë§Œë“  ì €ì¥ ë©”ì„œë“œ ì‚¬ìš©
                if await self.save_article_to_supabase(article):
                    saved_count += 1
                else:
                    failed_count += 1
                    
            except Exception as e:
                self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article['title'][:50]}... - {str(e)}[/red]")
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                failed_count += 1
        
        self.console.print(f"\nâœ… ì´ {saved_count}ê°œ ê¸°ì‚¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if failed_count > 0:
            self.console.print(f"[red]ì‹¤íŒ¨: {failed_count}ê°œ[/red]")

    async def create_default_issue(self):
        """ê¸°ë³¸ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì´ìŠˆ í™•ì¸
            existing = self.supabase_manager.client.table('issues').select('id').eq('id', 1).execute()
            
            if not existing.data:
                # ê¸°ë³¸ ì´ìŠˆ ìƒì„±
                issue_data = {
                    'id': 1,
                    'title': 'ê¸°ë³¸ ì´ìŠˆ',
                    'subtitle': 'í¬ë¡¤ëŸ¬ë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ì´ìŠˆ',
                    'summary': 'ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ëœ ì •ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ì´ìŠˆì…ë‹ˆë‹¤.',
                    'bias_left_pct': 0,
                    'bias_center_pct': 0,
                    'bias_right_pct': 0,
                    'dominant_bias': 'center',
                    'source_count': 0
                }
                
                result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                self.console.print("âœ… ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì™„ë£Œ")
            else:
                self.console.print("â„¹ï¸ ê¸°ë³¸ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                
        except Exception as e:
            self.console.print(f"âŒ ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    async def save_article_to_supabase(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
            await self.create_default_issue()
            
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            published_at = article_data.get('published_at')
            if isinstance(published_at, datetime):
                published_at = published_at.isoformat()
            
            # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
            insert_data = {
                'issue_id': 1,  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                'media_id': 3,  # í•œê²¨ë ˆ media_id
                'title': article_data['title'],
                'url': article_data['url'],
                'content': article_data['content'],
                'bias': self.media_bias.lower(),
                'published_at': published_at
            }
            
            # ê¸°ì‚¬ ì €ì¥
            result = self.supabase_manager.client.table('articles').insert(insert_data).execute()
            
            if result.data:
                self.console.print(f"âœ… ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:30]}...")
                return True
            else:
                self.console.print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:30]}...")
                return False
                
        except Exception as e:
            self.console.print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            return False

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with HaniPoliticsCrawler(max_articles=100) as crawler:
        # ê¸°ì‚¬ ìˆ˜ì§‘
        articles = await crawler.crawl_articles()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        await crawler.save_to_database(articles)

if __name__ == "__main__":
    asyncio.run(main())
