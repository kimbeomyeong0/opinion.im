#!/usr/bin/env python3
"""
ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from rich.text import Text
from rich.live import Live
from rich.layout import Layout
from rich.columns import Columns
from datetime import datetime
import re
from urllib.parse import urljoin, urlparse
import logging
from utils.supabase_manager_unified import UnifiedSupabaseManager
import json

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class JoongangPoliticsCrawler:
    def __init__(self, max_articles: int = 100):
        self.base_url = "https://www.joongang.co.kr"
        self.politics_url = "https://www.joongang.co.kr/politics"
        self.max_articles = max_articles
        self.console = Console()
        self.delay = 0.1
        
        # ì¤‘ì•™ì¼ë³´ëŠ” ì¤‘ë„ ì„±í–¥
        self.media_name = "ì¤‘ì•™ì¼ë³´"
        self.media_bias = "Right"  # media_outlets í…Œì´ë¸”ì˜ ê°’ê³¼ ì •í™•íˆ ì¼ì¹˜
        
        # Supabase ë§¤ë‹ˆì € ì´ˆê¸°í™”
        try:
            self.supabase_manager = UnifiedSupabaseManager()
            self.console.print("[green]Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ[/green]")
        except Exception as e:
            self.console.print(f"[red]Supabase ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}[/red]")
            raise

    async def create_default_issue(self):
        """ê¸°ë³¸ ì´ìŠˆë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            # ê¸°ì¡´ ì´ìŠˆ í™•ì¸
            existing = self.supabase_manager.client.table('issues').select('id').eq('id', 1).execute()

            if not existing.data:
                # ê¸°ë³¸ ì´ìŠˆ ìƒì„±
                issue_data = {
                    'id': 1,
                    'title': 'ê¸°ë³¸ ì´ìŠˆ',
                    'subtitle': 'í¬ë¡¤ëŸ¬ë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì„ ìœ„í•œ ê¸°ë³¸ ì´ìŠˆ',
                    'summary': 'ë‹¤ì–‘í•œ ì–¸ë¡ ì‚¬ì—ì„œ ìˆ˜ì§‘ëœ ì •ì¹˜ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ í¬í•¨í•˜ëŠ” ê¸°ë³¸ ì´ìŠˆì…ë‹ˆë‹¤.',
                    'bias_left_pct': 0,
                    'bias_center_pct': 0,
                    'bias_right_pct': 0,
                    'dominant_bias': 'center',
                    'source_count': 0
                }

                result = self.supabase_manager.client.table('issues').insert(issue_data).execute()
                logger.info("ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì„±ê³µ")
                return True
            else:
                logger.info("ê¸°ë³¸ ì´ìŠˆê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤")
                return True

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì´ìŠˆ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            return False

    async def save_article_to_supabase(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥"""
        try:
            # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
            await self.create_default_issue()
            
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            published_at = article_data.get('published_at')
            if isinstance(published_at, datetime):
                published_at = published_at.isoformat()
            
            # ê¸°ì‚¬ ë°ì´í„° ì¤€ë¹„
            insert_data = {
                'issue_id': 1,  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                'media_id': 5,  # ì¤‘ì•™ì¼ë³´ media_id
                'title': article_data['title'],
                'url': article_data['url'],
                'content': article_data['content'],
                'bias': self.media_bias,
                'published_at': published_at
            }
            
            # Supabaseì— ì €ì¥
            result = self.supabase_manager.client.table('articles').insert(insert_data).execute()
            
            if result.data:
                logger.info(f"ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:50]}...")
                return True
            else:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:50]}...")
                return False
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return False
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.session.close()
    
    async def get_politics_article_links(self) -> List[str]:
        """ì •ì¹˜ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹)"""
        all_article_links = []
        
        # í˜ì´ì§€ë„¤ì´ì…˜ì„ í†µí•œ ê¸°ì‚¬ ìˆ˜ì§‘
        max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ ì‹œë„
        articles_per_page = 25  # ì¤‘ì•™ì¼ë³´ëŠ” í˜ì´ì§€ë‹¹ ì•½ 25ê°œ ê¸°ì‚¬
        
        for page in range(1, max_pages + 1):
            page_url = f"{self.politics_url}?page={page}"
            self.console.print(f"[cyan]ğŸ” {page}í˜ì´ì§€ í¬ë¡¤ë§: {page_url}[/cyan]")
            
            try:
                page_links = await self._get_links_from_page(page_url)
                all_article_links.extend(page_links)
                self.console.print(f"[green]  - {page}í˜ì´ì§€ì—ì„œ {len(page_links)}ê°œ ë§í¬ ë°œê²¬[/green]")
                
                # ì¶©ë¶„í•œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¤‘ë‹¨
                if len(all_article_links) >= self.max_articles:
                    break
                    
                await asyncio.sleep(self.delay)
                
            except Exception as e:
                self.console.print(f"[red]  - {page}í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}[/red]")
                logger.error(f"{page}í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
                continue
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
        unique_links = list(set(all_article_links))
        valid_links = [link for link in unique_links if self._is_valid_article_url(link)]
        valid_links.sort(reverse=True)
        
        self.console.print(f"[bold green]ì´ {len(valid_links)}ê°œ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬[/bold green]")
        return valid_links[:self.max_articles]
    
    async def _get_links_from_page(self, url: str) -> List[str]:
        """íŠ¹ì • í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return []
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                article_links = []
                
                # story_list ì•ˆì˜ cardì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ
                cards = soup.select('.story_list .card')
                for card in cards:
                    headline_link = card.select_one('.headline a')
                    if headline_link:
                        href = headline_link.get('href')
                        if href:
                            if href.startswith('/'):
                                full_url = urljoin(self.base_url, href)
                            else:
                                full_url = href
                            if full_url not in article_links and self._is_valid_article_url(full_url):
                                article_links.append(full_url)
                
                return article_links
                
        except Exception as e:
            logger.error(f"í˜ì´ì§€ {url} ë§í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _is_valid_article_url(self, url: str) -> bool:
        """ìœ íš¨í•œ ê¸°ì‚¬ URLì¸ì§€ í™•ì¸"""
        if not url:
            return False
        
        # ì¤‘ì•™ì¼ë³´ ê¸°ì‚¬ URL íŒ¨í„´ í™•ì¸
        if '/article/' not in url:
            return False
        
        # URL ê¸¸ì´ í™•ì¸ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸)
        if len(url) < 30:
            return False
        
        return True
    
    async def crawl_article(self, url: str) -> Optional[Dict]:
        """ê°œë³„ ê¸°ì‚¬ í¬ë¡¤ë§"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                html = await response.text()
                soup = BeautifulSoup(html, 'html.parser')
                
                # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                title = self._extract_title(soup)
                content = self._extract_content(soup)
                published_time = self._extract_published_time(soup)
                
                if not title or not content:
                    return None
                
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': published_time
                }
                
        except Exception as e:
            logger.error(f"ê¸°ì‚¬ {url} í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ì œëª© ì¶”ì¶œ"""
        try:
            # ì¤‘ì•™ì¼ë³´ ì œëª© ì„ íƒì
            title_selectors = [
                'h1.headline',
                '.headline h1',
                'h1.title',
                '.title h1',
                'h1'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    if title and len(title) > 5:
                        return title
            
            return None
            
        except Exception as e:
            logger.error(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_content(self, soup: BeautifulSoup) -> Optional[str]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
        try:
            # ì¤‘ì•™ì¼ë³´ ë³¸ë¬¸ ì„ íƒì
            content_selectors = [
                '.article_body',
                '.article-content',
                '.content',
                '.body',
                'article',
                '.article'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                    for elem in content_elem.select('script, style, .ad, .advertisement'):
                        elem.decompose()
                    
                    content = content_elem.get_text(strip=True, separator=' ')
                    if content and len(content) > 100:
                        return content
            
            return None
            
        except Exception as e:
            logger.error(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    def _extract_published_time(self, soup: BeautifulSoup) -> Optional[datetime]:
        """ê¸°ì‚¬ ë°œí–‰ ì‹œê°„ ì¶”ì¶œ"""
        try:
            # ì¤‘ì•™ì¼ë³´ ì‹œê°„ ì„ íƒì
            time_selectors = [
                'meta[name="article:published_time"]',
                '.date',
                '.published_date',
                '.article_date',
                '.time'
            ]
            
            for selector in time_selectors:
                if selector.startswith('meta'):
                    time_elem = soup.select_one(selector)
                    if time_elem:
                        time_str = time_elem.get('content')
                        if time_str:
                            try:
                                # ISO 8601 í˜•ì‹ íŒŒì‹±
                                return datetime.fromisoformat(time_str.replace('Z', '+00:00'))
                            except:
                                pass
                else:
                    time_elem = soup.select_one(selector)
                    if time_elem:
                        time_str = time_elem.get_text(strip=True)
                        if time_str:
                            # ì¤‘ì•™ì¼ë³´ ë‚ ì§œ í˜•ì‹: "2025.08.20 22:48"
                            try:
                                return datetime.strptime(time_str, '%Y.%m.%d %H:%M')
                            except:
                                pass
            
            return None
            
        except Exception as e:
            logger.error(f"ë°œí–‰ ì‹œê°„ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return None
    
    async def save_to_database(self, articles: List[Dict]) -> None:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return
        
        self.console.print("\në°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
        
        # ê¸°ë³¸ ì´ìŠˆ ìƒì„± í™•ì¸
        await self.create_default_issue()
        
        # ê¸°ì‚¬ ì €ì¥
        saved_count = 0
        for article in articles:
            try:
                # ê¸°ì¡´ ê¸°ì‚¬ í™•ì¸
                existing = self.supabase_manager.client.table('articles').select('id').eq('url', article['url']).execute()
                
                if existing.data:
                    # ê¸°ì¡´ ê¸°ì‚¬ ì—…ë°ì´íŠ¸
                    self.supabase_manager.client.table('articles').update({
                        'title': article['title'],
                        'content': article['content'],
                        'published_at': article['published_at'].isoformat() if article['published_at'] else None
                    }).eq('url', article['url']).execute()
                    
                    self.console.print(f"[yellow]ê¸°ì¡´ ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {article['title'][:50]}...[/yellow]")
                else:
                    # ìƒˆ ê¸°ì‚¬ ì‚½ì…
                    self.supabase_manager.insert_article({
                        'issue_id': 1,  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
                        'media_id': 5,  # ì¤‘ì•™ì¼ë³´ media_id
                        'title': article['title'],
                        'url': article['url'],
                        'content': article['content'],
                        'bias': self.media_bias,  # media_outlets í…Œì´ë¸”ì˜ ê°’ê³¼ ì •í™•íˆ ì¼ì¹˜
                        'published_at': article['published_at']
                    })
                    
                    saved_count += 1
                    self.console.print(f"[green]ìƒˆ ê¸°ì‚¬ ì‚½ì…: {article['title'][:50]}...[/green]")
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                continue
        
        self.console.print(f"[bold green]âœ… {saved_count}ê°œ ê¸°ì‚¬ ì €ì¥ ì„±ê³µ![/bold green]")
        
        # ì´ìŠˆ í¸í–¥ì„± ì—…ë°ì´íŠ¸
        try:
            self.supabase_manager.update_issue_bias(1)  # ê¸°ë³¸ ì´ìŠˆ ID ì‚¬ìš©
            self.console.print(f"[green]ì´ìŠˆ í¸í–¥ì„± ì—…ë°ì´íŠ¸ ì„±ê³µ: 1[/green]")
        except Exception as e:
            logger.error(f"ì´ìŠˆ í¸í–¥ì„± ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
    
    def display_results(self, articles: List[Dict], elapsed_time: float) -> None:
        """í¬ë¡¤ë§ ê²°ê³¼ í‘œì‹œ"""
        if not articles:
            self.console.print("[red]í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return
        
        # ê²°ê³¼ ìš”ì•½
        success_count = len(articles)
        failed_count = self.max_articles - success_count
        speed = success_count / elapsed_time if elapsed_time > 0 else 0
        
        summary_panel = Panel(
            f"â±ï¸  í¬ë¡¤ë§ ì‹œê°„: {elapsed_time:.2f}ì´ˆ\n"
            f"ğŸ“° ë°œê²¬ëœ ê¸°ì‚¬: {self.max_articles}ê°œ\n"
            f"âœ… ì„±ê³µ: {success_count}ê°œ\n"
            f"âŒ ì‹¤íŒ¨: {failed_count}ê°œ\n"
            f"ğŸš€ ì†ë„: {speed:.1f} ê¸°ì‚¬/ì´ˆ",
            title="ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼",
            border_style="blue"
        )
        
        self.console.print(summary_panel)
        
        # ê¸°ì‚¬ ëª©ë¡ í…Œì´ë¸”
        table = Table(title="ğŸ“° í¬ë¡¤ë§ëœ ê¸°ì‚¬ ëª©ë¡")
        table.add_column("ë²ˆí˜¸", style="cyan", no_wrap=True)
        table.add_column("ì œëª©", style="white")
        table.add_column("ê¸¸ì´", style="green")
        table.add_column("ì‹œê°„", style="yellow")
        
        for i, article in enumerate(articles[:20], 1):  # ì²˜ìŒ 20ê°œë§Œ í‘œì‹œ
            title = article['title'][:50] + "..." if len(article['title']) > 50 else article['title']
            content_length = len(article['content']) if article['content'] else 0
            published_time = article['published_at'].strftime('%Y-%m-%d\n%H:%M') if article['published_at'] else "N/A"
            
            table.add_row(
                str(i),
                title,
                f"{content_length:,}ì",
                published_time
            )
        
        if len(articles) > 20:
            table.add_row("...", f"ë° {len(articles) - 20}ê°œ ë”", "", "")
        
        self.console.print(table)
    
    async def run(self) -> None:
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        start_time = time.time()
        
        # ì œëª© ì¶œë ¥
        title_panel = Panel(
            "ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬\n"
            "ğŸš€ ìµœì‹  ì •ì¹˜ ê¸°ì‚¬ 100ê°œë¥¼ ë¹ ë¥´ê²Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤",
            title="ì¤‘ì•™ì¼ë³´ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬",
            border_style="green"
        )
        self.console.print(title_panel)
        
        # 1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
        self.console.print("\nğŸ” ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        article_links = await self.get_politics_article_links()
        
        if not article_links:
            self.console.print("[red]ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
            return
        
        # 2ë‹¨ê³„: ê¸°ì‚¬ í¬ë¡¤ë§
        self.console.print(f"\nğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeElapsedColumn(),
            console=self.console
        ) as progress:
            task = progress.add_task("ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...", total=len(article_links))
            
            articles = []
            for link in article_links:
                article = await self.crawl_article(link)
                if article:
                    articles.append(article)
                
                progress.advance(task)
                await asyncio.sleep(self.delay)
        
        # 3ë‹¨ê³„: ê²°ê³¼ í‘œì‹œ
        elapsed_time = time.time() - start_time
        self.display_results(articles, elapsed_time)
        
        # 4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
        if articles:
            await self.save_to_database(articles)
        
        self.console.print("\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_article()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    async with JoongangPoliticsCrawler(max_articles=100) as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
