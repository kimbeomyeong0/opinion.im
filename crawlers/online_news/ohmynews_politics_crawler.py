#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì˜¤ë§ˆì´ë‰´ìŠ¤ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬
- 20ì´ˆ ë‚´ì— 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
- ë³¸ë¬¸ì„ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ (êµ°ë”ë”ê¸° ì œê±°)
- biasë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì • (Left)
"""

import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from supabase_manager_v2 import SupabaseManagerV2
import re

class OhMyNewsPoliticsCrawler:
    def __init__(self):
        self.base_url = "https://www.ohmynews.com/NWS_Web/ArticlePage/Total_Article.aspx"
        self.politics_url = f"{self.base_url}?PAGE_CD=C0400"
        self.manager = SupabaseManagerV2()
        self.media_id = 9  # ì˜¤ë§ˆì´ë‰´ìŠ¤ media_id
        self.issue_id = 1  # ê¸°ë³¸ issue_id
        self.collected_articles = set()
        
    async def get_media_outlet(self):
        """ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            result = self.manager.client.table('media_outlets').select('*').eq('id', self.media_id).execute()
            if result.data:
                return result.data[0]
            return None
        except Exception as e:
            print(f"âŒ ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def collect_article_links(self, html_content):
        """HTMLì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        links = []
        
        # ê¸°ì‚¬ ëª©ë¡ì—ì„œ ë§í¬ ì¶”ì¶œ
        articles = soup.find_all('li')
        for article in articles:
            link_elem = article.find('a', href=True)
            if link_elem:
                href = link_elem['href']
                if '/NWS_Web/View/at_pg.aspx?CNTN_CD=' in href:
                    full_url = urljoin('https://www.ohmynews.com', href)
                    if full_url not in self.collected_articles:
                        links.append(full_url)
                        self.collected_articles.add(full_url)
        
        return links
    
    async def fetch_page(self, session, url):
        """í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    print(f"âš ï¸  í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {url}, ìƒíƒœ: {response.status}")
                    return None
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
            return None
    
    def extract_article_content(self, html_content):
        """ê¸°ì‚¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ê³  ì •ë¦¬í•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ - ì˜¤ë§ˆì´ë‰´ìŠ¤ëŠ” h2.article_titì— ì‹¤ì œ ì œëª©ì´ ìˆìŒ
        title = ""
        title_elem = soup.find('h2', class_='article_tit')
        if title_elem:
            title = title_elem.get_text(strip=True)
        
        # ë³¸ë¬¸ ì¶”ì¶œ - ì˜¤ë§ˆì´ë‰´ìŠ¤ëŠ” div.at_contentsì— ì‹¤ì œ ë³¸ë¬¸ì´ ìˆìŒ
        content = ""
        content_elem = soup.select_one('div.at_contents')
        if content_elem:
            content = content_elem.get_text(strip=True)
        
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        if not content:
            content_elem = soup.select_one('div.content_lt')
            if content_elem:
                content = content_elem.get_text(strip=True)
        
        # ë³¸ë¬¸ì´ ì—¬ì „íˆ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
        if not content:
            content_elem = soup.find('div', class_=re.compile(r'content|body|text|article'))
            if content_elem:
                content = content_elem.get_text(strip=True)
        
        # ë‚ ì§œ ì¶”ì¶œ - ì˜¤ë§ˆì´ë‰´ìŠ¤ëŠ” 25.08.20 í˜•ì‹
        publish_date = None
        date_patterns = [
            r'(\d{2}\.\d{2}\.\d{2})',  # 25.08.20 í˜•ì‹ ìš°ì„ 
            r'(\d{4}\.\d{2}\.\d{2})',
            r'(\d{4}-\d{2}-\d{2})'
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, html_content)
            if date_match:
                date_str = date_match.group(1)
                
                # 25.08.20 í˜•ì‹ì„ 2025-08-20 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                if re.match(r'\d{2}\.\d{2}\.\d{2}', date_str):
                    parts = date_str.split('.')
                    if len(parts) == 3:
                        year = '20' + parts[0]  # 25 -> 2025
                        month = parts[1]
                        day = parts[2]
                        publish_date = f"{year}-{month}-{day}"
                else:
                    publish_date = date_str
                break
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œë“¤ ì œê±° - ì˜¤ë§ˆì´ë‰´ìŠ¤ íŠ¹í™”
        unwanted_patterns = [
            r'ì˜¤ë§ˆì´ë‰´ìŠ¤.*?',
            r'ì‚¬ì´íŠ¸ ì „ì²´ë³´ê¸°.*?',
            r'ì¸ê¸°ê¸°ì‚¬.*?',
            r'topHistory.*?',
            r'ì´ìš©ê°€ì´ë“œ.*?',
            r'ëª¨ë°”ì¼ ì´ìš©ì•ˆë‚´.*?',
            r'ë‰´ìŠ¤.*?',
            r'ì „ì²´ê¸°ì‚¬.*?',
            r'ì •ì¹˜.*?',
            r'ê²½ì œ.*?',
            r'ì‚¬íšŒ.*?',
            r'êµìœ¡.*?',
            r'ë¯¸ë””ì–´.*?',
            r'ë¯¼ì¡±Â·êµ­ì œ.*?',
            r'ì—¬ì„±.*?',
            r'ë§Œí‰Â·ë§Œí™”.*?',
            r'ê·¸ë˜í”½ë‰´ìŠ¤.*?',
            r'ì¹´ë“œë‰´ìŠ¤.*?',
            r'ì˜ìƒë‰´ìŠ¤.*?',
            r'ì‚¬ëŠ”ì´ì•¼ê¸°.*?',
            r'ë¬¸í™”.*?',
            r'ì—¬í–‰.*?',
            r'ì±….*?',
            r'ë™ë„¤ë‰´ìŠ¤.*?',
            r'ì§€ë„.*?',
            r'ì§€ì—­.*?',
            r'ì œíœ´ë§¤ì²´.*?',
            r'ì‹œë¦¬ì¦ˆ.*?',
            r'ì „ì²´ì—°ì¬.*?',
            r'ê¸€ì”¨ í¬ê²Œë³´ê¸°.*?',
            r'í˜ì´ìŠ¤ë¶.*?',
            r'íŠ¸ìœ„í„°.*?',
            r'ê³µìœ í•˜ê¸°.*?',
            r'ì¶”ì²œ.*?',
            r'ëŒ“ê¸€.*?',
            r'ì›ê³ ë£Œë¡œ ì‘ì›.*?',
            r'ìµœì¢… ì—…ë°ì´íŠ¸.*?',
            r'ã…£.*?',
            r'\[ì´ìŠˆ ë¶„ì„\].*?',
            r'ê³½ìš°ì‹ \(gorapakr\).*?',
            r'AD.*?',
            r'ê´‘ê³ .*?',
            r'í°ì‚¬ì§„ë³´ê¸°.*?',
            r'ê´€ë ¨ì‚¬ì§„ë³´ê¸°.*?',
            r'Please activate JavaScript.*?',
            r'LiveRe.*?',
            r'Copyright.*?',
            r'All rights reserved.*?'
        ]
        
        for pattern in unwanted_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)
        
        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
        content = re.sub(r'\s+', ' ', content).strip()
        
        return {
            'title': title,
            'content': content,
            'publish_date': publish_date
        }
    
    async def process_article(self, session, url):
        """ê°œë³„ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        try:
            html_content = await self.fetch_page(session, url)
            if not html_content:
                return None
            
            article_data = self.extract_article_content(html_content)
            
            if not article_data['title'] or not article_data['content']:
                return None
            
            # Supabaseì— ì €ì¥
            try:
                result = self.manager.client.table('articles').insert({
                    'issue_id': self.issue_id,
                    'media_id': self.media_id,
                    'title': article_data['title'],
                    'url': url,
                    'content': article_data['content'],
                    'bias': 'Left',  # ì˜¤ë§ˆì´ë‰´ìŠ¤ëŠ” Left bias
                    'published_at': article_data['publish_date']
                }).execute()
                
                if result.data:
                    return True
                else:
                    print(f"âš ï¸  ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {url}")
                    return False
                    
            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
                return False
                
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {url}, ì˜¤ë¥˜: {e}")
            return False
    
    async def crawl_articles(self, target_count=100):
        """ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        print(f"ğŸš€ ì˜¤ë§ˆì´ë‰´ìŠ¤ ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ë§ ì‹œì‘ (ëª©í‘œ: {target_count}ê°œ)")
        
        start_time = time.time()
        success_count = 0
        fail_count = 0
        
        # ë¯¸ë””ì–´ ì•„ìš¸ë › í™•ì¸
        media_outlet = await self.get_media_outlet()
        if not media_outlet:
            print("âŒ ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“° {media_outlet['name']} (ID: {self.media_id}) í¬ë¡¤ë§ ì‹œì‘")
        
        # í˜ì´ì§€ë³„ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
        page = 1
        max_pages = 10  # ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€
        
        async with aiohttp.ClientSession(
            connector=aiohttp.TCPConnector(limit=50, limit_per_host=50),
            timeout=aiohttp.ClientTimeout(total=20)
        ) as session:
            
            while len(self.collected_articles) < target_count and page <= max_pages:
                # í˜ì´ì§€ URL ìƒì„±
                if page == 1:
                    page_url = self.politics_url
                else:
                    page_url = f"{self.politics_url}&pageno={page}"
                
                print(f"ğŸ“„ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘... ({len(self.collected_articles)}ê°œ ìˆ˜ì§‘ë¨)")
                
                # í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
                html_content = await self.fetch_page(session, page_url)
                if not html_content:
                    print(f"âš ï¸  {page}í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
                    page += 1
                    continue
                
                # ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
                links = self.collect_article_links(html_content)
                if not links:
                    print(f"âš ï¸  {page}í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    page += 1
                    continue
                
                print(f"ğŸ”— {page}í˜ì´ì§€ì—ì„œ {len(links)}ê°œ ê¸°ì‚¬ ë§í¬ ë°œê²¬")
                
                # ê¸°ì‚¬ ì²˜ë¦¬ (ë™ì‹œ ì²˜ë¦¬) - ë” ì ê·¹ì ìœ¼ë¡œ
                tasks = []
                for link in links[:target_count - len(self.collected_articles)]:
                    task = self.process_article(session, link)
                    tasks.append(task)
                
                # ë™ì‹œ ì‹¤í–‰ - ë” ë§ì€ ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
                if tasks:
                    # 50ê°œì”© ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                    chunk_size = 50
                    for i in range(0, len(tasks), chunk_size):
                        chunk = tasks[i:i + chunk_size]
                        results = await asyncio.gather(*chunk, return_exceptions=True)
                        
                        for result in results:
                            if isinstance(result, Exception):
                                fail_count += 1
                            elif result:
                                success_count += 1
                            else:
                                fail_count += 1
                
                for result in results:
                    if isinstance(result, Exception):
                        fail_count += 1
                    elif result:
                        success_count += 1
                    else:
                        fail_count += 1
                
                # ëª©í‘œ ë‹¬ì„± í™•ì¸
                if len(self.collected_articles) >= target_count:
                    break
                
                page += 1
                
                # ì ì‹œ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€) - ìµœì í™”
                await asyncio.sleep(0.2)
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        print(f"\nğŸ‰ ì˜¤ë§ˆì´ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
        print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
        print(f"ğŸ“Š ì´ ìˆ˜ì§‘: {len(self.collected_articles)}ê°œ")
        
        if elapsed_time <= 20:
            print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±! 20ì´ˆ ì´ë‚´ ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
        else:
            print(f"âš ï¸  ëª©í‘œ ì´ˆê³¼: 20ì´ˆ ì´ˆê³¼ ({elapsed_time:.2f}ì´ˆ)")
        
        return success_count

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = OhMyNewsPoliticsCrawler()
    await crawler.crawl_articles(100)

if __name__ == "__main__":
    asyncio.run(main())
