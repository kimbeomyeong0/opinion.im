from typing import List, Dict
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í”„ë ˆì‹œì•ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ (ìµœì í™” ë²„ì „)
í”„ë ˆì‹œì•ˆ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì •ì¹˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
import logging
from bs4 import BeautifulSoup
from datetime import datetime
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from rich import box
import re
from utils.supabase_manager_unified import UnifiedSupabaseManager
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.common.html_parser import HTMLParserUtils

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Rich ì½˜ì†” ì„¤ì •
console = Console()

class PressianPoliticsCrawler:
    def __init__(self):
        self.base_url = "https://www.pressian.com"
        self.politics_url = "https://www.pressian.com/pages/news-politics-list"
        self.supabase_manager = UnifiedSupabaseManager()
        self.media_outlet = "í”„ë ˆì‹œì•ˆ"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    async def get_page_content(self, session, url):
        """í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            async with session.get(url, headers=self.headers) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨ ({url}): {e}")
            return None

    async def collect_article_links(self, session, max_articles=100):
        """ê¸°ì‚¬ ë§í¬ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
        console.print("ğŸ” í”„ë ˆì‹œì•ˆ ì •ì¹˜ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        
        all_links = []
        page = 1
        
        while len(all_links) < max_articles and page <= 10:  # ìµœëŒ€ 10í˜ì´ì§€
            if page == 1:
                url = self.politics_url
            else:
                url = f"{self.politics_url}?page={page}"
            
            console.print(f"ğŸ“„ {page}í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘... ({url})")
            
            html_content = await self.get_page_content(session, url)
            if not html_content:
                break
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°: div.arl_022 ul.list li p.title a
            article_elements = soup.select('div.arl_022 ul.list li')
            
            page_links = []
            for element in article_elements:
                link_elem = element.select_one('p.title a')
                if link_elem and link_elem.get('href'):
                    href = link_elem.get('href')
                    if href.startswith('/pages/articles/'):
                        full_url = f"{self.base_url}{href}"
                        page_links.append(full_url)
            
            if not page_links:
                console.print(f"âŒ {page}í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            all_links.extend(page_links)
            console.print(f"âœ… {page}í˜ì´ì§€ì—ì„œ {len(page_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ (ì´ {len(all_links)}ê°œ)")
            
            page += 1
        
        # ëª©í‘œ ê°œìˆ˜ë§Œí¼ ìë¥´ê¸°
        if len(all_links) > max_articles:
            all_links = all_links[:max_articles]
        
        console.print(f"ğŸ¯ ì´ {len(all_links)}ê°œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_links

    def extract_article_content(self, html_content, url):
        """ê¸°ì‚¬ ì œëª©ê³¼ ë³¸ë¬¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ - í”„ë ˆì‹œì•ˆì˜ ê²½ìš° title íƒœê·¸ë‚˜ og:title ë©”íƒ€íƒœê·¸ì—ì„œ ì¶”ì¶œ
        title = None
        
        # 1. og:title ë©”íƒ€íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ (ê°€ì¥ ì •í™•)
        meta_title = soup.find('meta', property='og:title')
        if meta_title and meta_title.get('content'):
            title = meta_title.get('content').strip()
        
        # 2. title íƒœê·¸ì—ì„œ ì œëª© ì¶”ì¶œ
        if not title:
            page_title = soup.find('title')
            if page_title:
                title = page_title.get_text(strip=True)
                # ì‚¬ì´íŠ¸ëª… ì œê±° (ì˜ˆ: " - í”„ë ˆì‹œì•ˆ")
                if ' - ' in title:
                    title = title.split(' - ')[0].strip()
        
        # 3. ë‹¤ë¥¸ ì„ íƒìë“¤ (í›„ì› ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
        if not title or 'í›„ì›' in title:
            title_selectors = [
                'h1',
                '.article_title',
                'h1.title',
                'h3.title',
                'p.title',
                '.title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    candidate_title = title_elem.get_text(strip=True)
                    if candidate_title and len(candidate_title) > 10 and 'í›„ì›' not in candidate_title:
                        title = candidate_title
                        break
        
        if not title or 'í›„ì›' in title:
            title = f"í”„ë ˆì‹œì•ˆ ê¸°ì‚¬ - {url.split('/')[-1]}"

        # ë³¸ë¬¸ ì¶”ì¶œ
        content = None
        content_selectors = [
            'div.article_body',
            'div.article-content',
            'div.content',
            'div.body',
            'div.article-body',
            'div.text',
            'article',
            'div.arl_022'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
                for unwanted in content_elem.select('script, style, .ad, .advertisement, .related, .comment, .ui-control, .font-control, .share-control, .sidebar, .side-news, .related-news, .reading-mode, .dark-mode, .font-size, .bookmark, .print, .share, .reaction, .recommend, .like, .subscribe, .donation, .footer, .navigation, .menu, .header, .banner'):
                    unwanted.decompose()
                
                content = content_elem.get_text(strip=True, separator='\n')
                if content and len(content) > 100:
                    # ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì œê±°
                    unwanted_patterns = [
                        'ì½ê¸°ëª¨ë“œ', 'ë‹¤í¬ëª¨ë“œ', 'í°íŠ¸í¬ê¸°', 'ê°€ê°€ê°€ê°€ê°€ê°€', 'ë¶ë§ˆí¬', 'ê³µìœ í•˜ê¸°', 'í”„ë¦°íŠ¸',
                        'ê¸°ì‚¬ë°˜ì‘', 'ì¶”ì²œí•´ìš”', 'ì¢‹ì•„ìš”', 'ê°ë™ì´ì—ìš”', 'í™”ë‚˜ìš”', 'ìŠ¬í¼ìš”',
                        'My ì¶”ì²œ ê¸°ì‚¬', 'ê°€ì¥ ë§ì´ ì½ì€ ê¸°ì‚¬', 'ëŒ“ê¸€ ë§ì€ ê¸°ì‚¬', 'ì‹¤ì‹œê°„ ìµœì‹  ë‰´ìŠ¤',
                        'ì£¼ìš”ë‰´ìŠ¤', 'ì´ìŠˆNOW', 'ê´€ë ¨ê¸°ì‚¬', 'ë”ë³´ê¸°', 'ëª©ë¡', 'ì´ì „ê¸€', 'ë‹¤ìŒê¸€',
                        'êµ¬ë…', 'ê¸°ì‚¬ í›„ì›í•˜ê¸°', 'ì¹´ì¹´ì˜¤í†¡', 'í˜ì´ìŠ¤ë¶', 'íŠ¸ìœ„í„°', 'ë¼ì¸', 'ë§í¬ë³µì‚¬',
                        'ì°¸ì‚¬ëŠ” ìˆ˜ìŠµí–ˆì§€ë§Œ', 'å°¹ ëŒ€í†µë ¹ íŒŒë©´', '3ëŒ€ íŠ¹ê²€', 'ì´ì¬ëª… ì •ë¶€',
                        'ë¯¸êµ­ ê°€ëŠ”', 'ì •ì˜ì„ ', 'ì„ìœ í™”í•™', 'ì •ë¶€, ì—°ë‚´', 'ê½ƒì´ ëœ'
                    ]
                    
                    for pattern in unwanted_patterns:
                        content = content.replace(pattern, '')
                    
                    # ê¸°ì ì •ë³´ ì´í›„ì˜ ëª¨ë“  ë¶ˆí•„ìš”í•œ ë‚´ìš© ì œê±°
                    lines = content.split('\n')
                    cleaned_lines = []
                    for line in lines:
                        # ê¸°ì ì •ë³´ë¥¼ ì°¾ìœ¼ë©´ ê·¸ ì´í›„ëŠ” ëª¨ë‘ ì œê±°
                        if 'ê¸°ì' in line and len(line.strip()) < 30:
                            cleaned_lines.append(line.strip())
                            break
                        cleaned_lines.append(line)
                    
                    content = '\n'.join(cleaned_lines)
                    
                    # ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì¶”ê°€ ì •ë¦¬
                    import re
                    
                    # ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
                    content = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', content)
                    
                    # í•´ì‹œíƒœê·¸ ì œê±°
                    content = re.sub(r'#\s*\w+', '', content)
                    
                    # ìˆ«ìë§Œ ìˆëŠ” ë¼ì¸ ì œê±°
                    content = re.sub(r'^\d+$', '', content, flags=re.MULTILINE)
                    
                    # ì‹œê°„ í˜•ì‹ ì œê±° (12:10 ê°™ì€)
                    content = re.sub(r'^\d{1,2}:\d{2}$', '', content, flags=re.MULTILINE)
                    
                    # ì—°ì†ëœ ë¹ˆ ì¤„ ì •ë¦¬
                    content = '\n'.join([line.strip() for line in content.split('\n') if line.strip()])
                    
                    if content and len(content) > 100:
                        break
        
        return title, content

    def extract_publish_date(self, html_content):
        """ë°œí–‰ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ë‚ ì§œ ì„ íƒìë“¤
        date_selectors = [
            '.date',
            'p.date',
            '.publish-date',
            '.article-date',
            'time',
            '[datetime]'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    # "ê¸°ì‚¬ì…ë ¥" ë¬¸ìì—´ ì œê±°
                    date_text = date_text.replace('ê¸°ì‚¬ì…ë ¥', '').strip()
                    
                    # í”„ë ˆì‹œì•ˆ ë‚ ì§œ í˜•ì‹: "2025.08.21. 09:59:25"
                    try:
                        # ë‚ ì§œ íŒŒì‹±
                        date_match = re.search(r'(\d{4})\.(\d{2})\.(\d{2})\.\s+(\d{2}):(\d{2}):(\d{2})', date_text)
                        if date_match:
                            year, month, day, hour, minute, second = date_match.groups()
                            date_obj = datetime(int(year), int(month), int(day), int(hour), int(minute), int(second))
                            return date_obj.isoformat()
                    except Exception as e:
                        logger.error(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_text} - {e}")
                        continue
        
        # ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        return datetime.now().isoformat()

    async def process_single_article(self, session, url, semaphore):
        """ë‹¨ì¼ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        async with semaphore:
            try:
                # ê¸°ì‚¬ê°€ ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸
                existing_article = self.supabase_manager.client.table('articles').select('id').eq('url', url).execute()
                if existing_article.data:
                    return True  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê¸°ì‚¬
                
                # ê¸°ì‚¬ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                html_content = await self.get_page_content(session, url)
                if not html_content:
                    return False
                
                # ì œëª©ê³¼ ë³¸ë¬¸ ì¶”ì¶œ
                title, content = self.extract_article_content(html_content, url)
                if not title or not content:
                    return False
                
                # ë°œí–‰ì¼ ì¶”ì¶œ
                publish_date = self.extract_publish_date(html_content)
                
                # ìƒˆ ê¸°ì‚¬ ì‚½ì…
                article_data = {
                    'title': title,
                    'url': url,
                    'content': content,
                    'published_at': publish_date,
                    'media_id': self.supabase_manager.get_media_outlet(self.media_outlet)['id'],
                    'issue_id': 6  # ì„ì‹œ issue_id
                }
                
                result = self.supabase_manager.insert_article(article_data)
                return result is not None
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ ì²˜ë¦¬ ì‹¤íŒ¨ ({url}): {e}")
                return False

    async def crawl_articles(self, article_links):
        """ê¸°ì‚¬ë“¤ì„ ë™ì‹œì— í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
        console.print(f"\nğŸ“° {len(article_links)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œì‘...")
        
        # ì„¸ì…˜ ì„¤ì • - ì„±ëŠ¥ ìµœì í™”
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=10, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=console
            ) as progress:
                
                task = progress.add_task("ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                # ë™ì‹œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¸ë§ˆí¬ì–´ (ìµœëŒ€ 30ê°œ ë™ì‹œ ìš”ì²­)
                semaphore = asyncio.Semaphore(30)
                
                async def process_with_progress(url):
                    result = await self.process_single_article(session, url, semaphore)
                    progress.update(task, advance=1)
                    return result
                
                # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë™ì‹œì— ì²˜ë¦¬
                results = await asyncio.gather(*[process_with_progress(url) for url in article_links], return_exceptions=True)
                
                success_count = sum(1 for result in results if result is True)
                failed_count = len(results) - success_count
        
        return success_count, failed_count

    async def run(self):
        """í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        start_time = datetime.now()
        
        console.print("ğŸš€ í”„ë ˆì‹œì•ˆ ì •ì¹˜ ê¸°ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘!")
        console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ì„¸ì…˜ ì„¤ì •
        connector = aiohttp.TCPConnector(limit=200, limit_per_host=50)
        timeout = aiohttp.ClientTimeout(total=10, connect=3)
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            article_links = await self.collect_article_links(session)
            
            if not article_links:
                console.print("âŒ ìˆ˜ì§‘í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # 2. ê¸°ì‚¬ í¬ë¡¤ë§
            success_count, failed_count = await self.crawl_articles(article_links)
        
        # ê²°ê³¼ ìš”ì•½
        end_time = datetime.now()
        duration = end_time - start_time
        
        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        table = Table(title="í”„ë ˆì‹œì•ˆ í¬ë¡¤ë§ ê²°ê³¼", box=box.ROUNDED)
        table.add_column("í•­ëª©", style="cyan", no_wrap=True)
        table.add_column("ê°’", style="magenta")
        
        table.add_row("ì´ ê¸°ì‚¬ ìˆ˜", str(len(article_links)))
        table.add_row("ì„±ê³µ", f"{success_count}ê°œ", style="green")
        table.add_row("ì‹¤íŒ¨", f"{failed_count}ê°œ", style="red")
        table.add_row("ì„±ê³µë¥ ", f"{(success_count/len(article_links)*100):.1f}%")
        table.add_row("ì†Œìš” ì‹œê°„", f"{duration.total_seconds():.2f}ì´ˆ")
        table.add_row("í‰ê·  ì†ë„", f"{len(article_links)/duration.total_seconds():.2f} ê¸°ì‚¬/ì´ˆ")
        
        console.print(table)
        console.print(f"âœ… í”„ë ˆì‹œì•ˆ í¬ë¡¤ë§ ì™„ë£Œ! ğŸ‰")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.crawl_articles()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])


    async def save_to_supabase(self, articles: List[Dict]) -> Dict[str, int]:
        """Supabaseì— ê¸°ì‚¬ ì €ì¥"""
        if not articles:
            return {"success": 0, "failed": 0}
        
        success_count = 0
        failed_count = 0
        
        try:
            for article in articles:
                if hasattr(self, 'supabase_manager') and self.supabase_manager:
                    if self.supabase_manager.insert_article(article):
                        success_count += 1
                    else:
                        failed_count += 1
                else:
                    failed_count += 1
        except Exception as e:
            print(f"âŒ Supabase ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            failed_count = len(articles)
        
        return {"success": success_count, "failed": failed_count}

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = PressianPoliticsCrawler()
    await crawler.run()

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))