#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MBC ì •ì¹˜ ì„¹ì…˜ í¬ë¡¤ëŸ¬
- 20ì´ˆ ë‚´ì— 100ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
- ë³¸ë¬¸ì„ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ (êµ°ë”ë”ê¸° ì œê±°)
- biasë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì • (media_outlets í…Œì´ë¸” ì°¸ê³ )
- ë‚ ì§œ ì´ë™ìœ¼ë¡œ ê¸°ì‚¬ ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import time
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.panel import Panel
from datetime import datetime
import re
from urllib.parse import urljoin
import logging
from utils.supabase_manager_unified import UnifiedSupabaseManager
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from utils.common.html_parser import HTMLParserUtils
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MBCPoliticsCrawler:
    def __init__(self):
        self.console = Console()
        self.supabase = UnifiedSupabaseManager()
        self.media_id = 11  # MBC
        self.issue_id = 1  # ê¸°ë³¸ ì´ìŠˆ ID
        self.base_url = "https://imnews.imbc.com"
        self.politics_url = "https://imnews.imbc.com/news/2025/politics/"
        self.session: Optional[aiohttp.ClientSession] = None
        
        self.max_articles = 100
        self.max_workers = 15
        self.timeout = 5
        self.delay = 0.05
        
        self.articles = []
        self.collected_articles = set()
        self.stats = {
            'total_found': 0,
            'successful': 0,
            'failed': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def __aenter__(self):
        connector = aiohttp.TCPConnector(limit=self.max_workers, limit_per_host=8)
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers={
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def get_media_outlet(self):
        """ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            result = self.supabase.client.table('media_outlets').select('*').eq('id', self.media_id).execute()
            if result.data:
                return result.data[0]
            return None
        except Exception as e:
            self.console.print(f"âŒ ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    async def get_politics_article_links(self) -> List[str]:
        """ë‚ ì§œ ì´ë™ìœ¼ë¡œ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘"""
        self.console.print(f"[cyan]ğŸ” Playwrightë¡œ ë‚ ì§œ ì´ë™í•˜ë©° ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...[/cyan]")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            
            try:
                await page.goto(self.politics_url, wait_until='networkidle')
                await page.wait_for_timeout(1500)
                
                article_links = []
                date_count = 0
                max_date_attempts = 25
                
                while len(article_links) < self.max_articles and date_count < max_date_attempts:
                    # í˜„ì¬ í˜ì´ì§€ ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
                    new_links = await self._extract_article_links_from_page(page)
                    article_links.extend(new_links)
                    
                    # ì´ì „ ë‚ ì§œë¡œ ì´ë™
                    prev_button = await page.query_selector('a.btn_date.date_prev')
                    if not prev_button:
                        break
                    
                    await prev_button.click()
                    await page.wait_for_timeout(1500)
                    await page.wait_for_load_state('networkidle')
                    
                    date_count += 1
                    if date_count % 5 == 0:
                        self.console.print(f"[yellow]  - {date_count}ì¼ ì „ê¹Œì§€ {len(article_links)}ê°œ ê¸°ì‚¬ ë°œê²¬[/yellow]")
                    
                    if len(article_links) >= self.max_articles:
                        break
                        
            finally:
                await browser.close()
        
        # ì¤‘ë³µ ì œê±° ë° ì œí•œ
        unique_links = list(dict.fromkeys(article_links))
        return unique_links[:self.max_articles]
    
    async def _extract_article_links_from_page(self, page) -> List[str]:
        """í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ"""
        try:
            article_elements = await page.query_selector_all('a[href*="/article/"]')
            links = []
            
            for element in article_elements:
                href = await element.get_attribute('href')
                if href and '/article/' in href:
                    full_url = urljoin(self.base_url, href)
                    if full_url not in self.collected_articles:
                        links.append(full_url)
                        self.collected_articles.add(full_url)
            
            return links
        except Exception as e:
            return []
    
    async def fetch_article_content(self, url: str) -> Optional[Dict]:
        """ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    html_content = await response.text()
                    return self.extract_article_content(html_content, url)
                return None
        except Exception as e:
            return None
    
    def extract_article_content(self, html_content: str, url: str) -> Optional[Dict]:
        """ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ë° ì •ë¦¬"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        try:
            # ì œëª© ì¶”ì¶œ
            title = ""
            title_elem = soup.find('span', alt=True)
            if title_elem and title_elem.get('alt'):
                title = title_elem['alt'].replace('&quot;', '"').strip()
            
            if not title:
                title_elem = soup.find('h1') or soup.find('h2') or soup.find('h3')
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = ""
            content_elem = soup.select_one('div.news_txt')
            if content_elem:
                content = content_elem.get_text(strip=True)
            
            if not content:
                content_elem = soup.find('div', class_=re.compile(r'content|body|text|article'))
                if content_elem:
                    content = content_elem.get_text(strip=True)
            
            # ë‚ ì§œ ì¶”ì¶œ
            publish_date = None
            date_elem = soup.find('span', class_='input')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                date_match = re.search(r'(\d{4}-\d{2}-\d{2})', date_text)
                if date_match:
                    publish_date = date_match.group(1)
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            unwanted_patterns = [
                r'ì…ë ¥\s+\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}',
                r'ìˆ˜ì •\s+\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}',
                r'ê¸°ì‚¬ì œê³µ\s+[^\n]*',
                r'ì €ì‘ê¶Œì\s+[^\n]*'
            ]
            
            for pattern in unwanted_patterns:
                content = re.sub(pattern, '', content)
            
            content = re.sub(r'\s+', ' ', content).strip()
            content = re.sub(r'\n+', '\n', content)
            
            if not title or not content:
                return None
            
            return {
                'title': title,
                'content': content,
                'publish_date': publish_date,
                'url': url
            }
            
        except Exception as e:
            return None
    
    async def save_to_supabase(self, article_data: Dict) -> bool:
        """ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥ (ì˜¬ë°”ë¥¸ í…Œì´ë¸” êµ¬ì¡° ì‚¬ìš©)"""
        try:
            # ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            media_outlet = await self.get_media_outlet()
            if not media_outlet:
                self.console.print("[red]ë¯¸ë””ì–´ ì•„ìš¸ë › ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return False
            
            # ê¸°ì¡´ ê¸°ì‚¬ í™•ì¸
            existing = self.supabase.client.table('articles').select('id').eq('url', article_data['url']).execute()
            
            if existing.data:
                # ê¸°ì¡´ ê¸°ì‚¬ ì—…ë°ì´íŠ¸
                self.supabase.client.table('articles').update({
                    'title': article_data['title'],
                    'content': article_data['content'],
                    'published_at': article_data['publish_date']
                }).eq('url', article_data['url']).execute()
                
                self.console.print(f"[yellow]ê¸°ì¡´ ê¸°ì‚¬ ì—…ë°ì´íŠ¸: {article_data['title'][:50]}...[/yellow]")
                return True
            else:
                # ìƒˆ ê¸°ì‚¬ ì‚½ì… (ì˜¬ë°”ë¥¸ í…Œì´ë¸” êµ¬ì¡° ì‚¬ìš©)
                insert_data = {
                    'issue_id': self.issue_id,
                    'media_id': self.media_id,
                    'title': article_data['title'],
                    'url': article_data['url'],
                    'content': article_data['content'],
                    'bias': media_outlet.get('bias', 'center'),  # media_outlets í…Œì´ë¸”ì˜ bias ì‚¬ìš©
                    'published_at': article_data['publish_date']
                }
                
                result = self.supabase.client.table('articles').insert(insert_data).execute()
                if result.data:
                    self.console.print(f"[green]ê¸°ì‚¬ ì €ì¥ ì„±ê³µ: {article_data['title'][:50]}...[/green]")
                    return True
                else:
                    self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì‹¤íŒ¨: {article_data['title'][:50]}...[/red]")
                    return False
                
        except Exception as e:
            self.console.print(f"[red]ê¸°ì‚¬ ì €ì¥ ì˜¤ë¥˜: {str(e)}[/red]")
            return False
    
    async def run(self):
        """í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
        self.stats['start_time'] = time.time()
        
        self.console.print(Panel.fit(
            "[bold cyan]MBC ì •ì¹˜ í¬ë¡¤ëŸ¬ ì‹œì‘[/bold cyan]\n"
            f"ëª©í‘œ: {self.max_articles}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘\n"
            f"URL: {self.politics_url}\n"
            f"ì´ìŠˆ ID: {self.issue_id}, ë¯¸ë””ì–´ ID: {self.media_id}",
            title="ğŸš€ í¬ë¡¤ëŸ¬ ì •ë³´"
        ))
        
        try:
            # 1. ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘
            self.console.print("\n[bold yellow]1ë‹¨ê³„: ê¸°ì‚¬ ë§í¬ ìˆ˜ì§‘[/bold yellow]")
            article_links = await self.get_politics_article_links()
            
            if not article_links:
                self.console.print("[red]ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/red]")
                return
            
            self.stats['total_found'] = len(article_links)
            self.console.print(f"[green]ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.[/green]")
            
            # 2. ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ë° ì €ì¥
            self.console.print("\n[bold yellow]2ë‹¨ê³„: ê¸°ì‚¬ ë‚´ìš© ìˆ˜ì§‘ ë° ì €ì¥[/bold yellow]")
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                console=self.console
            ) as progress:
                
                task = progress.add_task("ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘...", total=len(article_links))
                
                for i, url in enumerate(article_links):
                    if len(self.articles) >= self.max_articles:
                        break
                    
                    article_data = await self.fetch_article_content(url)
                    
                    if article_data:
                        if await self.save_to_supabase(article_data):
                            self.articles.append(article_data)
                            self.stats['successful'] += 1
                        else:
                            self.stats['failed'] += 1
                    else:
                        self.stats['failed'] += 1
                    
                    progress.update(task, advance=1)
                    
                    if (i + 1) % 20 == 0:
                        self.console.print(f"[cyan]ì§„í–‰ë¥ : {i + 1}/{len(article_links)} (ì„±ê³µ: {self.stats['successful']}, ì‹¤íŒ¨: {self.stats['failed']})[/cyan]")
                    
                    await asyncio.sleep(self.delay)
            
            # 3. ê²°ê³¼ ìš”ì•½
            self.stats['end_time'] = time.time()
            duration = self.stats['end_time'] - self.stats['start_time']
            
            self.console.print("\n" + "="*60)
            self.console.print(Panel.fit(
                f"[bold green]í¬ë¡¤ë§ ì™„ë£Œ![/bold green]\n\n"
                f"ğŸ“Š ìˆ˜ì§‘ ê²°ê³¼:\n"
                f"  â€¢ ì´ ë°œê²¬: {self.stats['total_found']}ê°œ\n"
                f"  â€¢ ì„±ê³µ: {self.stats['successful']}ê°œ\n"
                f"  â€¢ ì‹¤íŒ¨: {self.stats['failed']}ê°œ\n"
                f"  â€¢ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ\n"
                f"  â€¢ ì†ë„: {self.stats['successful']/duration:.1f}ê°œ/ì´ˆ",
                title=" ìµœì¢… ê²°ê³¼"
            ))
            
            if duration > 20:
                self.console.print("[yellow]âš ï¸  ëª©í‘œ ì‹œê°„(20ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.[/yellow]")
            else:
                self.console.print("[green]âœ… ëª©í‘œ ì‹œê°„(20ì´ˆ) ë‚´ì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤![/green]")
                
        except Exception as e:
            self.console.print(f"[red]í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}[/red]")
            logger.error(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")


    async def collect_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ìˆ˜ì§‘ (í‘œì¤€ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            result = await self.run()
            if hasattr(self, 'articles') and self.articles:
                return self.articles
            elif result:
                return result if isinstance(result, list) else []
            else:
                return []
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")
            return getattr(self, 'articles', [])

async def main():
    async with MBCPoliticsCrawler() as crawler:
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(asyncio.run(main()))
