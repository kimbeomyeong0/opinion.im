import asyncio
import aiohttp
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.table import Table
from rich.panel import Panel
from rich.live import Live
from rich.layout import Layout
import hashlib
from typing import List, Dict, Set, Optional
import logging
import json
from datetime import datetime
import re

class AdvancedNewsCrawler:
    def __init__(self, config: Dict):
        self.config = config
        self.console = Console()
        self.session = None
        self.visited_urls: Set[str] = set()
        self.news_data: List[Dict] = []
        self.duplicate_checker = set()
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_urls': 0,
            'successful_crawls': 0,
            'failed_crawls': 0,
            'total_news': 0
        }
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('crawler.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def __aenter__(self):
        timeout_config = aiohttp.ClientTimeout(total=self.config['timeout'])
        connector = aiohttp.TCPConnector(limit=100, limit_per_host=30)
        self.session = aiohttp.ClientSession(
            timeout=timeout_config,
            connector=connector,
            headers={'User-Agent': self.config['user_agent']}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _generate_hash(self, title: str, content: str) -> str:
        """í–¥ìƒëœ ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ í•´ì‹œ ìƒì„±"""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ì •ê·œí™”
        normalized_title = re.sub(r'[^\w\s]', '', title.lower().strip())
        normalized_content = re.sub(r'[^\w\s]', '', content.lower().strip())
        
        # ì œëª©ê³¼ ë‚´ìš©ì˜ ì²« 100ìë§Œ ì‚¬ìš©í•˜ì—¬ í•´ì‹œ ìƒì„±
        text = f"{normalized_title[:100]}{normalized_content[:100]}"
        return hashlib.md5(text.encode()).hexdigest()
    
    def _is_duplicate(self, title: str, content: str) -> bool:
        """í–¥ìƒëœ ì¤‘ë³µ ë‰´ìŠ¤ ì²´í¬"""
        content_hash = self._generate_hash(title, content)
        if content_hash in self.duplicate_checker:
            return True
        self.duplicate_checker.add(content_hash)
        return False
    
    async def fetch_page(self, url: str, retries: int = 0) -> Optional[str]:
        """ì›¹í˜ì´ì§€ ë¹„ë™ê¸° í˜ì¹­ with ì¬ì‹œë„ ë¡œì§"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                elif response.status == 429:  # Rate limit
                    if retries < self.config['max_retries']:
                        await asyncio.sleep(2 ** retries)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        return await self.fetch_page(url, retries + 1)
                    else:
                        self.logger.warning(f"Rate limit exceeded: {url}")
                        return None
                else:
                    self.logger.warning(f"HTTP {response.status}: {url}")
                    return None
        except asyncio.TimeoutError:
            if retries < self.config['max_retries']:
                await asyncio.sleep(1)
                return await self.fetch_page(url, retries + 1)
            else:
                self.logger.error(f"íƒ€ì„ì•„ì›ƒ: {url}")
                return None
        except Exception as e:
            self.logger.error(f"ì—ëŸ¬ ë°œìƒ {url}: {str(e)}")
            return None
    
    def parse_news(self, html: str, base_url: str) -> List[Dict]:
        """í–¥ìƒëœ HTML íŒŒì‹±"""
        soup = BeautifulSoup(html, 'lxml')
        news_items = []
        
        # ë” ì •êµí•œ ì„ íƒì íŒ¨í„´
        selectors = [
            # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ êµ¬ì¡°
            'article', '.news-item', '.article', '.post', '.story',
            '[class*="news"]', '[class*="article"]', '[class*="post"]', '[class*="story"]',
            # ì–¸ë¡ ì‚¬ë³„ íŠ¹í™” ì„ íƒì
            '.news-list li', '.article-list .item', '.post-list .post',
            '.news-content', '.article-content', '.post-content'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    try:
                        # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
                        title = self._extract_title(element)
                        
                        # ë‚´ìš© ì¶”ì¶œ
                        content = self._extract_content(element)
                        
                        # ë§í¬ ì¶”ì¶œ
                        link = self._extract_link(element, base_url)
                        
                        # ë‚ ì§œ ì¶”ì¶œ
                        date = self._extract_date(element)
                        
                        # ì´ë¯¸ì§€ ì¶”ì¶œ
                        image = self._extract_image(element, base_url)
                        
                        if title and content and not self._is_duplicate(title, content):
                            news_items.append({
                                'title': title,
                                'content': content[:300] + '...' if len(content) > 300 else content,
                                'link': link,
                                'source': urlparse(base_url).netloc,
                                'date': date,
                                'image': image,
                                'crawled_at': datetime.now().isoformat()
                            })
                    except Exception as e:
                        self.logger.debug(f"íŒŒì‹± ì—ëŸ¬: {str(e)}")
                        continue
        
        return news_items
    
    def _extract_title(self, element) -> str:
        """ì œëª© ì¶”ì¶œ ë¡œì§"""
        # ìš°ì„ ìˆœìœ„ë³„ ì œëª© ì¶”ì¶œ
        title_selectors = [
            'h1', 'h2', 'h3', 'h4',
            '[class*="title"]', '[class*="headline"]', '[class*="subject"]',
            '.title', '.headline', '.subject'
        ]
        
        for selector in title_selectors:
            title_elem = element.select_one(selector)
            if title_elem:
                title = title_elem.get_text(strip=True)
                if title and len(title) > 5:  # ì˜ë¯¸ìˆëŠ” ì œëª©ì¸ì§€ í™•ì¸
                    return title
        
        return ""
    
    def _extract_content(self, element) -> str:
        """ë‚´ìš© ì¶”ì¶œ ë¡œì§"""
        # ìš°ì„ ìˆœìœ„ë³„ ë‚´ìš© ì¶”ì¶œ
        content_selectors = [
            '[class*="content"]', '[class*="body"]', '[class*="text"]',
            '.content', '.body', '.text', 'p'
        ]
        
        for selector in content_selectors:
            content_elem = element.select_one(selector)
            if content_elem:
                content = content_elem.get_text(strip=True)
                if content and len(content) > 20:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                    return content
        
        return ""
    
    def _extract_link(self, element, base_url: str) -> str:
        """ë§í¬ ì¶”ì¶œ ë¡œì§"""
        link_elem = element.find('a')
        if link_elem and link_elem.get('href'):
            return urljoin(base_url, link_elem.get('href'))
        return ""
    
    def _extract_date(self, element) -> str:
        """ë‚ ì§œ ì¶”ì¶œ ë¡œì§"""
        date_selectors = [
            '[class*="date"]', '[class*="time"]', '[datetime]',
            '.date', '.time', 'time'
        ]
        
        for selector in date_selectors:
            date_elem = element.select_one(selector)
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                if date_text:
                    return date_text
                # datetime ì†ì„± í™•ì¸
                datetime_attr = date_elem.get('datetime')
                if datetime_attr:
                    return datetime_attr
        
        return ""
    
    def _extract_image(self, element, base_url: str) -> str:
        """ì´ë¯¸ì§€ ì¶”ì¶œ ë¡œì§"""
        img_elem = element.find('img')
        if img_elem and img_elem.get('src'):
            return urljoin(base_url, img_elem.get('src'))
        return ""
    
    async def crawl_urls(self, urls: List[str]) -> List[Dict]:
        """URL ëª©ë¡ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        self.stats['start_time'] = datetime.now()
        self.stats['total_urls'] = len(urls)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            console=self.console
        ) as progress:
            
            task = progress.add_task("í¬ë¡¤ë§ ì§„í–‰ë¥ ...", total=len(urls))
            
            # ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
                loop = asyncio.get_event_loop()
                futures = []
                
                for url in urls:
                    if url not in self.visited_urls:
                        self.visited_urls.add(url)
                        future = loop.run_in_executor(executor, lambda u=url: asyncio.run(self._crawl_single_url(u)))
                        futures.append(future)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in asyncio.as_completed(futures):
                    try:
                        result = await future
                        if result:
                            self.news_data.extend(result)
                            self.stats['successful_crawls'] += 1
                        else:
                            self.stats['failed_crawls'] += 1
                        progress.advance(task)
                        
                        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                        progress.update(task, description=f"í¬ë¡¤ë§ ì§„í–‰ë¥ ... ({len(self.news_data)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘)")
                        
                    except Exception as e:
                        self.logger.error(f"í¬ë¡¤ë§ ì—ëŸ¬: {str(e)}")
                        self.stats['failed_crawls'] += 1
                        progress.advance(task)
        
        self.stats['end_time'] = datetime.now()
        self.stats['total_news'] = len(self.news_data)
        
        elapsed_time = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        self.console.print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        return self.news_data
    
    async def _crawl_single_url(self, url: str) -> List[Dict]:
        """ë‹¨ì¼ URL í¬ë¡¤ë§"""
        html = await self.fetch_page(url)
        if html:
            await asyncio.sleep(self.config['delay_between_requests'])  # ìš”ì²­ ê°„ ì§€ì—°
            return self.parse_news(html, url)
        return []
    
    def display_results(self):
        """í–¥ìƒëœ ê²°ê³¼ í‘œì‹œ"""
        if not self.news_data:
            self.console.print("âŒ í¬ë¡¤ë§ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í†µê³„ ì •ë³´
        elapsed_time = (self.stats['end_time'] - self.stats['start_time']).total_seconds()
        stats_panel = Panel(
            f"ì´ {self.stats['total_news']}ê°œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘\n"
            f"ì„±ê³µ: {self.stats['successful_crawls']}ê°œ | ì‹¤íŒ¨: {self.stats['failed_crawls']}ê°œ\n"
            f"ì¤‘ë³µ ì œê±°: {len(self.duplicate_checker)}ê°œ\n"
            f"ë°©ë¬¸í•œ URL: {len(self.visited_urls)}ê°œ\n"
            f"ì†Œìš”ì‹œê°„: {elapsed_time:.2f}ì´ˆ",
            title="ğŸ“Š í¬ë¡¤ë§ í†µê³„",
            border_style="blue"
        )
        self.console.print(stats_panel)
        
        # ë‰´ìŠ¤ í…Œì´ë¸”
        table = Table(title="ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤")
        table.add_column("ì œëª©", style="cyan", width=35)
        table.add_column("ë‚´ìš©", style="white", width=50)
        table.add_column("ì¶œì²˜", style="green", width=15)
        table.add_column("ë‚ ì§œ", style="yellow", width=15)
        table.add_column("ë§í¬", style="blue", width=25)
        
        for news in self.news_data[:25]:  # ìƒìœ„ 25ê°œ í‘œì‹œ
            table.add_row(
                news['title'][:35] + '...' if len(news['title']) > 35 else news['title'],
                news['content'][:50] + '...' if len(news['content']) > 50 else news['content'],
                news['source'],
                news['date'][:15] if news['date'] else "N/A",
                news['link'][:25] + '...' if len(news['link']) > 25 else news['link']
            )
        
        self.console.print(table)
    
    def save_results(self, filename: str = None):
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"crawled_news_{timestamp}.json"
        
        data_to_save = {
            'metadata': {
                'crawled_at': datetime.now().isoformat(),
                'stats': self.stats,
                'config': self.config
            },
            'news_data': self.news_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)
        
        self.console.print(f"ğŸ’¾ ê²°ê³¼ê°€ '{filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    from config import CRAWLER_CONFIG, NEWS_SOURCES
    
    console = Console()
    console.print(Panel("ğŸš€ ê³ ê¸‰ ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì‹œì‘", style="bold blue"))
    
    # ëª¨ë“  ì–¸ë¡ ì‚¬ URL ìˆ˜ì§‘
    all_urls = []
    for category, urls in NEWS_SOURCES.items():
        all_urls.extend(urls)
        console.print(f"ğŸ“° {category}: {len(urls)}ê°œ ì–¸ë¡ ì‚¬")
    
    console.print(f"\nì´ {len(all_urls)}ê°œ ì–¸ë¡ ì‚¬ì—ì„œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n")
    
    async with AdvancedNewsCrawler(CRAWLER_CONFIG) as crawler:
        news_data = await crawler.crawl_urls(all_urls)
        crawler.display_results()
        crawler.save_results()

if __name__ == "__main__":
    asyncio.run(main())

```

```

