#!/usr/bin/env python3
"""
í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ì¡°ì„ ì¼ë³´ë¶€í„°) - ê°œì„ ëœ ë²„ì „
"""

import asyncio
import subprocess
import time
from datetime import datetime
from rich.console import Console
from rich.panel import Panel

console = Console()

# í¬ë¡¤ëŸ¬ ëª©ë¡ (ì¡°ì„ ì¼ë³´ë¶€í„° ìˆœì„œëŒ€ë¡œ)
CRAWLERS = [
    ("crawlers/major_news/chosun_politics_crawler.py", "ì¡°ì„ ì¼ë³´"),
    ("crawlers/major_news/donga_politics_crawler.py", "ë™ì•„ì¼ë³´"),
    ("crawlers/major_news/joongang_politics_crawler.py", "ì¤‘ì•™ì¼ë³´"),
    ("crawlers/major_news/hani_politics_crawler.py", "í•œê²¨ë ˆ"),
    ("crawlers/major_news/hankyung_politics_crawler.py", "í•œêµ­ê²½ì œ"),
    ("crawlers/major_news/kmib_politics_crawler.py", "êµ­ë¯¼ì¼ë³´"),
    ("crawlers/major_news/khan_politics_crawler.py", "ê²½í–¥ì‹ ë¬¸"),
    ("crawlers/major_news/kbs_politics_api_collector.py", "KBS"),
    ("crawlers/major_news/news1_politics_crawler.py", "ë‰´ìŠ¤1"),
    ("crawlers/major_news/segye_politics_crawler.py", "ì„¸ê³„ì¼ë³´"),
    ("crawlers/major_news/sbs_politics_crawler.py", "SBS"),
    ("crawlers/major_news/mk_politics_crawler.py", "ë§¤ì¼ê²½ì œ"),
    ("crawlers/major_news/yna_politics_crawler.py", "ì—°í•©ë‰´ìŠ¤"),
    ("crawlers/major_news/ytn_politics_crawler.py", "YTN"),
    ("crawlers/major_news/sedaily_politics_crawler.py", "ì„œìš¸ê²½ì œ"),
    ("crawlers/major_news/munhwa_politics_crawler.py", "ë¬¸í™”ì¼ë³´"),
    ("crawlers/major_news/jtbc_politics_collector.py", "JTBC"),
    ("crawlers/broadcasting/mbc_politics_crawler.py", "MBC"),
    ("crawlers/online_news/pressian_politics_crawler.py", "í”„ë ˆì‹œì•ˆ"),
    ("crawlers/online_news/ohmynews_politics_crawler.py", "ì˜¤ë§ˆì´ë‰´ìŠ¤"),
]

def analyze_crawler_output(output: str) -> dict:
    """í¬ë¡¤ëŸ¬ ì¶œë ¥ ê²°ê³¼ ë¶„ì„"""
    result = {
        'success': False,
        'articles_count': 0,
        'error_type': None,
        'details': []
    }
    
    lines = output.split('\n')
    
    # ì„±ê³µ ì§€í‘œë“¤ í™•ì¸
    success_indicators = [
        "ìˆ˜ì§‘ ê²°ê³¼:", "ì´ ìˆ˜ì§‘:", "ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ", "ì €ì¥ ì™„ë£Œ", 
        "í¬ë¡¤ë§ ì™„ë£Œ", "ìˆ˜ì§‘ ì™„ë£Œ", "ì™„ë£Œ", "ì„±ê³µ"
    ]
    
    # ì‹¤íŒ¨ ì§€í‘œë“¤ í™•ì¸
    failure_indicators = [
        "ì˜¤ë¥˜ ë°œìƒ", "ì‹¤íŒ¨", "ì—ëŸ¬", "Error", "Exception", 
        "ì—°ê²° ì‹¤íŒ¨", "ì¸ì¦ ì‹¤íŒ¨", "ê¶Œí•œ ì—†ìŒ"
    ]
    
    # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
    has_success = any(indicator in output for indicator in success_indicators)
    has_failure = any(indicator in output for indicator in failure_indicators)
    
    # ê¸°ì‚¬ ìˆ˜ ì¶”ì¶œ
    articles_count = 0
    for line in lines:
        if "ì´ ìˆ˜ì§‘:" in line:
            try:
                articles_str = line.split("ì´ ìˆ˜ì§‘:")[1].split("ê°œ")[0].strip()
                articles_count = int(articles_str)
                break
            except:
                pass
        elif "ìˆ˜ì§‘ëœ ê¸°ì‚¬:" in line:
            try:
                articles_str = line.split("ìˆ˜ì§‘ëœ ê¸°ì‚¬:")[1].split("ê°œ")[0].strip()
                articles_count = int(articles_str)
                break
            except:
                pass
    
    # ìµœì¢… ì„±ê³µ ì—¬ë¶€ íŒë‹¨
    if has_success and not has_failure and articles_count > 0:
        result['success'] = True
        result['articles_count'] = articles_count
    elif has_success and articles_count > 0:
        result['success'] = True
        result['articles_count'] = articles_count
    elif has_failure:
        result['error_type'] = "ëª…ì‹œì  ì˜¤ë¥˜"
    else:
        result['error_type'] = "ê²°ê³¼ ë¶ˆëª…í™•"
    
    # ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    for line in lines:
        if any(keyword in line for keyword in ["ìˆ˜ì§‘", "ì €ì¥", "ì™„ë£Œ", "ì„±ê³µ", "ì˜¤ë¥˜", "ì‹¤íŒ¨"]):
            result['details'].append(line.strip())
    
    return result

async def run_crawler(crawler_path: str, crawler_name: str, index: int):
    """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    start_time = time.time()
    
    console.print(f"\nğŸš€ [{index:2d}/20] {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘...")
    
    try:
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
        process = await asyncio.create_subprocess_exec(
            "python3", crawler_path,
            env={"PYTHONPATH": "."},
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        duration = time.time() - start_time
        
        # ì¶œë ¥ ê²°ê³¼ ë¶„ì„
        output = stdout.decode('utf-8')
        stderr_output = stderr.decode('utf-8')
        
        # í¬ë¡¤ëŸ¬ ì¶œë ¥ ë¶„ì„
        analysis = analyze_crawler_output(output)
        
        if analysis['success']:
            console.print(f"âœ… {crawler_name}: ì„±ê³µ ({duration:.1f}ì´ˆ)")
            console.print(f"   ğŸ“° {analysis['articles_count']}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            
            # ìƒì„¸ ì •ë³´ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            if analysis['details']:
                for detail in analysis['details'][:2]:  # ìµœëŒ€ 2ê°œë§Œ
                    console.print(f"   â„¹ï¸  {detail}")
                    
        else:
            # stderrì— ì‹¤ì œ ì˜¤ë¥˜ê°€ ìˆëŠ”ì§€ í™•ì¸
            if stderr_output and len(stderr_output.strip()) > 0:
                error_msg = stderr_output[:150] + "..." if len(stderr_output) > 150 else stderr_output
                console.print(f"âŒ {crawler_name}: ì‹¤íŒ¨ ({duration:.1f}ì´ˆ)")
                console.print(f"   ğŸ’¬ {error_msg}")
            else:
                # stdoutì—ì„œ ì˜¤ë¥˜ ì •ë³´ í™•ì¸
                console.print(f"âš ï¸  {crawler_name}: ê²°ê³¼ ë¶ˆëª…í™• ({duration:.1f}ì´ˆ)")
                console.print(f"   ğŸ” {analysis['error_type']}")
                if analysis['details']:
                    for detail in analysis['details'][:2]:
                        console.print(f"   â„¹ï¸  {detail}")
            
    except Exception as e:
        duration = time.time() - start_time
        console.print(f"âŒ {crawler_name}: ì‹¤í–‰ ì˜¤ë¥˜ ({duration:.1f}ì´ˆ) - {str(e)}")

async def run_all_crawlers():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰"""
    console.print(Panel.fit("ğŸ•·ï¸ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ ì‹œì‘ (ì¡°ì„ ì¼ë³´ë¶€í„°)", style="bold blue"))
    console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    console.print("=" * 80)
    
    total_start_time = time.time()
    
    for i, (crawler_path, crawler_name) in enumerate(CRAWLERS, 1):
        await run_crawler(crawler_path, crawler_name, i)
        
        # ë§ˆì§€ë§‰ í¬ë¡¤ëŸ¬ê°€ ì•„ë‹ˆë©´ ëŒ€ê¸°
        if i < len(CRAWLERS):
            console.print("â³ ë‹¤ìŒ í¬ë¡¤ëŸ¬ ì¤€ë¹„ ì¤‘... (3ì´ˆ ëŒ€ê¸°)")
            await asyncio.sleep(3)
    
    total_duration = time.time() - total_start_time
    
    # ê²°ê³¼ ìš”ì•½
    console.print("\n" + "=" * 80)
    console.print("ğŸ“Š í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì™„ë£Œ ìš”ì•½")
    console.print("=" * 80)
    console.print(f"ğŸ¯ ì „ì²´ í¬ë¡¤ëŸ¬: {len(CRAWLERS)}ê°œ")
    console.print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_duration:.1f}ì´ˆ")
    console.print(f"ğŸ“… ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    console.print("\nğŸ’¡ ì°¸ê³ : exit codeê°€ ì•„ë‹Œ ì‹¤ì œ ì¶œë ¥ ë‚´ìš©ìœ¼ë¡œ ì„±ê³µ ì—¬ë¶€ë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(run_all_crawlers())
