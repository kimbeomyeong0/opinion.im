#!/usr/bin/env python3
"""
20ê°œ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ ë° ê²°ê³¼ ë ˆí¬íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import subprocess
import time
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel
import json

console = Console()

# í¬ë¡¤ëŸ¬ ëª©ë¡ (20ê°œ)
CRAWLERS = [
    # Major News (17ê°œ)
    ("crawlers/major_news/chosun_politics_crawler.py", "ì¡°ì„ ì¼ë³´"),
    ("crawlers/major_news/donga_politics_crawler.py", "ë™ì•„ì¼ë³´"),
    ("crawlers/major_news/joongang_politics_crawler.py", "ì¤‘ì•™ì¼ë³´"),
    ("crawlers/major_news/hani_politics_crawler.py", "í•œê²¨ë ˆ"),
    ("crawlers/major_news/hankyung_politics_crawler.py", "í•œêµ­ê²½ì œ"),
    ("crawlers/major_news/kmib_politics_crawler.py", "êµ­ë¯¼ì¼ë³´"),
    ("crawlers/major_news/khan_politics_crawler.py", "ê²½í–¥ì‹ ë¬¸"),
    ("crawlers/major_news/kbs_politics_api_collector.py", "KBS"),
    ("crawlers/major_news/news1_politics_crawler.py", "ë‰´ìŠ¤1"),
    ("crawlers/major_news/segye_politics_crawler.py", "ì„¸ê³„ì¼ë³´"),
    ("crawlers/major_news/sbs_politics_crawler.py", "SBS"),
    ("crawlers/major_news/mk_politics_crawler.py", "ë§¤ì¼ê²½ì œ"),
    ("crawlers/major_news/yna_politics_crawler.py", "ì—°í•©ë‰´ìŠ¤"),
    ("crawlers/major_news/ytn_politics_crawler.py", "YTN"),
    ("crawlers/major_news/sedaily_politics_crawler.py", "ì„œìš¸ê²½ì œ"),
    ("crawlers/major_news/munhwa_politics_crawler.py", "ë¬¸í™”ì¼ë³´"),
    ("crawlers/major_news/jtbc_politics_collector.py", "JTBC"),
    
    # Broadcasting (1ê°œ)
    ("crawlers/broadcasting/mbc_politics_crawler.py", "MBC"),
    
    # Online News (2ê°œ)
    ("crawlers/online_news/pressian_politics_crawler.py", "í”„ë ˆì‹œì•ˆ"),
    ("crawlers/online_news/ohmynews_politics_crawler.py", "ì˜¤ë§ˆì´ë‰´ìŠ¤"),
]

class CrawlerResult:
    def __init__(self, name, path):
        self.name = name
        self.path = path
        self.status = "ëŒ€ê¸°ì¤‘"
        self.start_time = None
        self.end_time = None
        self.duration = None
        self.articles_collected = 0
        self.error_message = None
        self.exit_code = None

async def run_crawler(crawler_path: str, crawler_name: str) -> CrawlerResult:
    """ê°œë³„ í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    result = CrawlerResult(crawler_name, crawler_path)
    result.start_time = datetime.now()
    
    try:
        console.print(f"ğŸš€ {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹œì‘...")
        
        # í¬ë¡¤ëŸ¬ ì‹¤í–‰
        process = await asyncio.create_subprocess_exec(
            "python3", crawler_path,
            env={"PYTHONPATH": "."},
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        result.exit_code = process.returncode
        
        if result.exit_code == 0:
            result.status = "ì„±ê³µ"
            # stdoutì—ì„œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜ ì¶”ì¶œ ì‹œë„
            output = stdout.decode('utf-8')
            if "ìˆ˜ì§‘ ê²°ê³¼:" in output:
                # ìˆ˜ì§‘ ê²°ê³¼ íŒŒì‹±
                lines = output.split('\n')
                for line in lines:
                    if "ì´ ìˆ˜ì§‘:" in line:
                        try:
                            articles_str = line.split("ì´ ìˆ˜ì§‘:")[1].split("ê°œ")[0].strip()
                            result.articles_collected = int(articles_str)
                        except:
                            result.articles_collected = 0
                        break
        else:
            result.status = "ì‹¤íŒ¨"
            result.error_message = stderr.decode('utf-8')[:200] + "..." if len(stderr.decode('utf-8')) > 200 else stderr.decode('utf-8')
            
    except Exception as e:
        result.status = "ì˜¤ë¥˜"
        result.error_message = str(e)
        result.exit_code = -1
    
    result.end_time = datetime.now()
    result.duration = (result.end_time - result.start_time).total_seconds()
    
    return result

async def run_all_crawlers():
    """ëª¨ë“  í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    console.print(Panel.fit("ğŸ•·ï¸ 20ê°œ í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰ ì‹œì‘", style="bold blue"))
    console.print(f"ğŸ“… ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    console.print("=" * 80)
    
    results = []
    total_start_time = datetime.now()
    
    # í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰
    for i, (crawler_path, crawler_name) in enumerate(CRAWLERS, 1):
        console.print(f"\nğŸ“° [{i:2d}/20] {crawler_name} í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘...")
        
        result = await run_crawler(crawler_path, crawler_name)
        results.append(result)
        
        # ê²°ê³¼ ì¶œë ¥
        if result.status == "ì„±ê³µ":
            console.print(f"âœ… {crawler_name}: {result.articles_collected}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ({result.duration:.1f}ì´ˆ)")
        else:
            console.print(f"âŒ {crawler_name}: {result.status} - {result.error_message}")
        
        # í¬ë¡¤ëŸ¬ ê°„ ê°„ê²©
        if i < len(CRAWLERS):
            console.print("â³ ë‹¤ìŒ í¬ë¡¤ëŸ¬ ì¤€ë¹„ ì¤‘... (3ì´ˆ ëŒ€ê¸°)")
            await asyncio.sleep(3)
    
    total_end_time = datetime.now()
    total_duration = (total_end_time - total_start_time).total_seconds()
    
    # ê²°ê³¼ ë ˆí¬íŠ¸ ìƒì„±
    generate_report(results, total_duration)
    
    return results

def generate_report(results, total_duration):
    """ê²°ê³¼ ë ˆí¬íŠ¸ ìƒì„±"""
    console.print("\n" + "=" * 80)
    console.print("ğŸ“Š í¬ë¡¤ëŸ¬ ì‹¤í–‰ ê²°ê³¼ ë ˆí¬íŠ¸")
    console.print("=" * 80)
    
    # ìš”ì•½ í†µê³„
    successful = sum(1 for r in results if r.status == "ì„±ê³µ")
    failed = sum(1 for r in results if r.status == "ì‹¤íŒ¨")
    error = sum(1 for r in results if r.status == "ì˜¤ë¥˜")
    total_articles = sum(r.articles_collected for r in results if r.status == "ì„±ê³µ")
    
    console.print(f"ğŸ¯ ì „ì²´ í¬ë¡¤ëŸ¬: {len(results)}ê°œ")
    console.print(f"âœ… ì„±ê³µ: {successful}ê°œ")
    console.print(f"âŒ ì‹¤íŒ¨: {failed}ê°œ")
    console.print(f"âš ï¸ ì˜¤ë¥˜: {error}ê°œ")
    console.print(f"ğŸ“° ì´ ìˆ˜ì§‘ ê¸°ì‚¬: {total_articles:,}ê°œ")
    console.print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_duration:.1f}ì´ˆ")
    
    # ìƒì„¸ ê²°ê³¼ í…Œì´ë¸”
    table = Table(title="í¬ë¡¤ëŸ¬ë³„ ìƒì„¸ ê²°ê³¼")
    table.add_column("ìˆœë²ˆ", style="cyan", no_wrap=True)
    table.add_column("ì–¸ë¡ ì‚¬", style="magenta")
    table.add_column("ìƒíƒœ", style="bold")
    table.add_column("ìˆ˜ì§‘ ê¸°ì‚¬", style="green")
    table.add_column("ì†Œìš” ì‹œê°„", style="yellow")
    table.add_column("ë¹„ê³ ", style="red")
    
    for i, result in enumerate(results, 1):
        status_style = {
            "ì„±ê³µ": "âœ…",
            "ì‹¤íŒ¨": "âŒ",
            "ì˜¤ë¥˜": "âš ï¸"
        }.get(result.status, "â“")
        
        articles = f"{result.articles_collected:,}ê°œ" if result.articles_collected > 0 else "-"
        duration = f"{result.duration:.1f}ì´ˆ" if result.duration else "-"
        note = result.error_message if result.error_message else "-"
        
        table.add_row(
            str(i),
            result.name,
            f"{status_style} {result.status}",
            articles,
            duration,
            note
        )
    
    console.print(table)
    
    # ì„±ê³µ/ì‹¤íŒ¨ ìš”ì•½
    console.print("\nğŸ“ˆ ì„±ê³µí•œ í¬ë¡¤ëŸ¬:")
    for result in results:
        if result.status == "ì„±ê³µ":
            console.print(f"   âœ… {result.name}: {result.articles_collected:,}ê°œ ê¸°ì‚¬")
    
    if failed > 0 or error > 0:
        console.print("\nâŒ ì‹¤íŒ¨í•œ í¬ë¡¤ëŸ¬:")
        for result in results:
            if result.status in ["ì‹¤íŒ¨", "ì˜¤ë¥˜"]:
                console.print(f"   âŒ {result.name}: {result.error_message}")
    
    # JSON ê²°ê³¼ ì €ì¥
    report_data = {
        "summary": {
            "total_crawlers": len(results),
            "successful": successful,
            "failed": failed,
            "error": error,
            "total_articles": total_articles,
            "total_duration": total_duration,
            "start_time": total_start_time.isoformat(),
            "end_time": total_end_time.isoformat()
        },
        "results": [
            {
                "name": r.name,
                "path": r.path,
                "status": r.status,
                "articles_collected": r.articles_collected,
                "duration": r.duration,
                "error_message": r.error_message,
                "exit_code": r.exit_code
            }
            for r in results
        ]
    }
    
    with open("crawler_report.json", "w", encoding="utf-8") as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ 'crawler_report.json' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(run_all_crawlers())
