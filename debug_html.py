#!/usr/bin/env python3
"""
HTML êµ¬ì¡° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import httpx
import re
from bs4 import BeautifulSoup
from rich.console import Console

console = Console()

async def debug_html_structure():
    """HTML êµ¬ì¡° ë””ë²„ê¹…"""
    
    url = "https://www.chosun.com/politics/politics_general/2025/08/19/MX46I5KXANFQZICDOXZAUV3VMU/"
    
    console.print(f"ğŸ” URL: {url}")
    console.print("=" * 80)
    
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url)
            html = response.text
            
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. HTML ì „ì²´ ê¸¸ì´
        console.print(f"ğŸ“Š HTML ì „ì²´ ê¸¸ì´: {len(html)}ì")
        
        # 2. ëª¨ë“  íƒœê·¸ ë¶„ì„
        console.print("\nğŸ“‹ 2. ëª¨ë“  íƒœê·¸ ë¶„ì„:")
        all_tags = soup.find_all()
        tag_counts = {}
        for tag in all_tags:
            tag_name = tag.name
            if tag_name not in tag_counts:
                tag_counts[tag_name] = 0
            tag_counts[tag_name] += 1
        
        for tag_name, count in sorted(tag_counts.items()):
            console.print(f"   - {tag_name}: {count}ê°œ")
        
        # 3. script íƒœê·¸ ë‚´ìš© ë¶„ì„
        console.print("\nğŸ“‹ 3. Script íƒœê·¸ ë¶„ì„:")
        scripts = soup.find_all('script')
        console.print(f"ì´ script íƒœê·¸: {len(scripts)}ê°œ")
        
        for i, script in enumerate(scripts):
            if script.string:
                script_content = script.string
                console.print(f"   - script[{i}]: {len(script_content)}ì")
                
                # ë³¸ë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
                keywords = ['content', 'body', 'article', 'text', 'ë³¸ë¬¸', 'ê¸°ì‚¬']
                for keyword in keywords:
                    if keyword in script_content:
                        console.print(f"     âœ… '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                
                # JSON ë°ì´í„° ê²€ìƒ‰
                if 'content_elements' in script_content or 'articleBody' in script_content:
                    console.print(f"     ğŸ” ë³¸ë¬¸ ê´€ë ¨ ë°ì´í„° ë°œê²¬!")
                    
                    # JSON ë¶€ë¶„ ì¶”ì¶œ ì‹œë„
                    json_match = re.search(r'\{.*"content_elements".*\}', script_content, re.DOTALL)
                    if json_match:
                        console.print(f"     ğŸ“„ JSON ë°ì´í„° ê¸¸ì´: {len(json_match.group())}ì")
        
        # 4. div íƒœê·¸ ì¤‘ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë“¤
        console.print("\nğŸ“‹ 4. í…ìŠ¤íŠ¸ê°€ ìˆëŠ” div íƒœê·¸ë“¤:")
        text_divs = []
        for div in soup.find_all('div'):
            text = div.get_text(strip=True)
            if len(text) > 50:  # 50ì ì´ìƒ
                classes = ' '.join(div.get('class', []))
                text_divs.append((classes, len(text), text[:150]))
        
        text_divs.sort(key=lambda x: x[1], reverse=True)
        
        for i, (classes, length, text) in enumerate(text_divs[:10]):
            console.print(f"   {i+1}. div.{classes}: {length}ì")
            console.print(f"      ë‚´ìš©: {text}...")
        
        # 5. íŠ¹ì • í´ë˜ìŠ¤ë‚˜ IDë¥¼ ê°€ì§„ ìš”ì†Œë“¤
        console.print("\nğŸ“‹ 5. íŠ¹ì • í´ë˜ìŠ¤/ID ìš”ì†Œë“¤:")
        interesting_selectors = [
            '[class*="article"]',
            '[class*="content"]',
            '[class*="body"]',
            '[class*="text"]',
            '[id*="article"]',
            '[id*="content"]'
        ]
        
        for selector in interesting_selectors:
            elements = soup.select(selector)
            if elements:
                console.print(f"   âœ… {selector}: {len(elements)}ê°œ")
                for elem in elements[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                    text = elem.get_text(strip=True)
                    console.print(f"      - {elem.name}.{'.'.join(elem.get('class', []))}: {len(text)}ì")
            else:
                console.print(f"   âŒ {selector}: ì—†ìŒ")
        
        # 6. HTML ì „ì²´ì—ì„œ ë³¸ë¬¸ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰
        console.print("\nğŸ“‹ 6. ë³¸ë¬¸ ê´€ë ¨ í…ìŠ¤íŠ¸ ê²€ìƒ‰:")
        full_text = soup.get_text()
        
        # ë³¸ë¬¸ìœ¼ë¡œ ë³´ì´ëŠ” ê¸´ í…ìŠ¤íŠ¸ ì°¾ê¸°
        sentences = re.split(r'[.!?]', full_text)
        long_sentences = [s.strip() for s in sentences if len(s.strip()) > 100]
        
        console.print(f"100ì ì´ìƒ ë¬¸ì¥: {len(long_sentences)}ê°œ")
        for i, sentence in enumerate(long_sentences[:5]):
            console.print(f"   {i+1}. {sentence[:200]}...")
        
    except Exception as e:
        console.print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    asyncio.run(debug_html_structure())
