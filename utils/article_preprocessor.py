#!/usr/bin/env python3
"""
ê¸°ì‚¬ ì „ì²˜ë¦¬ ëª¨ë“ˆ
- ì¤‘ë³µ ì œê±° (URL + media_id, content ìœ ì‚¬ë„)
- ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬
- ë‚ ì§œ í˜•ì‹ í†µì¼
"""

import logging
from typing import List, Dict, Tuple, Set
from datetime import datetime
import hashlib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn
from rich.panel import Panel
from rich.table import Table

try:
    from utils.supabase_manager_unified import UnifiedSupabaseManager
except ImportError:
    from supabase_manager_unified import UnifiedSupabaseManager

class ArticlePreprocessor:
    """ê¸°ì‚¬ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.console = Console()
        self.supabase = UnifiedSupabaseManager()
        self.logger = logging.getLogger(__name__)
        
        # TF-IDF ë²¡í„°ë¼ì´ì € ì´ˆê¸°í™”
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=None,  # í•œêµ­ì–´ëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”
            ngram_range=(1, 2)
        )
        
        # ì¤‘ë³µ ì œê±° í†µê³„
        self.stats = {
            'total_articles': 0,
            'duplicate_url_media': 0,
            'duplicate_content_exact': 0,
            'duplicate_content_similar': 0,
            'short_content_removed': 0,
            'final_articles': 0
        }
    
    def preprocess_articles(self) -> bool:
        """ê¸°ì‚¬ ì „ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        if not self.supabase.is_connected():
            self.console.print("[red]Supabaseì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.[/red]")
            return False
        
        try:
            self.console.print(Panel("ğŸ” ê¸°ì‚¬ ì „ì²˜ë¦¬ ì‹œì‘", style="blue"))
            
            # 1. ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ
            articles = self._fetch_all_articles()
            if not articles:
                self.console.print("[yellow]ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
                return True
            
            self.stats['total_articles'] = len(articles)
            self.console.print(f"[green]ì´ {len(articles)}ê°œ ê¸°ì‚¬ ì¡°íšŒ ì™„ë£Œ[/green]")
            
            # 2. URL + media_id ì¤‘ë³µ ì œê±°
            articles = self._remove_url_media_duplicates(articles)
            
            # 3. ê°™ì€ ì–¸ë¡ ì‚¬ ë‚´ content ì¤‘ë³µ ì œê±°
            articles = self._remove_content_duplicates(articles)
            
            # 4. ì§§ì€ ê¸°ì‚¬ ì œê±°
            articles = self._remove_short_articles(articles)
            
            # 5. ë‚ ì§œ í˜•ì‹ í†µì¼
            articles = self._normalize_dates(articles)
            
            # 6. ìµœì¢… ê²°ê³¼ë¥¼ Supabaseì— ë°˜ì˜
            success = self._update_supabase(articles)
            
            # 7. ê²°ê³¼ ì¶œë ¥
            self._display_results()
            
            return success
            
        except Exception as e:
            self.logger.error(f"ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            self.console.print(f"[red]ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}[/red]")
            return False
    
    def _fetch_all_articles(self) -> List[Dict]:
        """ëª¨ë“  ê¸°ì‚¬ ì¡°íšŒ"""
        try:
            result = self.supabase.client.table('articles').select('*').execute()
            return result.data if result.data else []
        except Exception as e:
            self.logger.error(f"ê¸°ì‚¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _remove_url_media_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """URL + media_id ì¤‘ë³µ ì œê±°"""
        self.console.print("[blue]URL + media_id ì¤‘ë³µ ì œê±° ì¤‘...[/blue]")
        
        seen = set()
        unique_articles = []
        
        for article in articles:
            url = article.get('url', '')
            media_id = article.get('media_id')
            
            if not url or media_id is None:
                continue
                
            key = (url, media_id)
            if key not in seen:
                seen.add(key)
                unique_articles.append(article)
            else:
                self.stats['duplicate_url_media'] += 1
        
        self.console.print(f"[green]URL ì¤‘ë³µ ì œê±° ì™„ë£Œ: {self.stats['duplicate_url_media']}ê°œ ì œê±°[/green]")
        return unique_articles
    
    def _remove_content_duplicates(self, articles: List[Dict]) -> List[Dict]:
        """ê°™ì€ ì–¸ë¡ ì‚¬ ë‚´ content ì¤‘ë³µ ì œê±°"""
        self.console.print("[blue]Content ì¤‘ë³µ ì œê±° ì¤‘...[/blue]")
        
        # ì–¸ë¡ ì‚¬ë³„ë¡œ ê·¸ë£¹í™”
        media_groups = {}
        for article in articles:
            media_id = article.get('media_id')
            if media_id is not None:
                if media_id not in media_groups:
                    media_groups[media_id] = []
                media_groups[media_id].append(article)
        
        unique_articles = []
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=self.console
        ) as progress:
            task = progress.add_task("ì–¸ë¡ ì‚¬ë³„ ì¤‘ë³µ ì œê±°...", total=len(media_groups))
            
            for media_id, media_articles in media_groups.items():
                progress.update(task, description=f"ì–¸ë¡ ì‚¬ {media_id} ì²˜ë¦¬ ì¤‘...")
                
                # ì™„ì „íˆ ë™ì¼í•œ content ì œê±°
                content_hash_map = {}
                for article in media_articles:
                    content = article.get('content', '')
                    if not content:
                        continue
                    
                    content_hash = hashlib.md5(content.encode('utf-8')).hexdigest()
                    if content_hash not in content_hash_map:
                        content_hash_map[content_hash] = article
                    else:
                        self.stats['duplicate_content_exact'] += 1
                
                # ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
                similar_articles = self._remove_similar_content(list(content_hash_map.values()))
                unique_articles.extend(similar_articles)
                
                progress.advance(task)
        
        self.console.print(f"[green]Content ì¤‘ë³µ ì œê±° ì™„ë£Œ: ì •í™• ì¤‘ë³µ {self.stats['duplicate_content_exact']}ê°œ, ìœ ì‚¬ ì¤‘ë³µ {self.stats['duplicate_content_similar']}ê°œ ì œê±°[/green]")
        return unique_articles
    
    def _remove_similar_content(self, articles: List[Dict]) -> List[Dict]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°™ì€ ì–¸ë¡ ì‚¬ ë‚´ì—ì„œë§Œ)"""
        if len(articles) <= 1:
            return articles
        
        try:
            # TF-IDF ë²¡í„°í™”
            contents = [article.get('content', '') for article in articles]
            tfidf_matrix = self.vectorizer.fit_transform(contents)
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity_matrix = cosine_similarity(tfidf_matrix)
            
            # ìœ ì‚¬ë„ê°€ 0.95 ì´ìƒì¸ ìŒ ì°¾ê¸°
            to_remove = set()
            
            for i in range(len(similarity_matrix)):
                for j in range(i + 1, len(similarity_matrix)):
                    if similarity_matrix[i][j] >= 0.95:
                        # ë” ì§§ì€ ê¸°ì‚¬ë‚˜ ë” ëŠ¦ê²Œ ë°œí–‰ëœ ê¸°ì‚¬ë¥¼ ì œê±° ëŒ€ìƒìœ¼ë¡œ ì„ íƒ
                        article_i = articles[i]
                        article_j = articles[j]
                        
                        # ë°œí–‰ì¼ ë¹„êµ
                        date_i = self._parse_date(article_i.get('published_at'))
                        date_j = self._parse_date(article_j.get('published_at'))
                        
                        if date_i and date_j:
                            if date_i >= date_j:  # ë” ëŠ¦ê±°ë‚˜ ê°™ì€ ì‹œê°„ì˜ ê¸°ì‚¬ ì œê±°
                                to_remove.add(j)
                            else:
                                to_remove.add(i)
                        else:
                            # ë°œí–‰ì¼ì´ ì—†ìœ¼ë©´ ë” ì§§ì€ ê¸°ì‚¬ ì œê±°
                            if len(article_i.get('content', '')) <= len(article_j.get('content', '')):
                                to_remove.add(i)
                            else:
                                to_remove.add(j)
            
            # ì œê±° ëŒ€ìƒì´ ì•„ë‹Œ ê¸°ì‚¬ë“¤ë§Œ ë°˜í™˜
            unique_articles = [articles[i] for i in range(len(articles)) if i not in to_remove]
            self.stats['duplicate_content_similar'] += len(articles) - len(unique_articles)
            
            return unique_articles
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return articles
    
    def _remove_short_articles(self, articles: List[Dict]) -> List[Dict]:
        """ì§§ì€ ê¸°ì‚¬ ì œê±° (ë³¸ë¬¸ ê¸¸ì´ < 50ì)"""
        self.console.print("[blue]ì§§ì€ ê¸°ì‚¬ ì œê±° ì¤‘...[/blue]")
        
        filtered_articles = []
        for article in articles:
            content = article.get('content', '')
            if len(content) >= 50:
                filtered_articles.append(article)
            else:
                self.stats['short_content_removed'] += 1
        
        self.console.print(f"[green]ì§§ì€ ê¸°ì‚¬ ì œê±° ì™„ë£Œ: {self.stats['short_content_removed']}ê°œ ì œê±°[/green]")
        return filtered_articles
    
    def _normalize_dates(self, articles: List[Dict]) -> List[Dict]:
        """ë‚ ì§œ í˜•ì‹ì„ YYYY-MM-DD HH:MM:SSë¡œ í†µì¼"""
        self.console.print("[blue]ë‚ ì§œ í˜•ì‹ í†µì¼ ì¤‘...[/blue]")
        
        for article in articles:
            published_at = article.get('published_at')
            if published_at:
                normalized_date = self._normalize_date(published_at)
                if normalized_date:
                    article['published_at'] = normalized_date
        
        self.console.print("[green]ë‚ ì§œ í˜•ì‹ í†µì¼ ì™„ë£Œ[/green]")
        return articles
    
    def _normalize_date(self, date_value) -> str:
        """ë‚ ì§œ ê°’ì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if isinstance(date_value, str):
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            try:
                # ISO í˜•ì‹
                if 'T' in date_value:
                    dt = datetime.fromisoformat(date_value.replace('Z', '+00:00'))
                # ì¼ë°˜ì ì¸ í•œêµ­ ë‚ ì§œ í˜•ì‹ë“¤
                elif '-' in date_value:
                    if len(date_value.split('-')) == 3:
                        if ':' in date_value:
                            dt = datetime.strptime(date_value, '%Y-%m-%d %H:%M:%S')
                        else:
                            dt = datetime.strptime(date_value, '%Y-%m-%d')
                    else:
                        return date_value
                else:
                    return date_value
                
                return dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                return date_value
        elif hasattr(date_value, 'strftime'):
            # datetime ê°ì²´
            return date_value.strftime('%Y-%m-%d %H:%M:%S')
        
        return str(date_value)
    
    def _parse_date(self, date_value) -> datetime:
        """ë‚ ì§œ ê°’ì„ datetime ê°ì²´ë¡œ íŒŒì‹±"""
        if isinstance(date_value, str):
            try:
                if 'T' in date_value:
                    return datetime.fromisoformat(date_value.replace('Z', '+00:00'))
                elif '-' in date_value and ':' in date_value:
                    return datetime.strptime(date_value, '%Y-%m-%d %H:%M:%S')
                elif '-' in date_value:
                    return datetime.strptime(date_value, '%Y-%m-%d')
            except:
                pass
        elif hasattr(date_value, 'strftime'):
            return date_value
        
        return None
    
    def _update_supabase(self, articles: List[Dict]) -> bool:
        """ì „ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ Supabaseì— ë°˜ì˜"""
        self.console.print("[blue]Supabase ì—…ë°ì´íŠ¸ ì¤‘...[/blue]")
        
        try:
            # ê¸°ì¡´ í…Œì´ë¸” ë¹„ìš°ê¸° (ëª¨ë“  ë ˆì½”ë“œ ì‚­ì œ)
            # Supabaseì—ì„œëŠ” WHERE ì ˆì´ í•„ìš”í•˜ë¯€ë¡œ í•­ìƒ ì°¸ì¸ ì¡°ê±´ ì‚¬ìš©
            self.supabase.client.table('articles').delete().gte('id', 0).execute()
            
            # ìƒˆë¡œìš´ ë°ì´í„° ì‚½ì…
            if articles:
                result = self.supabase.client.table('articles').insert(articles).execute()
                if result.data:
                    self.stats['final_articles'] = len(result.data)
                    self.console.print(f"[green]Supabase ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(result.data)}ê°œ ê¸°ì‚¬ ì €ì¥[/green]")
                    return True
            
            self.console.print("[yellow]ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.[/yellow]")
            return True
            
        except Exception as e:
            self.logger.error(f"Supabase ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.console.print(f"[red]Supabase ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}[/red]")
            return False
    
    def _display_results(self):
        """ì „ì²˜ë¦¬ ê²°ê³¼ ì¶œë ¥"""
        self.console.print(Panel("ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼", style="green"))
        
        table = Table(title="ì „ì²˜ë¦¬ í†µê³„")
        table.add_column("í•­ëª©", style="cyan")
        table.add_column("ìˆ˜ëŸ‰", style="magenta")
        
        table.add_row("ì „ì²´ ê¸°ì‚¬", str(self.stats['total_articles']))
        table.add_row("URL+ë¯¸ë””ì–´ ì¤‘ë³µ ì œê±°", str(self.stats['duplicate_url_media']))
        table.add_row("ì •í™•í•œ ë‚´ìš© ì¤‘ë³µ ì œê±°", str(self.stats['duplicate_content_exact']))
        table.add_row("ìœ ì‚¬ ë‚´ìš© ì¤‘ë³µ ì œê±°", str(self.stats['duplicate_content_similar']))
        table.add_row("ì§§ì€ ê¸°ì‚¬ ì œê±°", str(self.stats['short_content_removed']))
        table.add_row("ìµœì¢… ê¸°ì‚¬", str(self.stats['final_articles']))
        
        self.console.print(table)
        
        # ìš”ì•½ ì •ë³´
        total_removed = (
            self.stats['duplicate_url_media'] + 
            self.stats['duplicate_content_exact'] + 
            self.stats['duplicate_content_similar'] + 
            self.stats['short_content_removed']
        )
        
        removal_rate = (total_removed / self.stats['total_articles']) * 100 if self.stats['total_articles'] > 0 else 0
        
        self.console.print(f"\n[green]ì´ {total_removed}ê°œ ê¸°ì‚¬ ì œê±° ({removal_rate:.1f}%)[/green]")
        self.console.print(f"[green]ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ![/green]")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    preprocessor = ArticlePreprocessor()
    success = preprocessor.preprocess_articles()
    
    if success:
        print("\nâœ… ì „ì²˜ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
